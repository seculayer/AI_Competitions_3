{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport re\nimport os\n\nimport torch\nimport torch.nn as nn\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\n\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2022-06-20T00:25:51.877454Z","iopub.execute_input":"2022-06-20T00:25:51.877776Z","iopub.status.idle":"2022-06-20T00:25:51.882961Z","shell.execute_reply.started":"2022-06-20T00:25:51.877746Z","shell.execute_reply":"2022-06-20T00:25:51.881885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T00:25:52.455821Z","iopub.execute_input":"2022-06-20T00:25:52.456167Z","iopub.status.idle":"2022-06-20T00:25:52.460679Z","shell.execute_reply.started":"2022-06-20T00:25:52.456105Z","shell.execute_reply":"2022-06-20T00:25:52.459737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 1234\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2022-06-20T00:25:53.078715Z","iopub.execute_input":"2022-06-20T00:25:53.079046Z","iopub.status.idle":"2022-06-20T00:25:53.085806Z","shell.execute_reply.started":"2022-06-20T00:25:53.079014Z","shell.execute_reply":"2022-06-20T00:25:53.084528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_to_word_list(text):\n    \"\"\" Preprocess and convert texts to a list of words \"\"\"\n    text = str(text)\n    text = text.lower()\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-06-20T00:25:53.505787Z","iopub.execute_input":"2022-06-20T00:25:53.506115Z","iopub.status.idle":"2022-06-20T00:25:53.521398Z","shell.execute_reply.started":"2022-06-20T00:25:53.506085Z","shell.execute_reply":"2022-06-20T00:25:53.520384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/quora-question-pairs/train.csv.zip\")\ntest_df = pd.read_csv(\"/kaggle/input/quora-question-pairs/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T00:25:53.974637Z","iopub.execute_input":"2022-06-20T00:25:53.974948Z","iopub.status.idle":"2022-06-20T00:26:00.739167Z","shell.execute_reply.started":"2022-06-20T00:25:53.974918Z","shell.execute_reply":"2022-06-20T00:26:00.738068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data analysis","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/quora-question-pairs/train.csv.zip\")\nprint(train_df.shape)\ntrain_df.dropna(inplace=True)\nfor question in ['question1', 'question2']:\n    train_df[question] = train_df[question].apply(lambda x: text_to_word_list(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-17T08:46:02.396207Z","iopub.execute_input":"2022-06-17T08:46:02.39658Z","iopub.status.idle":"2022-06-17T08:46:39.903308Z","shell.execute_reply.started":"2022-06-17T08:46:02.396552Z","shell.execute_reply":"2022-06-17T08:46:39.900463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 40","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:36:20.259963Z","iopub.execute_input":"2022-05-04T04:36:20.26031Z","iopub.status.idle":"2022-05-04T04:36:20.264734Z","shell.execute_reply.started":"2022-05-04T04:36:20.260274Z","shell.execute_reply":"2022-05-04T04:36:20.263791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Preparation","metadata":{}},{"cell_type":"code","source":"BERT_VERSION = 'bert-base-uncased'\nPOOLED_OUTPUT_DIM = 768 ","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:36:20.265986Z","iopub.execute_input":"2022-05-04T04:36:20.266592Z","iopub.status.idle":"2022-05-04T04:36:20.274861Z","shell.execute_reply.started":"2022-05-04T04:36:20.266552Z","shell.execute_reply":"2022-05-04T04:36:20.274111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(BERT_VERSION)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:36:20.277969Z","iopub.execute_input":"2022-05-04T04:36:20.278301Z","iopub.status.idle":"2022-05-04T04:36:20.621146Z","shell.execute_reply.started":"2022-05-04T04:36:20.278217Z","shell.execute_reply":"2022-05-04T04:36:20.620165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split data to train and validation sets\ntrain_df, val_df = train_test_split(train_df, test_size=0.1)\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:36:20.623804Z","iopub.execute_input":"2022-05-04T04:36:20.624337Z","iopub.status.idle":"2022-05-04T04:36:20.796624Z","shell.execute_reply.started":"2022-05-04T04:36:20.624296Z","shell.execute_reply":"2022-05-04T04:36:20.795805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertDataSet:\n    def __init__(self, first_questions, second_questions, targets, tokenizer):\n        self.first_questions = first_questions\n        self.second_questions = second_questions\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.length = len(first_questions)\n        \n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, item):\n        first_question = str(self.first_questions[item])\n        second_question = str(self.second_questions[item])\n\n        # removes extra white spaces from questions\n        first_question = \" \".join(first_question.split())\n        second_question = \" \".join(second_question.split())\n        \n        ### [CLS] question1 [SEP] questions2 [SEP] ... [PAD]\n        inputs = self.tokenizer.encode_plus(\n            first_question,\n            second_question,\n            add_special_tokens=True,\n            padding='max_length',\n            max_length=2 * MAX_LEN + 3, # max length of 2 questions and 3 special tokens\n            truncation=True   \n        )\n        \n        # return targets 0, when using data set in testing and targets are none\n        return {\n            \"ids\": torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n            \"mask\": torch.tensor(inputs[\"attention_mask\"], dtype=torch.long),\n            \"token_type_ids\": torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long),\n            \"targets\": torch.tensor(int(self.targets[item]), dtype=torch.long) if self.targets is not None else 0\n        }\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:36:20.798056Z","iopub.execute_input":"2022-05-04T04:36:20.798404Z","iopub.status.idle":"2022-05-04T04:36:20.810987Z","shell.execute_reply.started":"2022-05-04T04:36:20.798367Z","shell.execute_reply":"2022-05-04T04:36:20.809953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creates dataset and returns dataloader of it\ndef get_data_loader(df, targets, batch_size, shuffle, tokenizer):\n    dataset = BertDataSet(\n        first_questions=df[\"question1\"].values,\n        second_questions=df[\"question2\"].values,\n        targets=targets,\n        tokenizer=tokenizer\n    )\n    \n    data_loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size = batch_size,\n        shuffle=shuffle\n    )\n    \n    return data_loader","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:36:20.81266Z","iopub.execute_input":"2022-05-04T04:36:20.813256Z","iopub.status.idle":"2022-05-04T04:36:20.823097Z","shell.execute_reply.started":"2022-05-04T04:36:20.813218Z","shell.execute_reply":"2022-05-04T04:36:20.822137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training batch size we gonna use throughout this notebook.\nBS = 128","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:36:20.824441Z","iopub.execute_input":"2022-05-04T04:36:20.825067Z","iopub.status.idle":"2022-05-04T04:36:20.832314Z","shell.execute_reply.started":"2022-05-04T04:36:20.824989Z","shell.execute_reply":"2022-05-04T04:36:20.831273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create data loaders of training and validation data.\ntrain_data_loader = get_data_loader(\n    df=train_df,\n    targets=train_df[\"is_duplicate\"].values,\n    batch_size=BS,\n    shuffle=True,\n    tokenizer=tokenizer\n)\n\nval_data_loader = get_data_loader(\n    df=val_df,\n    targets=val_df[\"is_duplicate\"].values,\n    batch_size=4 * BS,\n    shuffle=True,\n    tokenizer=tokenizer\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:36:20.834006Z","iopub.execute_input":"2022-05-04T04:36:20.834616Z","iopub.status.idle":"2022-05-04T04:36:20.84403Z","shell.execute_reply.started":"2022-05-04T04:36:20.834577Z","shell.execute_reply":"2022-05-04T04:36:20.843294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class BertModel(nn.Module):\n    def __init__(self, bert_path):\n        super(BertModel, self).__init__()\n        self.bert_path = bert_path\n        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n        self.dropout = nn.Dropout(0.3)\n        self.out = nn.Linear(POOLED_OUTPUT_DIM, 1)\n\n    def forward(self, ids, mask, token_type_ids):\n        _, pooled = self.bert(ids, attention_mask=mask,token_type_ids=token_type_ids)\n        \n        # add dropout to prevent overfitting.\n        pooled = self.dropout(pooled) \n        return self.out(pooled)\n\nmodel = BertModel(BERT_VERSION).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:36:20.846547Z","iopub.execute_input":"2022-05-04T04:36:20.846788Z","iopub.status.idle":"2022-05-04T04:36:40.994527Z","shell.execute_reply.started":"2022-05-04T04:36:20.846765Z","shell.execute_reply":"2022-05-04T04:36:40.993635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# loss function is simple binary cross entropy loss\n# need sigmoid to put probabilities in [0,1] interval\ndef loss_fn(outputs, targets):\n    outputs = torch.squeeze(outputs)\n    return nn.BCELoss()(nn.Sigmoid()(outputs), targets)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:36:40.996092Z","iopub.execute_input":"2022-05-04T04:36:40.996582Z","iopub.status.idle":"2022-05-04T04:36:41.002018Z","shell.execute_reply.started":"2022-05-04T04:36:40.996542Z","shell.execute_reply":"2022-05-04T04:36:41.001253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# computes perplexity on validation data\ndef calculate_perplexity(data_loader, model, device):\n    model.eval()\n    \n    # tells Pytorch not to store values of intermediate computations for backward pass because we not gonna need gradients.\n    with torch.no_grad():\n        total_loss = 0\n        for batch in data_loader:\n            ids = batch[\"ids\"].to(device, dtype=torch.long)\n            mask = batch[\"mask\"].to(device, dtype=torch.long)\n            token_type_ids = batch[\"token_type_ids\"].to(device, dtype=torch.long)\n            targets = batch[\"targets\"].to(device, dtype=torch.float)\n\n            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n            total_loss += loss_fn(outputs, targets).item()\n            \n    model.train()\n\n    return np.exp(total_loss / len(data_loader))","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:36:41.003552Z","iopub.execute_input":"2022-05-04T04:36:41.003975Z","iopub.status.idle":"2022-05-04T04:36:41.015741Z","shell.execute_reply.started":"2022-05-04T04:36:41.003934Z","shell.execute_reply":"2022-05-04T04:36:41.014979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_loop(epochs, train_data_loader, val_data_loader, model, optimizer, device, scheduler=None):\n    it = 1\n    total_loss = 0\n    curr_perplexity = None\n    perplexity = None\n    \n    model.train()\n    for epoch in range(epochs):\n        print('Epoch: ', epoch + 1)\n        for batch in train_data_loader:\n            ids = batch[\"ids\"].to(device, dtype=torch.long)\n            mask = batch[\"mask\"].to(device, dtype=torch.long)\n            token_type_ids = batch[\"token_type_ids\"].to(device, dtype=torch.long)\n            targets = batch[\"targets\"].to(device, dtype=torch.float)\n\n            optimizer.zero_grad()\n            \n            # do forward pass, will save intermediate computations of the graph for later backprop use.\n            outputs = model(ids, mask=mask, token_type_ids=token_type_ids)\n            \n            loss = loss_fn(outputs, targets)\n            total_loss += loss.item()\n            \n            # running backprop.\n            loss.backward()\n            \n            # doing gradient descent step.\n            optimizer.step()\n            \n            # we are logging current loss/perplexity in every 100 iteration\n            if it % 100 == 0:\n                \n                # computing validation set perplexity in every 500 iteration.\n                if it % 500 == 0:\n                    curr_perplexity = calculate_perplexity(val_data_loader, model, device)\n                    \n                    if scheduler is not None:\n                        scheduler.step()\n\n                    # making checkpoint of best model weights.\n                    if not perplexity or curr_perplexity < perplexity:\n                        torch.save(model.state_dict(), 'saved_model')\n                        perplexity = curr_perplexity\n\n                print('| Iter', it, '| Avg Train Loss', total_loss / 100, '| Dev Perplexity', curr_perplexity)\n                total_loss = 0\n\n            it += 1\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:36:41.018553Z","iopub.execute_input":"2022-05-04T04:36:41.019224Z","iopub.status.idle":"2022-05-04T04:36:41.033596Z","shell.execute_reply.started":"2022-05-04T04:36:41.019164Z","shell.execute_reply":"2022-05-04T04:36:41.032493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run(model, train_df, device, train_data_loader, val_data_loader):\n    EPOCHS = 1\n    \n    lr = 3e-5\n    num_training_steps = int(len(train_data_loader) * EPOCHS)\n    optimizer = AdamW(model.parameters(), lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps\n    )\n    \n    \n    train_loop(EPOCHS, train_data_loader, val_data_loader,  model, optimizer, device, scheduler)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:36:41.036835Z","iopub.execute_input":"2022-05-04T04:36:41.037101Z","iopub.status.idle":"2022-05-04T04:36:41.045722Z","shell.execute_reply.started":"2022-05-04T04:36:41.037077Z","shell.execute_reply":"2022-05-04T04:36:41.044985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run(model, train_df, device, train_data_loader, val_data_loader)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T04:36:41.048057Z","iopub.execute_input":"2022-05-04T04:36:41.048716Z","iopub.status.idle":"2022-05-04T07:41:49.11909Z","shell.execute_reply.started":"2022-05-04T04:36:41.048676Z","shell.execute_reply":"2022-05-04T07:41:49.118054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/quora-question-pairs/test.csv\")\nprint(test_df.shape)\nfor question in ['question1', 'question2']:\n    test_df[question] = test_df[question].apply(lambda x: text_to_word_list(x))","metadata":{"execution":{"iopub.status.busy":"2022-06-17T08:46:52.130422Z","iopub.execute_input":"2022-06-17T08:46:52.130844Z","iopub.status.idle":"2022-06-17T08:47:21.239524Z","shell.execute_reply.started":"2022-06-17T08:46:52.130813Z","shell.execute_reply":"2022-06-17T08:47:21.235229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this function returns probabilities for every test case.\ndef test(model, test_df, device):\n    predictions = torch.empty(0).to(device, dtype=torch.float)\n    \n    test_dataset = BertDataSet(\n        first_questions=test_df[\"question1\"].values,\n        second_questions=test_df[\"question2\"].values,\n        targets=None,\n        tokenizer=tokenizer\n    )\n    \n    test_data_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=512\n    )\n    \n    with torch.no_grad():\n        model.eval()\n        for batch in tqdm(test_data_loader):\n            ids = batch[\"ids\"]\n            mask = batch[\"mask\"]\n            token_type_ids = batch[\"token_type_ids\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n\n            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n            predictions = torch.cat((predictions, nn.Sigmoid()(outputs)))\n    \n    return predictions.cpu().numpy().squeeze()\n\npredictions = test(model, test_df, device)\nlen(predictions)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:45:47.20551Z","iopub.execute_input":"2022-05-04T07:45:47.205863Z","iopub.status.idle":"2022-05-04T09:13:47.841466Z","shell.execute_reply.started":"2022-05-04T07:45:47.205828Z","shell.execute_reply":"2022-05-04T09:13:47.840581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# write down answers in is_duplicate column.\ntest_df['is_duplicate'] = predictions","metadata":{"execution":{"iopub.status.busy":"2022-05-04T09:13:47.845372Z","iopub.execute_input":"2022-05-04T09:13:47.847432Z","iopub.status.idle":"2022-05-04T09:13:47.856966Z","shell.execute_reply.started":"2022-05-04T09:13:47.847394Z","shell.execute_reply":"2022-05-04T09:13:47.855377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save results to submission.csv.\ntest_df[['test_id', 'is_duplicate']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T09:13:47.861728Z","iopub.execute_input":"2022-05-04T09:13:47.864195Z","iopub.status.idle":"2022-05-04T09:13:56.168839Z","shell.execute_reply.started":"2022-05-04T09:13:47.864155Z","shell.execute_reply":"2022-05-04T09:13:56.168025Z"},"trusted":true},"execution_count":null,"outputs":[]}]}