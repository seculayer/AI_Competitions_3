{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import skopt\n",
    "from skopt.callbacks import CheckpointSaver\n",
    "import mlflow\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object and col_type.name != 'category' and 'datetime' not in col_type.name:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        elif 'datetime' not in col_type.name:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train = pd.read_csv(\"data/train_le_reduced.csv\")\n",
    "    test = pd.read_csv(\"data/test_le_reduced.csv\")\n",
    "    y = pd.read_csv(\"data/y.csv\")\n",
    "    train = reduce_mem_usage(train)\n",
    "    test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(n_estimators=1000, learning_rate=0.1)\n",
    "lgbm.fit(train, y)\n",
    "preds = lgbm.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"lgbm_n1000_lr01_other_thresh0.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del lgbm\n",
    "gc.collect()\n",
    "\n",
    "lgbm = LGBMClassifier(n_estimators=1000, learning_rate=0.01)\n",
    "lgbm.fit(train, y)\n",
    "preds = lgbm.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"lgbm_n1000_lr001_other_thresh0.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del lgbm\n",
    "gc.collect()\n",
    "\n",
    "lgbm = LGBMClassifier(n_estimators=1000, learning_rate=0.03)\n",
    "lgbm.fit(train, y)\n",
    "preds = lgbm.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"lgbm_n1000_lr003_other_thresh0.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del lgbm\n",
    "gc.collect()\n",
    "\n",
    "lgbm = LGBMClassifier(n_estimators=1000, learning_rate=0.01)\n",
    "lgbm.fit(train, y)\n",
    "preds = lgbm.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"lgbm_n1000_lr001_other_thresh0.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del lgbm\n",
    "gc.collect()\n",
    "\n",
    "lgbm = LGBMClassifier(n_estimators=2000, learning_rate=0.1)\n",
    "lgbm.fit(train, y)\n",
    "preds = lgbm.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"lgbm_n2000_lr01_other_thresh0.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del lgbm\n",
    "gc.collect()\n",
    "\n",
    "lgbm = LGBMClassifier(n_estimators=2000, learning_rate=0.03)\n",
    "lgbm.fit(train, y)\n",
    "preds = lgbm.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"lgbm_n2000_lr003_other_thresh0.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del lgbm\n",
    "gc.collect()\n",
    "\n",
    "lgbm = LGBMClassifier(n_estimators=2000, learning_rate=0.01)\n",
    "lgbm.fit(train, y)\n",
    "preds = lgbm.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"lgbm_n2000_lr001_other_thresh0.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del lgbm\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(n_estimators=1000, tree_method=\"hist\", learning_rate=0.1)\n",
    "xgb.fit(train, y)\n",
    "preds = xgb.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"xgb_n1000_lr01_other_thresh0.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del xgb\n",
    "gc.collect()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=1000, tree_method=\"hist\", learning_rate=0.03)\n",
    "xgb.fit(train, y)\n",
    "preds = xgb.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"xgb_n1000_lr003_other_thresh0.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del xgb\n",
    "gc.collect()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=1000, tree_method=\"hist\", learning_rate=0.01)\n",
    "xgb.fit(train, y)\n",
    "preds = xgb.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"xgb_n1000_lr001_other_thresh0.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del xgb\n",
    "gc.collect()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=2000, tree_method=\"hist\", learning_rate=0.1)\n",
    "xgb.fit(train, y)\n",
    "preds = xgb.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"xgb_n2000_lr01_other_thresh0.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del xgb\n",
    "gc.collect()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=2000, tree_method=\"hist\", learning_rate=0.03)\n",
    "xgb.fit(train, y)\n",
    "preds = xgb.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"xgb_n2000_lr003_other_thresh0.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del xgb\n",
    "gc.collect()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=2000, tree_method=\"hist\", learning_rate=0.01)\n",
    "xgb.fit(train, y)\n",
    "preds = xgb.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"xgb_n2000_lr001_other_thresh0.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del xgb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train = pd.read_csv(\"data/train_le_reduced.csv\")\n",
    "    test = pd.read_csv(\"data/test_le_reduced.csv\")\n",
    "    y = pd.read_csv(\"data/y.csv\")\n",
    "    train = reduce_mem_usage(train)\n",
    "    test = reduce_mem_usage(test)\n",
    "\n",
    "# Threshold for train variables\n",
    "# C1: 0\n",
    "# Site_id: 10000\n",
    "# Site domain: 10000\n",
    "# Site_category: 10000\n",
    "# app_id: 10000\n",
    "# app_domain: 10000\n",
    "# app_category: 10000\n",
    "# device_id: 5000\n",
    "# device_ip: 5000\n",
    "# device_model: 5000\n",
    "# device_type: 0\n",
    "# device_conn_type: 0\n",
    "# C14: 10000\n",
    "# C15: 0\n",
    "# C16: 0\n",
    "# C17: 10000\n",
    "# C18: 0\n",
    "# C19: 10000\n",
    "# C20: 10000\n",
    "# C21: 0\n",
    "thresh_10000_vars = [\"site_id\", \"site_domain\", \"site_category\", \"app_id\", \"app_domain\", \"app_category\", \"C14\", \"C17\", \"C19\", \"C20\"]\n",
    "thresh_5000_vars = [\"device_id\", \"device_ip\", \"device_model\", \"device_type\", \"device_conn_type\"]\n",
    "thresh = 0\n",
    "for var in thresh_10000_vars:\n",
    "    value_counts = train[var].value_counts()\n",
    "    mask = train[var].isin(value_counts.index[value_counts<100])\n",
    "    train.loc[mask, var] = -1\n",
    "\n",
    "for var in thresh_5000_vars:\n",
    "    value_counts = train[var].value_counts()\n",
    "    mask = train[var].isin(value_counts.index[value_counts<100])\n",
    "    train.loc[mask, var] = -1\n",
    "\n",
    "# Threshold for test variables\n",
    "# C1: 0\n",
    "# Site_id: 10000\n",
    "# Site domain: 10000\n",
    "# Site_category: 10000\n",
    "# app_id: 10000\n",
    "# app_domain: 10000\n",
    "# app_category: 10000\n",
    "# device_id: 5000\n",
    "# device_ip: 5000\n",
    "# device_model: 5000\n",
    "# device_type: 0\n",
    "# device_conn_type: 0\n",
    "# C14: 10000\n",
    "# C15: 0\n",
    "# C16: 0\n",
    "# C17: 10000\n",
    "# C18: 0\n",
    "# C19: 10000\n",
    "# C20: 10000\n",
    "# C21: 0\n",
    "thresh_10000_vars = [\"site_id\", \"site_domain\", \"site_category\", \"app_id\", \"app_domain\", \"app_category\", \"C14\", \"C17\", \"C19\", \"C20\"]\n",
    "thresh_5000_vars = [\"device_id\", \"device_ip\", \"device_model\", \"device_type\", \"device_conn_type\"]\n",
    "thresh = 0\n",
    "for var in thresh_10000_vars:\n",
    "    value_counts = test[var].value_counts()\n",
    "    mask = test[var].isin(value_counts.index[value_counts<100])\n",
    "    test.loc[mask, var] = -1\n",
    "\n",
    "for var in thresh_5000_vars:\n",
    "    value_counts = test[var].value_counts()\n",
    "    mask = test[var].isin(value_counts.index[value_counts<100])\n",
    "    test.loc[mask, var] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "cat = CatBoostClassifier(iterations=1000, learning_rate=0.3)\n",
    "cat.fit(train, y)\n",
    "preds = cat.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"cat_n1000_lr03_other_thresh100.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del cat\n",
    "gc.collect()\n",
    "\n",
    "cat = CatBoostClassifier(iterations=1000, learning_rate=0.1)\n",
    "cat.fit(train, y)\n",
    "preds = cat.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"cat_n1000_lr01_other_thresh100.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del cat\n",
    "gc.collect()\n",
    "\n",
    "cat = CatBoostClassifier(iterations=2000, learning_rate=0.3)\n",
    "cat.fit(train, y)\n",
    "preds = cat.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"cat_n2000_lr03_other_thresh100.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del cat\n",
    "gc.collect()\n",
    "\n",
    "cat = CatBoostClassifier(iterations=2000, learning_rate=0.1)\n",
    "cat.fit(train, y)\n",
    "preds = cat.predict_proba(test)\n",
    "sub = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "sub[\"click\"] = 1 - preds\n",
    "sub[[\"id\", \"click\"]].to_csv(\"cat_n2000_lr01_other_thresh100.csv\", index=False)\n",
    "\n",
    "import gc\n",
    "del cat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abhishek's method\n",
    "Taken from: https://www.kaggle.com/competitions/avazu-ctr-prediction/discussion/10927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from math import log, exp, sqrt\n",
    "\n",
    "\n",
    "# TL; DR\n",
    "# the main learning process start at line 122\n",
    "\n",
    "\n",
    "# parameters #################################################################\n",
    "\n",
    "train = 'data/train.csv'  # path to training file\n",
    "test = 'data/test.csv'  # path to testing file\n",
    "\n",
    "D = 2 ** 20  # number of weights use for each model, we have 32 of them\n",
    "alpha = .1   # learning rate for sgd optimization\n",
    "\n",
    "\n",
    "# function, generator definitions ############################################\n",
    "\n",
    "# A. x, y generator\n",
    "# INPUT:\n",
    "#     path: path to train.csv or test.csv\n",
    "#     label_path: (optional) path to trainLabels.csv\n",
    "# YIELDS:\n",
    "#     ID: id of the instance (can also acts as instance count)\n",
    "#     x: a list of indices that its value is 1\n",
    "#     y: (if label_path is present) label value of y1 to y33\n",
    "def data(path, traindata=False):\n",
    "    for t, line in enumerate(open(path)):\n",
    "        # initialize our generator\n",
    "        if t == 0:\n",
    "            # create a static x,\n",
    "            # so we don't have to construct a new x for every instance\n",
    "            x = [0] * 27\n",
    "            continue\n",
    "        # parse x\n",
    "        for m, feat in enumerate(line.rstrip().split(',')):\n",
    "            if m == 0:\n",
    "                ID = int(feat)\n",
    "            elif traindata and m == 1:\n",
    "                y = [float(feat)]\n",
    "            else:\n",
    "                # one-hot encode everything with hash trick\n",
    "                # categorical: one-hotted\n",
    "                # boolean: ONE-HOTTED\n",
    "                # numerical: ONE-HOTTED!\n",
    "                # note, the build in hash(), although fast is not stable,\n",
    "                #       i.e., same value won't always have the same hash\n",
    "                #       on different machines\n",
    "                if traindata:\n",
    "                    x[m] = abs(hash(str(m) + '_' + feat)) % D\n",
    "                else:\n",
    "                    x[m+1] = abs(hash(str(m+1) + '_' + feat)) % D\n",
    "\n",
    "        yield (ID, x, y) if traindata else (ID, x)\n",
    "\n",
    "# B. Bounded logloss\n",
    "# INPUT:\n",
    "#     p: our prediction\n",
    "#     y: real answer\n",
    "# OUTPUT\n",
    "#     bounded logarithmic loss of p given y\n",
    "def logloss(p, y):\n",
    "    p = max(min(p, 1. - 10e-15), 10e-15)\n",
    "    return -log(p) if y == 1. else -log(1. - p)\n",
    "\n",
    "\n",
    "# C. Get probability estimation on x\n",
    "# INPUT:\n",
    "#     x: features\n",
    "#     w: weights\n",
    "# OUTPUT:\n",
    "#     probability of p(y = 1 | x; w)\n",
    "def predict(x, w):\n",
    "    wTx = 0.\n",
    "    for i in x:  # do wTx\n",
    "        wTx += w[i] * 1.  # w[i] * x[i], but if i in x we got x[i] = 1.\n",
    "    return 1. / (1. + exp(-max(min(wTx, 20.), -20.)))  # bounded sigmoid\n",
    "\n",
    "\n",
    "# D. Update given model\n",
    "# INPUT:\n",
    "# alpha: learning rate\n",
    "#     w: weights\n",
    "#     n: sum of previous absolute gradients for a given feature\n",
    "#        this is used for adaptive learning rate\n",
    "#     x: feature, a list of indices\n",
    "#     p: prediction of our model\n",
    "#     y: answer\n",
    "# MODIFIES:\n",
    "#     w: weights\n",
    "#     n: sum of past absolute gradients\n",
    "def update(alpha, w, n, x, p, y):\n",
    "    for i in x:\n",
    "        # alpha / sqrt(n) is the adaptive learning rate\n",
    "        # (p - y) * x[i] is the current gradient\n",
    "        # note that in our case, if i in x then x[i] = 1.\n",
    "        n[i] += abs(p - y)\n",
    "        w[i] -= (p - y) * 1. * alpha / sqrt(n[i])\n",
    "\n",
    "\n",
    "# training and testing #######################################################\n",
    "start = datetime.now()\n",
    "\n",
    "K = [0]\n",
    "\n",
    "w = [[0.] * D]\n",
    "n = [[0.] * D]\n",
    "\n",
    "loss = 0.\n",
    "\n",
    "tt = 1\n",
    "for ID, x, y in data(train, traindata = True):\n",
    "\n",
    "    # get predictions and train on all labels\n",
    "    for k in K:\n",
    "        p = predict(x, w[k])\n",
    "        update(alpha, w[k], n[k], x, p, y[k])\n",
    "        loss += logloss(p, y[k])  # for progressive validation\n",
    "\n",
    "    # print out progress, so that we know everything is working\n",
    "    if tt % 100000 == 0:\n",
    "        print('%s\\tencountered: %d\\tcurrent logloss: %f' % (\n",
    "                datetime.now(), tt, (loss * 1./tt)))\n",
    "    tt += 1\n",
    "\n",
    "with open('submission.csv', 'w') as outfile:\n",
    "    outfile.write('id,click\\n')\n",
    "    for ID, x in data(test):\n",
    "        for k in K:\n",
    "            p = predict(x, w[k])\n",
    "            outfile.write('%s,%s\\n' % (ID, str(p)))\n",
    "\n",
    "print('Done, elapsed time: %s' % str(datetime.now() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosters ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = pd.read_csv(\"lgbm_n1000_other_thresh0.csv\")\n",
    "xgb = pd.read_csv(\"xgb_n1000_lr01_other_thresh0.csv\")\n",
    "cat = pd.read_csv(\"xgb_n2000_lr01_other_thresh0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_thresh = 0.333333333\n",
    "xgb_thresh = 0.333333333\n",
    "cat_thresh = 0.333333333\n",
    "ensemble = lgbm.copy()\n",
    "ensemble[\"click\"] = lgbm[\"click\"] * lgbm_thresh + xgb[\"click\"] * xgb_thresh + cat[\"click\"] * cat_thresh\n",
    "ensemble.to_csv(\"ensemble-lgbm033_xgb033_cat033.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster_ensemble = pd.read_csv(\"ensemble-lgbm033_xgb033_cat033.csv\")\n",
    "abhi = pd.read_csv(\"submission.csv\")\n",
    "booster_ensemble_thresh = 0.5\n",
    "abhi_thresh = 0.5\n",
    "ensemble = booster_ensemble.copy()\n",
    "ensemble[\"click\"] = booster_ensemble[\"click\"] * booster_ensemble_thresh + abhi[\"click\"] * abhi_thresh\n",
    "ensemble.to_csv(\"ensemble-boosterensemble03_abhi07.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('kaggle_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29b2d023be3c919e7b75b4b99df1ad4463933915e6dc710c9217053dd6eea466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
