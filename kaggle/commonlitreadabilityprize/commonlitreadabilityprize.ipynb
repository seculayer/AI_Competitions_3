{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\n\nfrom sklearn.model_selection import KFold\n\nimport gc\ngc.enable()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-22T06:04:47.838497Z","iopub.execute_input":"2022-06-22T06:04:47.838942Z","iopub.status.idle":"2022-06-22T06:04:57.386627Z","shell.execute_reply.started":"2022-06-22T06:04:47.838904Z","shell.execute_reply":"2022-06-22T06:04:57.385817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 16\nMAX_LEN = 300\n# EVAL_SCHEDULE 값 조절\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"/kaggle/input/roberta-base\"\nTOKENIZER_PATH = \"/kaggle/input/roberta-base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2022-06-22T06:04:57.388107Z","iopub.execute_input":"2022-06-22T06:04:57.388729Z","iopub.status.idle":"2022-06-22T06:04:57.39512Z","shell.execute_reply.started":"2022-06-22T06:04:57.38868Z","shell.execute_reply":"2022-06-22T06:04:57.39421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2022-06-22T06:04:57.396761Z","iopub.execute_input":"2022-06-22T06:04:57.397272Z","iopub.status.idle":"2022-06-22T06:04:57.410205Z","shell.execute_reply.started":"2022-06-22T06:04:57.397222Z","shell.execute_reply":"2022-06-22T06:04:57.409372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\n\n# Remove incomplete entries if any.\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\nsubmission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-22T06:04:57.539257Z","iopub.execute_input":"2022-06-22T06:04:57.539821Z","iopub.status.idle":"2022-06-22T06:04:57.635209Z","shell.execute_reply.started":"2022-06-22T06:04:57.539782Z","shell.execute_reply":"2022-06-22T06:04:57.634013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:59:44.394908Z","iopub.execute_input":"2022-05-30T04:59:44.395211Z","iopub.status.idle":"2022-05-30T04:59:44.487509Z","shell.execute_reply.started":"2022-05-30T04:59:44.395179Z","shell.execute_reply":"2022-05-30T04:59:44.486668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len_train = train_df['excerpt'].apply(lambda x: len(x.split()))\nsns.histplot(len_train, bins = 20)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:59:44.863507Z","iopub.execute_input":"2022-05-30T04:59:44.864086Z","iopub.status.idle":"2022-05-30T04:59:45.058795Z","shell.execute_reply.started":"2022-05-30T04:59:44.864047Z","shell.execute_reply":"2022-05-30T04:59:45.057996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nlen_test = test_df['excerpt'].apply(lambda x: len(x.split()))\nsns.histplot(len_test, bins = 20)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T06:05:00.306496Z","iopub.execute_input":"2022-06-22T06:05:00.30719Z","iopub.status.idle":"2022-06-22T06:05:00.633548Z","shell.execute_reply.started":"2022-06-22T06:05:00.307137Z","shell.execute_reply":"2022-06-22T06:05:00.632155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        \n        self.text = df.excerpt.tolist()\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:59:45.072404Z","iopub.execute_input":"2022-05-30T04:59:45.072911Z","iopub.status.idle":"2022-05-30T04:59:45.080588Z","shell.execute_reply.started":"2022-05-30T04:59:45.072874Z","shell.execute_reply":"2022-05-30T04:59:45.079723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n                \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids = input_ids, attention_mask = attention_mask)        \n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n                \n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:59:45.082443Z","iopub.execute_input":"2022-05-30T04:59:45.08314Z","iopub.status.idle":"2022-05-30T04:59:45.093031Z","shell.execute_reply.started":"2022-05-30T04:59:45.083088Z","shell.execute_reply":"2022-05-30T04:59:45.092207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_mse(model, data_loader):\n    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n    model.eval()            \n    mse_sum = 0\n\n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)                        \n            target = target.to(DEVICE)           \n            \n            pred = model(input_ids, attention_mask)                       \n\n            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n                \n\n    return mse_sum / len(data_loader.dataset)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:59:45.094304Z","iopub.execute_input":"2022-05-30T04:59:45.094707Z","iopub.status.idle":"2022-05-30T04:59:45.106384Z","shell.execute_reply.started":"2022-05-30T04:59:45.094671Z","shell.execute_reply":"2022-05-30T04:59:45.105475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:59:45.109406Z","iopub.execute_input":"2022-05-30T04:59:45.109693Z","iopub.status.idle":"2022-05-30T04:59:45.11583Z","shell.execute_reply.started":"2022-05-30T04:59:45.10967Z","shell.execute_reply":"2022-05-30T04:59:45.115049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, model_path, train_loader, val_loader,\n          optimizer, scheduler=None, num_epochs=NUM_EPOCHS):    \n    best_val_rmse = None\n    best_epoch = 0\n    step = 0\n    last_eval_step = 0\n    eval_period = EVAL_SCHEDULE[0][1]    \n\n    start = time.time()\n\n    for epoch in range(num_epochs):                           \n        val_rmse = None         \n\n        for batch_num, (input_ids, attention_mask, target) in enumerate(train_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)            \n            target = target.to(DEVICE)\n            \n            # initailize gradient -> compute gradient for each batch iteration\n            optimizer.zero_grad()\n            \n            model.train()\n\n            pred = model(input_ids, attention_mask)\n                                                        \n            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n                        \n            mse.backward()\n\n            optimizer.step()\n            if scheduler:\n                scheduler.step()\n            \n            if step >= last_eval_step + eval_period:\n                # Evaluate the model on val_loader.\n                elapsed_seconds = time.time() - start\n                num_steps = step - last_eval_step\n                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n                last_eval_step = step\n                \n                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n\n                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n                      f\"val_rmse: {val_rmse:0.4}\")\n\n                for rmse, period in EVAL_SCHEDULE:\n                    if val_rmse >= rmse:\n                        eval_period = period\n                        break                               \n                \n                if not best_val_rmse or val_rmse < best_val_rmse:                    \n                    best_val_rmse = val_rmse\n                    best_epoch = epoch\n                    torch.save(model.state_dict(), model_path)\n                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n                else:       \n                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n                          f\"(from epoch {best_epoch})\")                                    \n                    \n                start = time.time()\n                                            \n            step += 1\n                        \n    \n    return best_val_rmse","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:59:45.117416Z","iopub.execute_input":"2022-05-30T04:59:45.117757Z","iopub.status.idle":"2022-05-30T04:59:45.131921Z","shell.execute_reply.started":"2022-05-30T04:59:45.117724Z","shell.execute_reply":"2022-05-30T04:59:45.131165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    \n    roberta_parameters = named_parameters[:197]\n    # not consider pooling layer \n    attention_parameters = named_parameters[199:203]\n    regressor_parameters = named_parameters[203:]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n\n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": regressor_group})\n\n    for layer_num, (name, params) in enumerate(roberta_parameters):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n\n        lr = 2e-5\n        \n        # Roberta layer 4 ~\n        if layer_num >= 69:        \n            lr = 5e-5\n            \n        # Roberta layer 8 ~ \n        if layer_num >= 133:\n            lr = 1e-4\n\n        parameters.append({\"params\": params,\n                           \"weight_decay\": weight_decay,\n                           \"lr\": lr})\n\n    return AdamW(parameters)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:59:45.134527Z","iopub.execute_input":"2022-05-30T04:59:45.135106Z","iopub.status.idle":"2022-05-30T04:59:45.143851Z","shell.execute_reply.started":"2022-05-30T04:59:45.135075Z","shell.execute_reply":"2022-05-30T04:59:45.143145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\n\nSEED = 1000\nlist_val_rmse = []\n\nkfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n\nfor fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):    \n    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n    model_path = f\"model_{fold + 1}.pth\"\n        \n    set_random_seed(SEED + fold)\n    \n    train_dataset = LitDataset(train_df.loc[train_indices])    \n    val_dataset = LitDataset(train_df.loc[val_indices])    \n        \n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                              drop_last=True, shuffle=True, num_workers=2)    \n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n                            drop_last=False, shuffle=False, num_workers=2)    \n        \n    set_random_seed(SEED + fold)    \n    \n    model = LitModel().to(DEVICE)\n    \n    # customize optimizer \n    optimizer = create_optimizer(model)                        \n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_training_steps = NUM_EPOCHS * len(train_loader),\n        num_warmup_steps = 50)    \n    \n    list_val_rmse.append(train(model, model_path, train_loader,\n                               val_loader, optimizer, scheduler=scheduler))\n\n    del model\n    gc.collect()\n    \n    print(\"\\nPerformance estimates:\")\n    print(list_val_rmse)\n    print(\"Mean:\", np.array(list_val_rmse).mean())\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:59:45.145077Z","iopub.execute_input":"2022-05-30T04:59:45.145531Z","iopub.status.idle":"2022-05-30T05:00:16.325608Z","shell.execute_reply.started":"2022-05-30T04:59:45.145495Z","shell.execute_reply":"2022-05-30T05:00:16.324049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"test_dataset = LitDataset(test_df, inference_only=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T05:00:16.3269Z","iopub.status.idle":"2022-05-30T05:00:16.327648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_predictions = np.zeros((len(list_val_rmse), len(test_df)))\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor index in range(len(list_val_rmse)):            \n    model_path = f\"model_{index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path))    \n    model.to(DEVICE)\n    \n    all_predictions[index] = predict(model, test_loader)\n    \n    del model\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T05:00:16.328781Z","iopub.status.idle":"2022-05-30T05:00:16.32947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = all_predictions.mean(axis=0)\nsubmission_df.target = predictions\nprint(submission_df)\nsubmission_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T05:00:16.330527Z","iopub.status.idle":"2022-05-30T05:00:16.331223Z"},"trusted":true},"execution_count":null,"outputs":[]}]}