{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "\n",
    "import skopt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, f1_score, recall_score, precision_score, balanced_accuracy_score, classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "SPACE = [skopt.space.Real(0.005, 0.1, name='learning_rate', prior='log-uniform'),\n",
    "         skopt.space.Integer(3, 100, name='max_depth'),\n",
    "         skopt.space.Real(0.1, 1, name='colsample_bytree', prior='uniform'),\n",
    "         skopt.space.Real(0.5, 1.0, name='subsample', prior='uniform')]\n",
    "\n",
    "\n",
    "STATIC_PARAMS = {\"objective\": \"binary:logistic\",\n",
    "                #  \"metric\": \"custom\",\n",
    "                 \"n_estimators\": 5000,\n",
    "                 \"random_state\": 314,\n",
    "                 \"eval_metric\": \"auc\"\n",
    "                }\n",
    "\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_csv('input/train_reduced.csv')\n",
    "    test = pd.read_csv('input/test_reduced.csv')\n",
    "    sub = pd.read_csv('input/sample_submission.csv.zip')\n",
    "    return train, test, sub\n",
    "\n",
    "\n",
    "def preprocess(train, test):\n",
    "    train = train.drop(['ID', 'target'], axis=1)\n",
    "    test = test.drop('ID', axis=1)\n",
    "\n",
    "    train = train.dropna(axis=1, thresh=2000)\n",
    "    test = test.dropna(axis=1, thresh=2000)\n",
    "\n",
    "    train = train.fillna(-1)\n",
    "    test = test.fillna(-1)\n",
    "    train, test = _label_encode(train, test)\n",
    "    return train, test\n",
    "\n",
    "def _label_encode(train, test):\n",
    "    for f in train.columns:\n",
    "        if train[f].dtype=='object': \n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train[f].values) + list(test[f].values))\n",
    "            train[f] = lbl.transform(list(train[f].values))\n",
    "            test[f] = lbl.transform(list(test[f].values))\n",
    "    return train, test\n",
    "\n",
    "def train_evaluate(all_params):\n",
    "    # Threshold to determine whether or not to save the f1 score obtained\n",
    "    AUC_SCORE_THRESHOLD = 0.79\n",
    "    start_time = time.time()\n",
    "    model = XGBClassifier(**all_params)\n",
    "    model.fit(X_train, y_train, early_stopping_rounds=7, eval_set=[(X_train, y_train), (X_val, y_val)])\n",
    "    # print(\"Yes\")\n",
    "    evals_result = model.evals_result()\n",
    "    # print(evals_result)\n",
    "    train_auc = evals_result[\"validation_0\"][\"auc\"][-1]\n",
    "    val_auc = evals_result[\"validation_1\"][\"auc\"][-1]\n",
    "    target_run_time = time.time() - start_time\n",
    "    # report = classification_report(y_val, model.predict(X_val))\n",
    "    print(f\"Took {target_run_time} seconds to optimize\")\n",
    "    print(all_params)\n",
    "    if val_auc > AUC_SCORE_THRESHOLD:\n",
    "        # print(lgbm.best_score_)\n",
    "        with mlflow.start_run():\n",
    "            print(all_params)\n",
    "            print(f\"Train auc score: {train_auc}\")\n",
    "            print(f\"Val auc score: {val_auc}\")\n",
    "            print(\"-------------------\")\n",
    "            \n",
    "            # params_dict, best_gini = make_params_to_log(results)\n",
    "            mlflow.log_params(all_params)\n",
    "            mlflow.log_metric(f\"train_auc\", train_auc)\n",
    "            mlflow.log_metric(f\"val_auc\", val_auc)\n",
    "            mlflow.log_metric(f\"target_run_time\", target_run_time)\n",
    "    return val_auc\n",
    "\n",
    "def custom_f1_score(threshold):\n",
    "    # https://stackoverflow.com/questions/63399806/how-to-pass-additional-parameters-to-lgbm-custom-loss-function\n",
    "    def func(y_true, y_pred):\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        y_pred = (y_pred>=threshold).astype(int)\n",
    "        report = classification_report(y_true, y_pred, output_dict=True)\n",
    "        f1_score_0 = report['0.0']['f1-score']\n",
    "        f1_score_1 = report['1.0']['f1-score']\n",
    "        f_score = f1_score_0 * 0.33333 + f1_score_1 * 0.66666\n",
    "        return \"f1_score\", f_score, True\n",
    "    return func\n",
    "\n",
    "def custom_recall_score(y_true, y_pred):\n",
    "    num_classes = int(len(y_pred) / len(y_true))\n",
    "    y_pred = y_pred.reshape(num_classes, -1).T\n",
    "    # y_pred = y_pred.argmax(axis = 1)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    # y_pred = np.apply_along_axis(self.le.inverse_transform, 1, y_pred)\n",
    "    return \"recall\", recall, True\n",
    "\n",
    "def custom_precision_score(y_true, y_pred):\n",
    "    num_classes = int(len(y_pred) / len(y_true))\n",
    "    y_pred = y_pred.reshape(num_classes, -1).T\n",
    "    # y_pred = y_pred.argmax(axis = 1)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    # y_pred = np.apply_along_axis(self.le.inverse_transform, 1, y_pred)\n",
    "    return \"precision\", precision, True\n",
    "\n",
    "def custom_balanced_score(y_true, y_pred):\n",
    "    num_classes = int(len(y_pred) / len(y_true))\n",
    "    y_pred = y_pred.reshape(num_classes, -1).T\n",
    "    # y_pred = y_pred.argmax(axis = 1)\n",
    "    balanced_score = balanced_accuracy_score(y_true, y_pred)\n",
    "    # y_pred = np.apply_along_axis(self.le.inverse_transform, 1, y_pred)\n",
    "    return \"balanced_score\", balanced_score, True\n",
    "\n",
    "@skopt.utils.use_named_args(SPACE)\n",
    "def objective(**params):\n",
    "    all_params = {**params, **STATIC_PARAMS}\n",
    "    return -1.0 * train_evaluate(all_params)\n",
    "\n",
    "\n",
    "def make_params_to_log(results):\n",
    "    params_names = [\"learning_rate\", \"max_depth\",\n",
    "                    \"colsample_bytree\", \"subsample\"]\n",
    "    params_dict = {}\n",
    "    for i in range(len(params_names)):\n",
    "        params_dict[params_names[i]] = results.x[i]\n",
    "    best_f1 = results.fun\n",
    "\n",
    "    return params_dict, best_f1\n",
    "\n",
    "\n",
    "def make_train_val_sets(X_train, y_train):\n",
    "    return train_test_split(X_train, y_train, train_size=0.75, test_size=0.25, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce df memory\n",
    "\n",
    "def read_data():\n",
    "    train = pd.read_csv('input/train.csv.zip')\n",
    "    test = pd.read_csv('input/test.csv.zip')\n",
    "    sub = pd.read_csv('input/sample_submission.csv.zip')\n",
    "    return train, test, sub\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object and col_type.name != 'category' and 'datetime' not in col_type.name:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        elif 'datetime' not in col_type.name:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    print(\"Starting loading data\")\n",
    "    train, test, sub = read_data()\n",
    "    y = train.target.values\n",
    "    print(\"Finished loading data\")\n",
    "    print(\"Start memory optimization\")\n",
    "    train = reduce_mem_usage(train)\n",
    "    test = reduce_mem_usage(test)\n",
    "    print(\"Finish memory optimization\")\n",
    "    \n",
    "    train.to_csv(\"input/train_reduced.csv\", index=False)\n",
    "    test.to_csv(\"input/test_reduced.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abishek parameters\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"Starting loading data\")\n",
    "    train, test, sub = read_data()\n",
    "    y = train.target.values\n",
    "    print(\"Finished loading data\")\n",
    "    \n",
    "    train, test = preprocess(train, test)\n",
    "\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(train, y, test_size=0.33, random_state=42, stratify=y)\n",
    "    \n",
    "    # del train\n",
    "    # del test\n",
    "    # del X\n",
    "    # del y\n",
    "    # gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(n_estimators=5000, nthread=-1, max_depth=17,\n",
    "                        learning_rate=0.01, silent=False, subsample=0.8, colsample_bytree=0.7)\n",
    "clf.fit(train, y)\n",
    "\n",
    "preds = clf.predict_proba(test)[:,1]\n",
    "sub.target = preds\n",
    "sub.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "#     print(\"Starting loading data\")\n",
    "#     train, test, sub = read_data()\n",
    "#     y = train.target.values\n",
    "#     print(\"Finished loading data\")\n",
    "    \n",
    "#     train, test = preprocess(train, test)\n",
    "\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(train, y, test_size=0.33, random_state=42, stratify=y)\n",
    "    \n",
    "#     del train\n",
    "#     del test\n",
    "#     # del X\n",
    "#     del y\n",
    "#     gc.collect()\n",
    "    \n",
    "#     experiment_name = f\"XGB\"\n",
    "#     mlflow.set_experiment(experiment_name)\n",
    "\n",
    "#     results = skopt.forest_minimize(objective, SPACE, n_calls=100, n_random_starts=50, random_state=42)\n",
    "#     params_dict, best_auc_score = make_params_to_log(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
