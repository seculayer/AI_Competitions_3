{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfaa5a14",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 필요한 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b160ed9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581d306e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU 사용가능 여부 체크\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a31e26",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 하이퍼파라미터 설정 및 시드 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f10fe3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "\n",
    "EPOCHS = 5 # 반복 횟수\n",
    "LR = 1e-5 # 학습률\n",
    "BS = 8 # 배치 크기\n",
    "SEED = 41 # 랜덤 시드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcff018e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 함수를 정의하여 모든 시드를 사전에 고정시킴\n",
    "# 앞에서 설정한 SEED(랜덤 시드) 사용\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457f4e0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3865f5f5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "990a1588",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>also I was the point person on my company’s tr...</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>You must’ve had your hands full.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>So let’s talk a little bit about your duties.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>0</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>TRAIN_9984</td>\n",
       "      <td>You or me?</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>1038</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>TRAIN_9985</td>\n",
       "      <td>I got it. Uh, Joey, women don't have Adam's ap...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>1038</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>TRAIN_9986</td>\n",
       "      <td>You guys are messing with me, right?</td>\n",
       "      <td>Joey</td>\n",
       "      <td>1038</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>TRAIN_9987</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>All</td>\n",
       "      <td>1038</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>TRAIN_9988</td>\n",
       "      <td>That was a good one. For a second there, I was...</td>\n",
       "      <td>Joey</td>\n",
       "      <td>1038</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9989 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                          Utterance  \\\n",
       "0     TRAIN_0000  also I was the point person on my company’s tr...   \n",
       "1     TRAIN_0001                   You must’ve had your hands full.   \n",
       "2     TRAIN_0002                            That I did. That I did.   \n",
       "3     TRAIN_0003      So let’s talk a little bit about your duties.   \n",
       "4     TRAIN_0004                             My duties?  All right.   \n",
       "...          ...                                                ...   \n",
       "9984  TRAIN_9984                                         You or me?   \n",
       "9985  TRAIN_9985  I got it. Uh, Joey, women don't have Adam's ap...   \n",
       "9986  TRAIN_9986               You guys are messing with me, right?   \n",
       "9987  TRAIN_9987                                              Yeah.   \n",
       "9988  TRAIN_9988  That was a good one. For a second there, I was...   \n",
       "\n",
       "              Speaker  Dialogue_ID    Target  \n",
       "0            Chandler            0   neutral  \n",
       "1     The Interviewer            0   neutral  \n",
       "2            Chandler            0   neutral  \n",
       "3     The Interviewer            0   neutral  \n",
       "4            Chandler            0  surprise  \n",
       "...               ...          ...       ...  \n",
       "9984         Chandler         1038   neutral  \n",
       "9985             Ross         1038   neutral  \n",
       "9986             Joey         1038  surprise  \n",
       "9987              All         1038   neutral  \n",
       "9988             Joey         1038       joy  \n",
       "\n",
       "[9989 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5b6b5c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9989 entries, 0 to 9988\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ID           9989 non-null   object\n",
      " 1   Utterance    9989 non-null   object\n",
      " 2   Speaker      9989 non-null   object\n",
      " 3   Dialogue_ID  9989 non-null   int64 \n",
      " 4   Target       9989 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 390.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23f782c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>Why do all the coffee cups have figures below?</td>\n",
       "      <td>Mark</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>Oh. It's so Monica can follow. Of this way, if...</td>\n",
       "      <td>Rachell</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>You know what?</td>\n",
       "      <td>Rachell</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>Come on, Lydia, you can do it.</td>\n",
       "      <td>Joeyy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>To push!</td>\n",
       "      <td>Joeyy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>TEST_2605</td>\n",
       "      <td>Yeah, I mean, go Ross, no one will even notice...</td>\n",
       "      <td>Rachell</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>TEST_2606</td>\n",
       "      <td>They don't listen to me?</td>\n",
       "      <td>Rossi</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>TEST_2607</td>\n",
       "      <td>Of course, they listen to you! Everyone listen...</td>\n",
       "      <td>Rachell</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>TEST_2608</td>\n",
       "      <td>Monica, do you really think I should try this ...</td>\n",
       "      <td>Rossi</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>TEST_2609</td>\n",
       "      <td>I think you look good.</td>\n",
       "      <td>Mornica</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2610 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                          Utterance  Speaker  \\\n",
       "0     TEST_0000     Why do all the coffee cups have figures below?     Mark   \n",
       "1     TEST_0001  Oh. It's so Monica can follow. Of this way, if...  Rachell   \n",
       "2     TEST_0002                                     You know what?  Rachell   \n",
       "3     TEST_0003                     Come on, Lydia, you can do it.    Joeyy   \n",
       "4     TEST_0004                                           To push!    Joeyy   \n",
       "...         ...                                                ...      ...   \n",
       "2605  TEST_2605  Yeah, I mean, go Ross, no one will even notice...  Rachell   \n",
       "2606  TEST_2606                           They don't listen to me?    Rossi   \n",
       "2607  TEST_2607  Of course, they listen to you! Everyone listen...  Rachell   \n",
       "2608  TEST_2608  Monica, do you really think I should try this ...    Rossi   \n",
       "2609  TEST_2609                             I think you look good.  Mornica   \n",
       "\n",
       "      Dialogue_ID  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               1  \n",
       "4               1  \n",
       "...           ...  \n",
       "2605          279  \n",
       "2606          279  \n",
       "2607          279  \n",
       "2608          279  \n",
       "2609          279  \n",
       "\n",
       "[2610 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67c59c94",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2610 entries, 0 to 2609\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ID           2610 non-null   object\n",
      " 1   Utterance    2610 non-null   object\n",
      " 2   Speaker      2610 non-null   object\n",
      " 3   Dialogue_ID  2610 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 81.7+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b61e12",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a58a3f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## train, test의 각 feature 특징 파악"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b00976",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b5342da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     4710\n",
       "joy         1743\n",
       "surprise    1205\n",
       "anger       1109\n",
       "sadness      683\n",
       "disgust      271\n",
       "fear         268\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d0d3f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Target 분석  \n",
    "예측해야 하는 감정의 종류는 총 7가지  \n",
    ": `neutral(중립), joy(기쁨), surprise(놀람), anger(분노), sadness(슬픔), disgust(혐오), fear(두려움)`  \n",
    "- neutral(중립) 감정이 가장 많음, 거의 절반 정도를 차지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2005c9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "469f2242",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Joey        1510\n",
       "Ross        1458\n",
       "Rachel      1435\n",
       "Phoebe      1321\n",
       "Monica      1299\n",
       "Chandler    1283\n",
       "Janice        58\n",
       "Carol         46\n",
       "Emily         43\n",
       "Tag           41\n",
       "Name: Speaker, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train['Speaker'].value_counts()))\n",
    "train['Speaker'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89ff9552",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Joeyy       411\n",
       "Chadler     379\n",
       "Rossi       373\n",
       "Rachell     356\n",
       "Mornica     346\n",
       "Phoebe      291\n",
       "Janice       31\n",
       "Emily        16\n",
       "Director     16\n",
       "Gunther      13\n",
       "Name: Speaker, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(test['Speaker'].value_counts()))\n",
    "test['Speaker'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49042bd4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Speaker 분석\n",
    "train 데이터에서 압도적으로 많이 등장하는 인물은 6명  \n",
    ": `Joey, Ross, Rachel, Phoebe, Monica, Chandler`  \n",
    "6명의 이름을 통해 이 데이터가 유명한 미드 'FRIENDS'의 대화 데이터인 것을 알 수 있었음  \n",
    "\n",
    "test 데이터를 보면 역시 6명이 많이 등장하는데, 이름이 train에서와 조금씩 다름(알파벳 1글자씩 다름)  \n",
    ": 아마도 인위적으로 조작을 한 것 같음, test의 대화에서는 train의 인물과 동일한 것을 보아 이 이름들을 추후 train과 일치시켜줘야 할 필요가 있어보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2209e89",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3. Dialogue_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd67a4cf",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1038\n",
      "523    24\n",
      "54     24\n",
      "450    24\n",
      "649    24\n",
      "530    24\n",
      "Name: Dialogue_ID, dtype: int64\n",
      "987    1\n",
      "790    1\n",
      "581    1\n",
      "490    1\n",
      "742    1\n",
      "Name: Dialogue_ID, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(train['Dialogue_ID'].value_counts()))\n",
    "print(train['Dialogue_ID'].value_counts().head())\n",
    "print(train['Dialogue_ID'].value_counts().tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbb20756",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n",
      "17     33\n",
      "175    23\n",
      "70     23\n",
      "169    23\n",
      "125    23\n",
      "Name: Dialogue_ID, dtype: int64\n",
      "19     1\n",
      "236    1\n",
      "276    1\n",
      "277    1\n",
      "91     1\n",
      "Name: Dialogue_ID, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(test['Dialogue_ID'].value_counts()))\n",
    "print(test['Dialogue_ID'].value_counts().head())\n",
    "print(test['Dialogue_ID'].value_counts().tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761fe479",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dialogue_ID 분석  \n",
    "train과 test에 존재하는 담화 상황 개수는 각각 1038, 280개임  \n",
    "한 담화 상황에 존재하는 대화 개수는 최대 24개에서 최소 1개까지 존재하고, test 데이터에서는 유일하게 33개의 문장으로 이뤄진 담화 상황도 존재하였음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0571aa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## train, test의 Utterance 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ec6886",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Utterance 같은 경우는 문장 형태이기 때문에 단어 단위로 쪼개서 분석하고자 하였음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e05dc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### train 문장들에 존재하는 단어 파악"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416df1b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "텐서플로우 케라스의 Tokenizer()로 단어를 쪼개는 '토큰화' 작업을 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9efe59f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# default: !,? 등의 문장 부호까지 제거\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train['Utterance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68beec5e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5921"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train에 존재하는 단어 수\n",
    "\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c02f2b96",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'you': 2, 'the': 3, 'to': 4, 'a': 5, 'and': 6, 'oh': 7, 'it': 8, 'that': 9, 'no': 10, 'what': 11, 'is': 12, 'me': 13, 'this': 14, 'so': 15, 'just': 16, 'okay': 17, 'yeah': 18, 'my': 19, 'of': 20, 'in': 21, 'have': 22, 'we': 23, 'do': 24, 'hey': 25, 'i’m': 26, 'well': 27, 'on': 28, 'know': 29, 'not': 30, 'all': 31, 'are': 32, 'for': 33, 'was': 34, 'right': 35, 'with': 36, 'be': 37, 'but': 38, 'your': 39, 'don’t': 40, 'go': 41, 'like': 42, 'gonna': 43, 'get': 44, 'it’s': 45, 'here': 46, 'out': 47, 'really': 48, 'uh': 49, 'about': 50, 'can': 51, 'he': 52, 'up': 53, \"i'm\": 54, 'think': 55, 'look': 56, 'there': 57, 'her': 58, 'how': 59, 'now': 60, 'one': 61, 'that’s': 62, 'if': 63, 'at': 64, 'got': 65, 'mean': 66, 'see': 67, 'you’re': 68, 'come': 69, 'him': 70, 'god': 71, 'why': 72, 'did': 73, 'she': 74, 'ross': 75, 'want': 76, 'sorry': 77, \"it's\": 78, 'good': 79, 'tell': 80, 'they': 81, 'joey': 82, 'when': 83, 'great': 84, 'guys': 85, 'hi': 86, \"y'know\": 87, 'going': 88, \"don't\": 89, 'y’know': 90, 'yes': 91, 'would': 92, 'then': 93, 'some': 94, 'umm': 95, 'could': 96, 'time': 97, 'were': 98, 'because': 99, 'back': 100, 'had': 101, 'say': 102, 'too': 103, 'little': 104, 'can’t': 105, 'love': 106, 'something': 107, 'chandler': 108, 'who': 109, 'been': 110, \"that's\": 111, 'wait': 112, 'as': 113, \"you're\": 114, 'guy': 115, 'should': 116, 'wanna': 117, 'ok': 118, 'an': 119, 'rachel': 120, 'phoebe': 121, 'thank': 122, 'or': 123, 'monica': 124, 'huh': 125, 'over': 126, 'from': 127, 'am': 128, 'them': 129, 'make': 130, 'doing': 131, 'his': 132, 'i’ll': 133, 'said': 134, 'we’re': 135, 'thing': 136, 'will': 137, 'us': 138, 'take': 139, 'please': 140, 'never': 141, 'sure': 142, 'people': 143, 'maybe': 144, 'ah': 145, 'where': 146, 'much': 147, 'let': 148, 'still': 149, 'listen': 150, 'does': 151, 'down': 152, 'um': 153, 'man': 154, 'way': 155, 'again': 156, 'two': 157, 'by': 158, 'off': 159, 'wow': 160, 'need': 161, 'very': 162, 'our': 163, 'actually': 164, 'give': 165, 'first': 166, 'fine': 167, 'he’s': 168, 'believe': 169, 'pheebs': 170, 'i’ve': 171, 'even': 172, 'didn’t': 173, 'thought': 174, 'big': 175, 'thanks': 176, 'only': 177, 'nice': 178, \"can't\": 179, 'night': 180, 'call': 181, 'more': 182, 'gotta': 183, 'has': 184, 'before': 185, 'stuff': 186, 'she’s': 187, 'work': 188, 'stop': 189, 'talk': 190, 'these': 191, 'place': 192, 'other': 193, 'whoa': 194, 'feel': 195, 'hello': 196, \"he's\": 197, 'ya': 198, 'lot': 199, 'those': 200, 'there’s': 201, 'ever': 202, 'any': 203, 'put': 204, 'last': 205, 'nothing': 206, 'married': 207, 'things': 208, 'new': 209, 'baby': 210, 'anything': 211, 'bye': 212, 'remember': 213, 'what’s': 214, 'let’s': 215, 'pretty': 216, 'rach': 217, 'girl': 218, 'went': 219, 'wanted': 220, 'ohh': 221, 'room': 222, 'into': 223, 'honey': 224, \"i'll\": 225, \"didn't\": 226, 'better': 227, 'around': 228, 'made': 229, 'they’re': 230, 'happened': 231, 'told': 232, 'kinda': 233, 'play': 234, 'name': 235, 'wedding': 236, 'guess': 237, 'care': 238, 'always': 239, 'help': 240, 'kind': 241, 'date': 242, 'thinking': 243, 'talking': 244, 'best': 245, 'having': 246, 'pick': 247, 'fun': 248, 'friends': 249, 'getting': 250, 'happy': 251, 'together': 252, \"we're\": 253, 'doesn’t': 254, 'left': 255, 'ask': 256, 'second': 257, 'day': 258, 'phone': 259, 'next': 260, \"what's\": 261, 'tonight': 262, 'friend': 263, 'bad': 264, 'ooh': 265, \"i've\": 266, 'totally': 267, 'woman': 268, 'which': 269, 'yet': 270, 'stupid': 271, 'ow': 272, 'five': 273, 'wrong': 274, 'eat': 275, 'funny': 276, 'whole': 277, 'job': 278, 'coming': 279, 'another': 280, \"she's\": 281, 'else': 282, 'ready': 283, 'try': 284, 'done': 285, 'ring': 286, 'stay': 287, 'problem': 288, 'years': 289, 'find': 290, 'hear': 291, 'tomorrow': 292, 'than': 293, 'old': 294, 'long': 295, 'after': 296, 'such': 297, 'called': 298, 'we’ll': 299, 'later': 300, 'might': 301, 'idea': 302, 'i’d': 303, 'check': 304, 'game': 305, 'home': 306, 'probably': 307, 'anyway': 308, 'course': 309, 'looking': 310, 'myself': 311, \"let's\": 312, 'ho': 313, 'trying': 314, 'move': 315, 'money': 316, 'enough': 317, 'someone': 318, \"they're\": 319, 'three': 320, 'supposed': 321, 'hard': 322, 'away': 323, 'came': 324, 'today': 325, 'coffee': 326, 'wants': 327, '‘cause': 328, 'leave': 329, 'keep': 330, 'mom': 331, 'party': 332, 'may': 333, 'hot': 334, 'every': 335, 'crazy': 336, 'happen': 337, 'weird': 338, 'ha': 339, 'being': 340, 'life': 341, 'part': 342, 'hold': 343, 'minute': 344, 'real': 345, 'took': 346, 'women': 347, 'bing': 348, 'morning': 349, 'wouldn’t': 350, 'apartment': 351, 'each': 352, 'ben': 353, 'naked': 354, 'mine': 355, 'matter': 356, 'high': 357, 'hand': 358, 'sex': 359, 'year': 360, 'saying': 361, 'everybody': 362, 'serious': 363, 'knew': 364, 'head': 365, 'anymore': 366, 'father': 367, 'dr': 368, 'dinner': 369, \"there's\": 370, 'show': 371, 'both': 372, 'most': 373, 'either': 374, 'hours': 375, 'sleep': 376, 'knows': 377, 'dad': 378, 'mr': 379, 'their': 380, 'excuse': 381, 'telling': 382, 'anyone': 383, 'alright': 384, 'turn': 385, 'you’ll': 386, 'says': 387, 'hang': 388, 'bring': 389, 'geller': 390, 'amazing': 391, 'same': 392, 'looks': 393, 'you’ve': 394, 'marry': 395, 'seen': 396, 'live': 397, 'use': 398, 'janice': 399, 'minutes': 400, 'world': 401, 'watch': 402, 'start': 403, 'until': 404, 'question': 405, 'won’t': 406, 'everything': 407, 'couple': 408, 'comes': 409, 'half': 410, 'house': 411, 'joe': 412, 'wish': 413, 'though': 414, 'late': 415, 'since': 416, 'beautiful': 417, 'meet': 418, 'chance': 419, 'yourself': 420, 'break': 421, 'deal': 422, 'wearing': 423, 'lost': 424, 'hmm': 425, 'making': 426, 'kiss': 427, 'also': 428, 'person': 429, 'forget': 430, \"doesn't\": 431, 'open': 432, 'throw': 433, 'understand': 434, 'kids': 435, 'asked': 436, 'own': 437, 'mind': 438, 'dude': 439, 'four': 440, 'read': 441, 'saw': 442, 'walk': 443, 'already': 444, 'who’s': 445, 'school': 446, 'doctor': 447, 'without': 448, 'bed': 449, 'hurt': 450, 'hate': 451, 'number': 452, 'hell': 453, 'sister': 454, 'anybody': 455, \"we'll\": 456, 'somebody': 457, 'through': 458, 'ten': 459, 'while': 460, 'cool': 461, 'pants': 462, 'bit': 463, 'absolutely': 464, 'once': 465, 'mad': 466, 'alone': 467, 'babies': 468, 'cannot': 469, 'kill': 470, 'ahh': 471, 'kidding': 472, 'used': 473, 'sweet': 474, 'glad': 475, \"i'd\": 476, 'promise': 477, 'cute': 478, 'must': 479, 'gone': 480, 'under': 481, 'sounds': 482, 'wear': 483, 'girls': 484, 'true': 485, 'food': 486, 'miss': 487, \"wasn't\": 488, 'sit': 489, 'wasn’t': 490, 'couldn’t': 491, 'hope': 492, 'makes': 493, 'between': 494, \"won't\": 495, \"'\": 496, 'bob': 497, 'haven’t': 498, 'able': 499, 'working': 500, 'different': 501, 'gave': 502, 'yep': 503, 'times': 504, 'kid': 505, 'brother': 506, 'gay': 507, 'buy': 508, 'gets': 509, 'worry': 510, 'easy': 511, 'found': 512, 'carol': 513, 'sweetie': 514, 'goes': 515, 'parents': 516, 'free': 517, 'started': 518, 'exactly': 519, 'ball': 520, '‘em': 521, 'movie': 522, \"isn't\": 523, 'cut': 524, 'important': 525, 'bet': 526, 'sir': 527, 'story': 528, 'book': 529, 'ugh': 530, 'soon': 531, 'picture': 532, 'pay': 533, 'isn’t': 534, 'here’s': 535, 'relax': 536, 'liked': 537, 'thanksgiving': 538, 'broke': 539, 'end': 540, 'unbelievable': 541, \"you've\": 542, 'perfect': 543, 'feeling': 544, 'face': 545, 'kathy': 546, 'small': 547, 'machine': 548, 'everyone': 549, 'win': 550, 'family': 551, 'plan': 552, 'candy': 553, 'hour': 554, 'asleep': 555, 'buddy': 556, 'table': 557, 'wh': 558, 'boy': 559, 'dance': 560, 'girlfriend': 561, 'huge': 562, 'men': 563, 'eh': 564, 'lady': 565, 'six': 566, 'touch': 567, 'hit': 568, 'means': 569, 'green': 570, 'moving': 571, 'museum': 572, 'city': 573, 'message': 574, 'marriage': 575, 'fire': 576, 'car': 577, 'mon': 578, 'realize': 579, 'uhh': 580, 'bathroom': 581, 'clothes': 582, 'many': 583, 'word': 584, 'taking': 585, 'brought': 586, 'street': 587, \"you'd\": 588, 'ass': 589, 'seeing': 590, 'pregnant': 591, 'asking': 592, 'almost': 593, 'its': 594, 'close': 595, 'playing': 596, 'shower': 597, 'son': 598, 'mrs': 599, 's': 600, 'song': 601, 'soup': 602, 'eyes': 603, 'seven': 604, 'roommate': 605, 'seem': 606, 'thinks': 607, 'line': 608, 'fell': 609, 'reason': 610, 'dead': 611, 'blue': 612, 'david': 613, 'talked': 614, 'few': 615, 'change': 616, 'bitch': 617, 'tickets': 618, 'fact': 619, 'smell': 620, 'whatever': 621, 'scared': 622, 'ohhh': 623, 'special': 624, 'aw': 625, 'aww': 626, 'aren’t': 627, 'lose': 628, \"you'll\": 629, \"who's\": 630, 'you’d': 631, 'spend': 632, 'lives': 633, 'eye': 634, 'dog': 635, 'least': 636, 'restaurant': 637, 'stick': 638, 'list': 639, 'plans': 640, 'dum': 641, \"we've\": 642, 'sick': 643, 'heads': 644, 'building': 645, 'shut': 646, 'top': 647, 'moment': 648, 'nobody': 649, \"wouldn't\": 650, 'shouldn’t': 651, 'stand': 652, 'grandmother': 653, 'ew': 654, 'rest': 655, 'welcome': 656, 'bag': 657, 'figure': 658, 'terrible': 659, 'won': 660, 'seriously': 661, 'quick': 662, 'doin’': 663, 'loved': 664, 'team': 665, '8': 666, 'forgot': 667, 'side': 668, 'fault': 669, 'moved': 670, 'bucks': 671, 'card': 672, 'definitely': 673, 'birthday': 674, 'shirt': 675, 'ago': 676, 'underwear': 677, 'floor': 678, 'monica’s': 679, 'wife': 680, 'feels': 681, 'week': 682, 'questions': 683, 'shoot': 684, 'susan': 685, 'write': 686, 'porn': 687, \"'cause\": 688, 'spent': 689, 'hands': 690, '30': 691, 'answer': 692, 'apparently': 693, 'route': 694, 'fast': 695, 'happens': 696, 'boyfriend': 697, 'luck': 698, \"couldn't\": 699, 'weeks': 700, 'heart': 701, 'daddy': 702, 'ones': 703, 'excited': 704, 'box': 705, 'office': 706, 'quit': 707, 'richard': 708, 'chef': 709, 'stuck': 710, 'turned': 711, 'living': 712, 'chick': 713, 'meant': 714, 'run': 715, 'joshua': 716, 'months': 717, 'hole': 718, 'lunch': 719, 'heard': 720, 'third': 721, 'yours': 722, 'felt': 723, 'drunk': 724, '10': 725, 'days': 726, 'cookies': 727, 'idiot': 728, 'emily': 729, 'store': 730, 'front': 731, 'town': 732, \"haven't\": 733, 'owe': 734, 'bought': 735, 'cup': 736, 'door': 737, 'chandler’s': 738, 'sometimes': 739, 'red': 740, 'window': 741, '2': 742, 'light': 743, 'e': 744, 'shot': 745, 'business': 746, 'die': 747, 'mother': 748, 'boxes': 749, 'class': 750, 'mail': 751, 'chip': 752, 'point': 753, 'sucks': 754, 'wrote': 755, 'mark': 756, 'scene': 757, 'damn': 758, 'loves': 759, 'water': 760, 'hair': 761, 'tribbiani': 762, 'completely': 763, 'surprise': 764, 'sweater': 765, 'mistake': 766, 'against': 767, 'ran': 768, 'calling': 769, 'looked': 770, 'tired': 771, 'decided': 772, 'fight': 773, 'dollars': 774, 'needs': 775, 'taste': 776, 'lie': 777, 'weekend': 778, 'set': 779, 'news': 780, 'entire': 781, 'rule': 782, 'knock': 783, 'leg': 784, 'interested': 785, 'sad': 786, 'figured': 787, 'suck': 788, 'park': 789, \"c'mon\": 790, 'speak': 791, 'grade': 792, 'grab': 793, 'pass': 794, 'watching': 795, 'bigger': 796, 'nah': 797, 'starts': 798, 'drake': 799, 'actor': 800, 'alan': 801, 'full': 802, 'empty': 803, 'inside': 804, 'wondering': 805, 'london': 806, 'so…': 807, 'professor': 808, 'middle': 809, 'cat': 810, 'difference': 811, 'pizza': 812, 'straight': 813, 'switch': 814, 'yay': 815, 'nervous': 816, 'although': 817, 'met': 818, 'breaking': 819, 'calm': 820, 'audition': 821, \"here's\": 822, 'interesting': 823, 'kissed': 824, 'duck': 825, 'king': 826, 'gimme': 827, 'rules': 828, 'seems': 829, 'clear': 830, 'favorite': 831, 'uncle': 832, 'secret': 833, 'jill': 834, 'neither': 835, 'would’ve': 836, 'we’ve': 837, 'sitting': 838, 'crush': 839, 'somewhere': 840, 'test': 841, '1': 842, 'fair': 843, 'missed': 844, 'nickname': 845, 'fat': 846, 'dating': 847, 'favor': 848, 'mess': 849, 'barry': 850, 'early': 851, 'band': 852, 'nooo': 853, 'trust': 854, 'yesterday': 855, 'casey': 856, 'sing': 857, 'downstairs': 858, 'human': 859, 'invite': 860, 'nine': 861, 'choose': 862, 'sent': 863, 'porsche': 864, 'died': 865, 'finish': 866, 'wonder': 867, 'divorce': 868, 'it’ll': 869, 'stripper': 870, 'strong': 871, 'holding': 872, 'lights': 873, 'romantic': 874, 'lying': 875, 'bijan': 876, 'birds': 877, 'crap': 878, 'la': 879, 'eight': 880, 'using': 881, 'quarter': 882, '6': 883, 'system': 884, 'must’ve': 885, 'month': 886, 'italian': 887, 'mm': 888, 'tough': 889, 'proud': 890, 'jealous': 891, 'finally': 892, 'paid': 893, 'health': 894, 'pain': 895, 'speech': 896, 'kept': 897, 'feelings': 898, 'fan': 899, 'putting': 900, 'worth': 901, 'tv': 902, 'warm': 903, 'letting': 904, 'bike': 905, 'stole': 906, 'starting': 907, 'ma': 908, \"'em\": 909, 'worked': 910, 'pack': 911, 'sensitive': 912, 'especially': 913, 'careful': 914, 'how’s': 915, 'keeps': 916, 'personal': 917, 'shopping': 918, 'twice': 919, 'happening': 920, 'joey’s': 921, 'christmas': 922, 'gotten': 923, 'vegas': 924, 'along': 925, 'dancing': 926, 'leaving': 927, 'showing': 928, 'across': 929, 'dear': 930, 'worried': 931, 'worse': 932, 'changed': 933, 'trip': 934, 'clean': 935, 'obviously': 936, 'pete': 937, 'page': 938, 'tag': 939, 'laundry': 940, 'cooking': 941, 'piece': 942, 'angela': 943, 'forever': 944, 'instead': 945, 'mona': 946, 'sometime': 947, 'cover': 948, 'speed': 949, 'order': 950, 'relationship': 951, 'charming': 952, 'button': 953, 'imagine': 954, 'drink': 955, 'freak': 956, 'body': 957, 'behind': 958, 'drop': 959, 'apologize': 960, '’': 961, '7': 962, 'sound': 963, 'giving': 964, 'turkey': 965, 'purse': 966, 'far': 967, 'lovely': 968, 'shh': 969, 'cry': 970, 'teach': 971, 'he’ll': 972, 'cowboy': 973, 'mouth': 974, 'noo': 975, 'larry': 976, 'save': 977, 'cause': 978, 'upset': 979, 'picked': 980, 'birth': 981, 'hoo': 982, 'frank': 983, 'smart': 984, 'decide': 985, 'ride': 986, 'rooms': 987, 'places': 988, 'ate': 989, 'children': 990, 'movies': 991, 'husband': 992, 'issac': 993, 'tulsa': 994, 'dump': 995, 'bar': 996, 'fall': 997, 'surprised': 998, 'truth': 999, 'central': 1000, 'hoping': 1001, 'evil': 1002, 'horny': 1003, 'bunch': 1004, '‘kay': 1005, 'conversation': 1006, \"aren't\": 1007, 'catch': 1008, 'fashion': 1009, 'incredible': 1010, 'size': 1011, 'wonderful': 1012, 'inappropriate': 1013, 'broken': 1014, 'age': 1015, 'sudden': 1016, 'pocket': 1017, 'hotel': 1018, 'sandwich': 1019, 'worst': 1020, 'everything’s': 1021, 'otherwise': 1022, 'technically': 1023, 'follow': 1024, 'college': 1025, 'couch': 1026, 'fix': 1027, 'takes': 1028, 'hooked': 1029, 'less': 1030, 'boring': 1031, 'annoying': 1032, 'ladies': 1033, 'future': 1034, 'except': 1035, '50': 1036, 'ticket': 1037, 'bride': 1038, 'dress': 1039, 'white': 1040, 'remembered': 1041, 'guest': 1042, 'wine': 1043, 'congratulations': 1044, 'outside': 1045, 'ms': 1046, 'buffay': 1047, 'hundred': 1048, 'what’d': 1049, 'kick': 1050, 'sell': 1051, 'double': 1052, 'universe': 1053, 'cheesecake': 1054, 'aunt': 1055, 'foot': 1056, 'cares': 1057, 'choice': 1058, 'morning’s': 1059, 'roll': 1060, 'post': 1061, 'appreciate': 1062, 'listening': 1063, 'explain': 1064, 'past': 1065, 'basically': 1066, 'trouble': 1067, 'daughter': 1068, 'checking': 1069, 'walking': 1070, 'tie': 1071, 'dream': 1072, 'feet': 1073, 'words': 1074, 'cheese': 1075, 'pie': 1076, 'nope': 1077, 'ice': 1078, 'bonnie': 1079, 'walked': 1080, 'sleeping': 1081, 'tall': 1082, 'kicking': 1083, 'flowers': 1084, 'boutros': 1085, 'york': 1086, 'engaged': 1087, 'coat': 1088, 'final': 1089, '4': 1090, 'jake': 1091, 'teacher': 1092, 'books': 1093, 'brain': 1094, 'thoughts': 1095, 'busy': 1096, 'view': 1097, \"where's\": 1098, 'comfortable': 1099, 'tiny': 1100, 'honest': 1101, 'wake': 1102, 'learned': 1103, 'damnit': 1104, 'boat': 1105, 'd': 1106, 'maid': 1107, 'honor': 1108, 'cross': 1109, '000': 1110, 'ross’s': 1111, 'sold': 1112, 'sense': 1113, 'jam': 1114, 'mary': 1115, 'gift': 1116, 'bugs': 1117, 'nap': 1118, 'milk': 1119, 'pig': 1120, 'reading': 1121, 'ex': 1122, 'names': 1123, 'outta': 1124, 'presents': 1125, 'hanging': 1126, 'backup': 1127, 'noodle': 1128, 'hospital': 1129, 'guy’s': 1130, 'keys': 1131, 'grandmother’s': 1132, 'spending': 1133, '5': 1134, 'paper': 1135, 'teaching': 1136, 'case': 1137, 'james': 1138, 'key': 1139, 'sports': 1140, 'breathe': 1141, 'share': 1142, 'unless': 1143, 'mike': 1144, 'pink': 1145, 'hung': 1146, 'embarrassed': 1147, 'lucky': 1148, 'dirty': 1149, 'script': 1150, 'camera': 1151, 'ridiculous': 1152, 'jack': 1153, 'genius': 1154, 'robot': 1155, 'fired': 1156, 'easier': 1157, 'music': 1158, 'rachel’s': 1159, 'bank': 1160, 'op': 1161, 'ended': 1162, 'accident': 1163, 'bastard': 1164, 'tux': 1165, 'control': 1166, 'sink': 1167, 'blew': 1168, 'canoe': 1169, 'center': 1170, 'throwing': 1171, 'fake': 1172, 'child': 1173, 'jason': 1174, 'familiar': 1175, '00': 1176, 'handle': 1177, 'realise': 1178, 'they’ve': 1179, 'assistant': 1180, 'chicken': 1181, 'nipple': 1182, 'accept': 1183, 'sec': 1184, 'asks': 1185, 'british': 1186, 'rock': 1187, 'actual': 1188, 'strip': 1189, 'covered': 1190, '17': 1191, 'n': 1192, 'enjoy': 1193, 'become': 1194, 'dumped': 1195, 'copy': 1196, 'sauce': 1197, 'extra': 1198, 'writing': 1199, 'turns': 1200, 'rather': 1201, 'desk': 1202, 'julie': 1203, 'expecting': 1204, \"what're\": 1205, 'ugly': 1206, 'fill': 1207, 'soo': 1208, 'twenty': 1209, 'experience': 1210, 'standing': 1211, 'opened': 1212, '3': 1213, 'hates': 1214, 'lemonade': 1215, 'jerk': 1216, 'suppose': 1217, 'goodbye': 1218, 'holiday': 1219, '25': 1220, 'john': 1221, 'drive': 1222, 'one’s': 1223, 'subway': 1224, 'tour': 1225, 'hah': 1226, 'chute': 1227, 'smelly': 1228, 'isabella': 1229, 'elizabeth': 1230, 'board': 1231, 'works': 1232, 'french': 1233, 'well…': 1234, 'hormones': 1235, 'celebrate': 1236, 'invited': 1237, 'expect': 1238, 'anywhere': 1239, 'saving': 1240, '40': 1241, 'trick': 1242, 'forward': 1243, 'spot': 1244, 'memories': 1245, 'computer': 1246, 'plane': 1247, 'air': 1248, 'lean': 1249, 'phoebe’s': 1250, 'san': 1251, 'quite': 1252, 'secrets': 1253, 'killed': 1254, 'interview': 1255, \"goin'\": 1256, \"doin'\": 1257, 'waiting': 1258, 'wha': 1259, 'chloe': 1260, 'j': 1261, 'candles': 1262, 'plus': 1263, 'poker': 1264, 'note': 1265, 'roger': 1266, 'hooker': 1267, 'deep': 1268, 'selling': 1269, 'possible': 1270, 'hall': 1271, 'device': 1272, 'needed': 1273, 'shoes': 1274, 'lately': 1275, 'o’clock': 1276, 'woo': 1277, 'dumb': 1278, 'letter': 1279, 'safe': 1280, 'single': 1281, 'lots': 1282, 'jail': 1283, 'she’ll': 1284, 'hurry': 1285, 'kitchen': 1286, 'bedroom': 1287, 'short': 1288, 'calls': 1289, 'reset': 1290, 'ways': 1291, 'dry': 1292, 'hm': 1293, 'gunther': 1294, 'island': 1295, 'judge': 1296, 'swear': 1297, 'magic': 1298, 'picking': 1299, 'black': 1300, 'bunny': 1301, 'hats': 1302, 'tree': 1303, 'brave': 1304, 'suds': 1305, 'heldi': 1306, 'bug': 1307, 'monday': 1308, 'fit': 1309, 'closing': 1310, 'beer': 1311, 'poor': 1312, 'touched': 1313, 'vase': 1314, 'how’d': 1315, 'legs': 1316, 'bond': 1317, 'afternoon': 1318, 'arms': 1319, 'bucket': 1320, 'garbage': 1321, 'insurance': 1322, 'toilet': 1323, 'young': 1324, 'during': 1325, 'pool': 1326, 'blind': 1327, 'scary': 1328, 'delivery': 1329, \"rachel's\": 1330, 'loud': 1331, 'dark': 1332, 'earrings': 1333, 'bus': 1334, 'practice': 1335, 'treeger': 1336, 'sort': 1337, 'joke': 1338, 'stevens': 1339, 'usually': 1340, 'address': 1341, 'nuts': 1342, 'nothin’': 1343, 'attracted': 1344, 'character': 1345, 'likes': 1346, 'ace': 1347, 'hearing': 1348, 'death': 1349, 'company': 1350, 'meeting': 1351, 'vestibule': 1352, 'train': 1353, 'department': 1354, 'land': 1355, 'saved': 1356, 'hero': 1357, 'dammit': 1358, 'opportunity': 1359, 'trade': 1360, 'entertainment': 1361, 'yemen': 1362, 'library': 1363, 'everywhere': 1364, 'quickly': 1365, 'europe': 1366, 'act': 1367, 'zoo': 1368, 'groom': 1369, 'regular': 1370, 'handling': 1371, 'present': 1372, 'drinking': 1373, 'horrible': 1374, 'account': 1375, 'dates': 1376, 'cafeteria': 1377, 'salad': 1378, 'rid': 1379, 'chi': 1380, 'attention': 1381, 'he’d': 1382, 'space': 1383, 'cups': 1384, 'sheet': 1385, 'shop': 1386, 'bra': 1387, 'oooh': 1388, 'soap': 1389, 'burn': 1390, 'litter': 1391, 'plastic': 1392, 'notice': 1393, 'freaked': 1394, \"monica's\": 1395, 'kip': 1396, 'massage': 1397, 'noise': 1398, 'service': 1399, 'rip': 1400, 'click': 1401, 've': 1402, 'bite': 1403, 'major': 1404, 'it’d': 1405, 'somebody’s': 1406, 'guitar': 1407, \"shouldn't\": 1408, 'tea': 1409, 'switching': 1410, 'divorced': 1411, 'slept': 1412, 'danny': 1413, 'friday': 1414, 'jane': 1415, 'rogers': 1416, 'shame': 1417, 'cost': 1418, 'trash': 1419, 'rosselini': 1420, 'admit': 1421, 'i…': 1422, 'twelve': 1423, 'biggest': 1424, 'altar': 1425, 'named': 1426, 'chasing': 1427, 'fifth': 1428, 'pull': 1429, 'blazer': 1430, 'tried': 1431, 'boss': 1432, 'sexual': 1433, 'knicks': 1434, 'whose': 1435, 'ad': 1436, 'pages': 1437, 'hypothetically': 1438, 'market': 1439, 'cab': 1440, 'ruin': 1441, 'neck': 1442, 'uncomfortable': 1443, 'fourth': 1444, \"guy's\": 1445, 'percent': 1446, 'states': 1447, 'nurse': 1448, 'goofing': 1449, '”': 1450, 'longer': 1451, 'jokes': 1452, 'she’d': 1453, 'bottom': 1454, 'uhh…': 1455, 'games': 1456, 'pair': 1457, 'gym': 1458, 'lipstick': 1459, 'robert': 1460, 'killing': 1461, 'million': 1462, 'dinosaur': 1463, 'aruba': 1464, 'farber': 1465, 'cream': 1466, 'y': 1467, 'gee': 1468, 'coats': 1469, 'hitting': 1470, 'thread': 1471, 'cameras': 1472, '500': 1473, '15': 1474, 'treat': 1475, 'fruit': 1476, 'goggles': 1477, 'dentist': 1478, 'allowed': 1479, 'international': 1480, 'country': 1481, 'centimeters': 1482, 'honeymoon': 1483, 'millions': 1484, 'respect': 1485, 'sisters': 1486, 'jim': 1487, 'tight': 1488, 'bee': 1489, 'enjoying': 1490, 'remoray': 1491, 'rhyme': 1492, 'poughkeepsie': 1493, 'wash': 1494, 'bars': 1495, 'letters': 1496, 'wet': 1497, 'puck': 1498, 'ordered': 1499, 'unfair': 1500, 'crying': 1501, 'near': 1502, 'numbers': 1503, 'whew': 1504, 'afraid': 1505, \"how's\": 1506, 'host': 1507, 'screw': 1508, 'heh': 1509, 'george': 1510, 'deserve': 1511, 'chuck': 1512, 'loose': 1513, 'available': 1514, 'realized': 1515, 'noooo': 1516, 'area': 1517, 'needle': 1518, 'cobb': 1519, 'gun': 1520, 'annulment': 1521, 'basket': 1522, 'quitting': 1523, 'played': 1524, 'shoe': 1525, 'bake': 1526, 'bla': 1527, 'o': 1528, 'ladle': 1529, 'section': 1530, 'smooth': 1531, 'information': 1532, 'ursula': 1533, 'learn': 1534, 'base': 1535, 'montreal': 1536, 'duties': 1537, 'heading': 1538, 'division': 1539, 'perhaps': 1540, 'poem': 1541, 'ameri': 1542, 'notch': 1543, 'specifics': 1544, 'kangaroo': 1545, 'dying': 1546, 'impression': 1547, \"thinkin'\": 1548, 'sees': 1549, 'pure': 1550, 'estelle': 1551, 'stomach': 1552, 'century': 1553, 'conference': 1554, 'gentlemen': 1555, 'hat': 1556, 'oy': 1557, 'commercial': 1558, 'responsibilities': 1559, 'sandwiches': 1560, 'issue': 1561, 'talkin’': 1562, 'public': 1563, 'insane': 1564, 'nerve': 1565, 'unfortunately': 1566, 'sign': 1567, 'narrowed': 1568, 'raymond': 1569, 'borrowed': 1570, 'bloody': 1571, 'barely': 1572, 'advice': 1573, 'paul': 1574, 'prefer': 1575, 'boyfriends': 1576, 'wall': 1577, 'tape': 1578, \"weren't\": 1579, 'history': 1580, 'peel': 1581, 'appointment': 1582, 'burning': 1583, 'dropped': 1584, 'bringing': 1585, 'known': 1586, \"what'd\": 1587, 'dana': 1588, 'actors': 1589, 'wanting': 1590, 'bobby': 1591, 'goodacre': 1592, 'rich': 1593, 'apartments': 1594, 'lines': 1595, 'phil': 1596, 'pal': 1597, 'toby': 1598, 'semi': 1599, 'everyday': 1600, 'eleven': 1601, 'perform': 1602, 'staring': 1603, 'oo': 1604, 'coulda': 1605, 'pardon': 1606, 'built': 1607, 'credit': 1608, 'cast': 1609, 'drinks': 1610, 'vince': 1611, 'jumping': 1612, 'lived': 1613, 'support': 1614, 'planning': 1615, \"ross's\": 1616, 'typical': 1617, 'send': 1618, 'circles': 1619, 'stars': 1620, 'smoke': 1621, \"how'd\": 1622, 'excellent': 1623, 'pieces': 1624, 'ourselves': 1625, 'clinic': 1626, 'raise': 1627, 'male': 1628, 'saturday': 1629, 'cassie': 1630, 'th': 1631, 'slice': 1632, 'barcelona': 1633, 'hiking': 1634, 'club': 1635, '19': 1636, 'whenever': 1637, 'someplace': 1638, 'cold': 1639, 'threw': 1640, \"that'd\": 1641, 'favour': 1642, 'rent': 1643, 'tells': 1644, 'sells': 1645, 'spacecamp': 1646, 'afford': 1647, 'grandma': 1648, 'impossible': 1649, 'steady': 1650, 'seconds': 1651, 'field': 1652, 'correct': 1653, 'julio': 1654, 'blond': 1655, 'boobs': 1656, 'serve': 1657, 'disagree': 1658, 'collection': 1659, 'knocked': 1660, 'pressure': 1661, 'obvious': 1662, 'ooooh': 1663, 'critic': 1664, 'review': 1665, 'classy': 1666, 'kitty': 1667, 'hungry': 1668, 'evening': 1669, 'stack': 1670, 'waving': 1671, 'sexy': 1672, 'toast': 1673, 'oatmeal': 1674, '‘cha': 1675, 'furniture': 1676, 'breakfast': 1677, 'commitment': 1678, 'hers': 1679, 'g': 1680, 'heckles': 1681, 'oboe': 1682, \"one's\": 1683, 'fool': 1684, 'peeking': 1685, 'intense': 1686, 'falling': 1687, 'singing': 1688, 'hidden': 1689, 'beard': 1690, 'drew': 1691, 'pretending': 1692, 'road': 1693, 'musician': 1694, '20': 1695, 'tomato': 1696, 'finished': 1697, 'taken': 1698, 'license': 1699, 'weirdest': 1700, 'labor': 1701, 'phoebs': 1702, 'expensive': 1703, 'in…': 1704, 'ton': 1705, 'mix': 1706, 'laminated': 1707, '13': 1708, 'misunderstood': 1709, 'lets': 1710, 'bell': 1711, 'agreed': 1712, 'jamie': 1713, 'cramp': 1714, 'sergei': 1715, 'gali': 1716, 'opening': 1717, \"we'd\": 1718, 'upstairs': 1719, 'action': 1720, 'awesome': 1721, 'toy': 1722, 'fund': 1723, 'beach': 1724, 'fear': 1725, 'm': 1726, 'court': 1727, 'knocking': 1728, 'engagement': 1729, 'telethon': 1730, 'gala': 1731, 'fancy': 1732, 'glass': 1733, 'yell': 1734, 'punch': 1735, 'armadillo': 1736, 'cards': 1737, 'insist': 1738, 'carry': 1739, 'certainly': 1740, 'combination': 1741, 'law': 1742, \"that'll\": 1743, 'itself': 1744, 'yelling': 1745, 'focus': 1746, 'where’s': 1747, 'marcel': 1748, 'diego': 1749, 'whoever': 1750, 'results': 1751, 'suddenly': 1752, 'pulling': 1753, 'unagi': 1754, 'aware': 1755, 'homemade': 1756, 'moon': 1757, 'return': 1758, 'safely': 1759, 'oww': 1760, 'plate': 1761, 'princess': 1762, 'rude': 1763, 'drums': 1764, 'weren’t': 1765, 'women’s': 1766, 'skin': 1767, 'flesh': 1768, 'breast': 1769, 'paying': 1770, 'mac': 1771, 'c': 1772, 'schedule': 1773, 'goin’': 1774, 'oberman': 1775, 'cervix': 1776, 'sweetheart': 1777, 'emergency': 1778, 'jump': 1779, 'mindy': 1780, 'tile': 1781, 'lifetime': 1782, 'junior': 1783, 'heat': 1784, 'possibly': 1785, 'empire': 1786, 'manager': 1787, 'running': 1788, 'closed': 1789, 'forgive': 1790, 'chelsea': 1791, 'naming': 1792, 'cancel': 1793, '‘til': 1794, 'fighting': 1795, 'blood': 1796, 'including': 1797, 'awful': 1798, 'ehh': 1799, 'energy': 1800, 'blocks': 1801, 'eating': 1802, 'should’ve': 1803, 'points': 1804, 'dressed': 1805, 'oil': 1806, 'sale': 1807, 'sweeping': 1808, \"o'clock\": 1809, 'docks': 1810, 'borrow': 1811, 'america': 1812, 'attractive': 1813, 'odds': 1814, 'drugs': 1815, 'beat': 1816, 'sake': 1817, 'boom': 1818, '700': 1819, 'private': 1820, 'sperm': 1821, 'brown': 1822, 'liking': 1823, 'privacy': 1824, 'color': 1825, 'disgusting': 1826, 'ocean': 1827, 'lily': 1828, 'gosh': 1829, 'bill': 1830, 'ed': 1831, 'begley': 1832, 'jr': 1833, 'wells': 1834, '16': 1835, 'natural': 1836, 'instincts': 1837, \"everything's\": 1838, 'fly': 1839, 'whispering': 1840, 'pregnancy': 1841, 'group': 1842, 'pearls': 1843, 'clown': 1844, 'eww': 1845, 'chicago': 1846, 'reservations': 1847, 'showed': 1848, 'sickness': 1849, 'power': 1850, 'considered': 1851, 'bother': 1852, 'ping': 1853, 'pong': 1854, 'terrific': 1855, 'butt': 1856, 'noticed': 1857, 'alarm': 1858, 'marks': 1859, 'mmm': 1860, 'answering': 1861, '27': 1862, 'saves': 1863, 'taught': 1864, 'president': 1865, 'contraction': 1866, 'stealing': 1867, 'dial': 1868, 'television': 1869, 'chan': 1870, 'magioni': 1871, 'paleontology': 1872, 'screwed': 1873, 'join': 1874, 'forbidden': 1875, 'novels': 1876, 'evaluation': 1877, 'concentrate': 1878, 'popular': 1879, 'matthews': 1880, 'partner': 1881, 'twin': 1882, 'flea': 1883, 'drapes': 1884, 'twins': 1885, 'fling': 1886, 'sugar': 1887, 'fortunately': 1888, 'poking': 1889, 'retract': 1890, 'flip': 1891, 'ducks': 1892, 'clowns': 1893, 'hollow': 1894, 'uterus': 1895, 'finals': 1896, 'it’s…': 1897, 'slip': 1898, 'worker': 1899, 'snack': 1900, 'gum': 1901, 'neighbors': 1902, 'towel': 1903, 'exciting': 1904, 'lawyer': 1905, 'negative': 1906, 'keeping': 1907, 'star': 1908, 'du': 1909, 'recipe': 1910, 'cracking': 1911, 'awww': 1912, 'normal': 1913, 'limit': 1914, 'brothers': 1915, 'seat': 1916, 'gives': 1917, 'vent': 1918, 'filing': 1919, 'friendly': 1920, 'smile': 1921, 'type': 1922, 'bamboozled': 1923, 'swing': 1924, 'greatest': 1925, 'kristen': 1926, 'kristin': 1927, 'sorta': 1928, '200': 1929, 'clearly': 1930, 'fifteen': 1931, 'amy': 1932, 'b': 1933, 'muscles': 1934, 'security': 1935, 'giant': 1936, 'leather': 1937, 'puppet': 1938, 'forty': 1939, 'geez': 1940, 'slow': 1941, 'problems': 1942, 'blame': 1943, 'scrud': 1944, \"laundry's\": 1945, 'tank': 1946, 'gas': 1947, 'healthy': 1948, 'siadic': 1949, 'following': 1950, 'waiter': 1951, 'postpone': 1952, 'director': 1953, 'heaven': 1954, 'thquirt': 1955, 'squeeze': 1956, 'strength': 1957, 'paris': 1958, 'conscious': 1959, 'lift': 1960, 'rain': 1961, 'freaking': 1962, 'staying': 1963, 'stage': 1964, 'breasts': 1965, 'minister': 1966, 'comet': 1967, 'bapstein': 1968, 'decision': 1969, 'chase': 1970, 'loser': 1971, 'thirsty': 1972, 'chop': 1973, 'sauti': 1974, 'there’ll': 1975, 'certain': 1976, 'beg': 1977, 'waitress': 1978, 'seemed': 1979, 'war': 1980, 'epic': 1981, 'suggestions': 1982, 'messed': 1983, 'dessert': 1984, 'buried': 1985, 'spirit': 1986, 'partying': 1987, 'stranger': 1988, 'video': 1989, 'hug': 1990, 'lined': 1991, 'hernia': 1992, 'indian': 1993, 'dinosaurs': 1994, 'sum': 1995, 'laughing': 1996, 'scientists': 1997, 'importantly': 1998, 'damage': 1999, 'diaper': 2000, 'ahhh': 2001, 'dropping': 2002, 'pee': 2003, 'awkward': 2004, 'common': 2005, 'exploded': 2006, 'gina': 2007, 'dina': 2008, 'lecture': 2009, 'kyle': 2010, \"ben's\": 2011, \"it'll\": 2012, 'hint': 2013, 'propose': 2014, \"they've\": 2015, 'umm…': 2016, 'uma': 2017, 'thurman': 2018, 'actress': 2019, 'feminist': 2020, 'charity': 2021, 'embarrassing': 2022, 'embarrass': 2023, 'easily': 2024, 'secure': 2025, 'total': 2026, 'wuss': 2027, 'lobby': 2028, 'charge': 2029, 'vic': 2030, 'eighth': 2031, 'deli': 2032, 'pet': 2033, 'navy': 2034, 'memory': 2035, 'tomatoes': 2036, 'waaay': 2037, 'league': 2038, 'chair': 2039, 'era': 2040, 'queen': 2041, \"baby's\": 2042, 'von': 2043, 'level': 2044, 'earlier': 2045, 'apologise': 2046, 'anniversary': 2047, 'promised': 2048, 'complement': 2049, 'ohh…': 2050, \"sister's\": 2051, 'krista': 2052, 'atm': 2053, 'switched': 2054, 'gravy': 2055, 'fridge': 2056, 'maniac': 2057, 'ye': 2058, 'adding': 2059, 'thin': 2060, 'worn': 2061, 'film': 2062, 'famous': 2063, 'internet': 2064, 'marrying': 2065, 'legitimate': 2066, 'member': 2067, 'saliva': 2068, 'playfully': 2069, 'cushion': 2070, 'stain': 2071, 'spill': 2072, 'exact': 2073, 'robe': 2074, 'latte': 2075, 'pretend': 2076, 'sissy': 2077, 'hadn’t': 2078, 'nowhere': 2079, 'virginity': 2080, 'santa': 2081, 'yuh': 2082, 'invitation': 2083, 'finds': 2084, 'drag': 2085, 'oughta': 2086, 'bum': 2087, 'neat': 2088, 'match': 2089, 'apart': 2090, 'rushed': 2091, 'nuclear': 2092, 'goods': 2093, 'enjoyed': 2094, 'create': 2095, 'sophie': 2096, 'unbelievably': 2097, 'nurses': 2098, 'harder': 2099, 'magician': 2100, 'shrill': 2101, 'mill': 2102, 'dealing': 2103, 'johnson': 2104, 'checked': 2105, 'messages': 2106, 'round': 2107, 'lap': 2108, 'nutmeg': 2109, 'professional': 2110, 'sailing': 2111, '‘sup': 2112, 'strangers': 2113, 'signed': 2114, 'kay': 2115, 'helping': 2116, 'india': 2117, 'atlantic': 2118, 'invented': 2119, 'square': 2120, 'seventh': 2121, 'windows': 2122, 'betting': 2123, 'answers': 2124, 'written': 2125, 'goalie': 2126, 'hairy': 2127, 'eligible': 2128, 'you…': 2129, 'wig': 2130, 'ain’t': 2131, 'respectfully': 2132, 'rocks': 2133, 'ignore': 2134, 'sunshine': 2135, 'sky': 2136, 'begin': 2137, 'ooohh': 2138, 'drama': 2139, 'de': 2140, 'spilled': 2141, 'research': 2142, 'fits': 2143, 'competition': 2144, 'chest': 2145, 'convinced': 2146, 'gorilla': 2147, 'pile': 2148, \"everybody's\": 2149, 'football': 2150, 'nights': 2151, 'stuffing': 2152, 'they’ll': 2153, 'honing': 2154, 'dishes': 2155, 'caught': 2156, 'grow': 2157, 'cook': 2158, 'trusted': 2159, 'shave': 2160, 'impressions': 2161, 'butter': 2162, 'lamp': 2163, 're': 2164, 'ukrainian': 2165, 'god…': 2166, 'aid': 2167, 'fantastic': 2168, 'r': 2169, 'fellow': 2170, 'appears': 2171, '‘bout': 2172, 'songs': 2173, 'draw': 2174, \"friend's\": 2175, 'moustache': 2176, 'iced': 2177, 'juice': 2178, 'hmmm': 2179, 'fella': 2180, 'voice': 2181, 'delicious': 2182, 'criticism': 2183, 'mood': 2184, 'killer': 2185, 'feast': 2186, 'pumpkin': 2187, 'difficult': 2188, 'gang': 2189, 'dare': 2190, 'grown': 2191, 'blow': 2192, 'saint': 2193, 'stolen': 2194, 'starving': 2195, 'lookin’': 2196, 'arm': 2197, \"roger's\": 2198, 'spooky': 2199, 'ole': 2200, 'factory': 2201, 'pin': 2202, 'f': 2203, 'waking': 2204, 'feeding': 2205, 'jingle': 2206, 'nut': 2207, 'whip': 2208, 'ahead': 2209, 'popcorn': 2210, 'pushing': 2211, 'success': 2212, 'disease': 2213, 'freeze': 2214, 'vacation': 2215, 'florida': 2216, 'krog': 2217, 'fully': 2218, 'herself': 2219, 'baby’s': 2220, 'waitressing': 2221, 'specials': 2222, 'chocolate': 2223, 'midnight': 2224, 'truck': 2225, 'jordie': 2226, 'suit': 2227, 'sun': 2228, 'mountain': 2229, 'jogging': 2230, 'bird': 2231, 'property': 2232, 'lovers': 2233, 'waxine': 2234, 'linen': 2235, 'strips': 2236, 'motion': 2237, 'painful': 2238, 'wax': 2239, 'satin': 2240, 'learning': 2241, 'scientist': 2242, 'russia': 2243, \"santa's\": 2244, 'waiters': 2245, 'chefs': 2246, 'audience': 2247, 'depressed': 2248, 'doug': 2249, 'bear': 2250, 'behaved': 2251, \"joey's\": 2252, 'tuesday': 2253, 'balls': 2254, 'flying': 2255, 'tom': 2256, 'blackout': 2257, 'beginning': 2258, 'sheer': 2259, 'blouse': 2260, 'doctors': 2261, 'became': 2262, 'yelled': 2263, 'routine': 2264, 'catching': 2265, 'could’ve': 2266, 't': 2267, 'american': 2268, 'forward—stop': 2269, 'push': 2270, 'janine': 2271, 'ba': 2272, 'cheshhh': 2273, 'wind': 2274, 'airport': 2275, \"he'll\": 2276, 'fabulous': 2277, 'charlie': 2278, \"ain't\": 2279, 'maternity': 2280, 'hogging': 2281, 'july': 2282, 'apple': 2283, 'cubicle': 2284, 'cube': 2285, 'indeedy': 2286, 'sweep': 2287, 'hammer': 2288, 'cruel': 2289, 'ringing': 2290, 'helped': 2291, 'sid': 2292, 'deposit': 2293, 'assumed': 2294, 'alive': 2295, '48': 2296, 'joan': 2297, 'collins': 2298, 'sea': 2299, 'large': 2300, 'cancelled': 2301, 'counter': 2302, 'flattered': 2303, 'sensed': 2304, 'apollo': 2305, 'sixth': 2306, 'stood': 2307, 'katie': 2308, 'nicest': 2309, 'kicked': 2310, 'rapist': 2311, 'brochures': 2312, 'coffeehouse': 2313, 'paste': 2314, 'situation': 2315, 'maria': 2316, 'silk': 2317, 'theme': 2318, 'nipples': 2319, 'career': 2320, 'gogo': 2321, 'depends': 2322, 'bail': 2323, 'pathetic': 2324, 'needy': 2325, 'egg': 2326, 'muffins': 2327, 'exist': 2328, 'controls': 2329, 'h': 2330, 'wasting': 2331, 'guilty': 2332, 'graduated': 2333, 'lobster': 2334, 'shake': 2335, 'quack': 2336, 'plates': 2337, '‘ya': 2338, 'celtics': 2339, 'bernice': 2340, '‘is': 2341, 'tiles': 2342, 'build': 2343, 'unit': 2344, 'cinderelly': 2345, 'customer': 2346, 'scare': 2347, 'kisser': 2348, 'toner': 2349, 'supply': 2350, 'catering': 2351, 'lock': 2352, 'gettin’': 2353, 'often': 2354, 'cilantro': 2355, 'hopping': 2356, 'daily': 2357, 'joseph': 2358, 'francis': 2359, 'conquered': 2360, 'physical': 2361, 'ultimate': 2362, 'champion': 2363, 'training': 2364, 'designer': 2365, 'row': 2366, 'beeped': 2367, 'spleen': 2368, 'bald': 2369, 'chess': 2370, 'soak': 2371, 'blazers': 2372, 'rejoin': 2373, '22': 2374, '18': 2375, 'spelled': 2376, 'boobies': 2377, 'mint': 2378, 'treasures': 2379, 'price': 2380, 'score': 2381, 'disposable': 2382, 'groceries': 2383, 'report': 2384, 'eaten': 2385, 'sharp': 2386, 'wheel': 2387, 'jackass': 2388, 'contestants': 2389, 'horse': 2390, 'allergies': 2391, 'soda': 2392, 'mom’s': 2393, 'sweety': 2394, 'bags': 2395, 'refuse': 2396, 'tramp': 2397, 'faster': 2398, 'broom': 2399, 'opposite': 2400, 'batch': 2401, 'dies': 2402, 'mailman': 2403, 'teeth': 2404, 'hurely': 2405, 'sucker': 2406, 'boarding': 2407, 'jacket': 2408, \"god's\": 2409, 'lens': 2410, 'countdown': 2411, 'alien': 2412, 'suite': 2413, 'lounge': 2414, 'manly': 2415, 'slut': 2416, 'brings': 2417, 'avenue': 2418, 'reasons': 2419, 'hardly': 2420, 'wife’s': 2421, 'nelson': 2422, 'grass': 2423, 'dingy': 2424, 'doubt': 2425, 'hamper': 2426, 'remove': 2427, 'liam': 2428, 'unusual': 2429, 'abott': 2430, 'cable': 2431, 'loan': 2432, 'jet': 2433, 'helps': 2434, 'pledges': 2435, 'performance': 2436, 'guts': 2437, \"sayin'\": 2438, 'drunken': 2439, 'stunning': 2440, 'attack': 2441, 'connect': 2442, '33': 2443, 'issues': 2444, 'bagel': 2445, 'confident': 2446, 'manage': 2447, 'hardest': 2448, 'that’': 2449, 'picnic': 2450, 'finger': 2451, '35': 2452, 'impressive': 2453, 'guard': 2454, 'breath': 2455, 'walls': 2456, 'boxer': 2457, 'shorts': 2458, 'woman’s': 2459, 'man’s': 2460, 'awfully': 2461, 'application': 2462, 'closer': 2463, 'panicked': 2464, '’74': 2465, 'latour': 2466, 'unacceptable': 2467, 'punk': 2468, 'state': 2469, 'mississ': 2470, 'nude': 2471, 'blades': 2472, 'complicated': 2473, 'just…': 2474, 'magazines': 2475, 'aisle': 2476, 'incredibly': 2477, 'paints': 2478, 'charcoal': 2479, 'and…': 2480, '41': 2481, 'uh…i': 2482, 'paranoid': 2483, 'proposed': 2484, 'goose': 2485, 'bumps': 2486, 'rush': 2487, \"would've\": 2488, 'chin': 2489, 'puff': 2490, 'forgotten': 2491, 'tastes': 2492, 'crust': 2493, 'illinois': 2494, 'hopefully': 2495, 'ours': 2496, \"c'mere\": 2497, 'rough': 2498, 'forgetting': 2499, 'competitive': 2500, 'fish': 2501, 'fight’s': 2502, 'lotion': 2503, 'planned': 2504, 'howie': 2505, 'friend’s': 2506, 'hated': 2507, 'label': 2508, 'search': 2509, 'irony': 2510, 'sidney': 2511, 'benefits': 2512, 'lapsed': 2513, 'agent': 2514, 'rented': 2515, 'mention': 2516, 'triple': 2517, '93': 2518, '76': 2519, 'barium': 2520, 'local': 2521, 'laugh': 2522, '80': 2523, 'urgent': 2524, 'lasagne': 2525, 'we’d': 2526, 'los': 2527, 'gary': 2528, 'boots': 2529, 'tan': 2530, 'opera': 2531, 'recent': 2532, 'whom': 2533, 'i…i': 2534, 'neil': 2535, 'lasagna': 2536, 'paolo': 2537, 'fulfilling': 2538, 'yikes': 2539, 'animal': 2540, 'bumped': 2541, 'none': 2542, 'accent': 2543, 'nanny': 2544, 'betty': 2545, 'chocolates': 2546, 'zelner': 2547, 'abort': 2548, 'uniform': 2549, 'specially': 2550, 'project': 2551, 'classes': 2552, '–': 2553, 'sheets': 2554, 'cried': 2555, 'suggestion': 2556, 'potatoes': 2557, 'kinds': 2558, 'underdog': 2559, 'balloon': 2560, 'cartoon': 2561, 'reached': 2562, \"d'you\": 2563, 'observe': 2564, \"'you\": 2565, 'quiet': 2566, 'chairs': 2567, '45': 2568, 'identical': 2569, 'study': 2570, 'testing': 2571, 'smells': 2572, 'officially': 2573, 'fairly': 2574, 'broadway': 2575, 'balance': 2576, 'tweezers': 2577, 'acting': 2578, 'direction': 2579, 'south': 2580, 'clever': 2581, 'ideas': 2582, 'wished': 2583, 'joanna': 2584, 'beating': 2585, 'mistakes': 2586, 'jerks': 2587, 'they’d': 2588, 'ballroom': 2589, 'whitney': 2590, 'mouse': 2591, 'terry': 2592, 'hopeless': 2593, 'stryker': 2594, 'miracle': 2595, 'tin': 2596, 'emotional': 2597, 'burned': 2598, 'intuitive': 2599, \"they'll\": 2600, 'stevie': 2601, 'backstage': 2602, 'smoking': 2603, '9': 2604, 'mouthing': 2605, \"could've\": 2606, 'beans': 2607, 'france': 2608, 'sunday': 2609, 'patch': 2610, 'lighter': 2611, 'thumb': 2612, 'knuckle': 2613, 'mel': 2614, 'aaron': 2615, 'himself': 2616, 'fanny': 2617, 'carrying': 2618, 'medical': 2619, 'terms': 2620, 'steal': 2621, 'laughed': 2622, 'blast': 2623, 'naps': 2624, 'jew': 2625, 'inspector': 2626, 'designers': 2627, 'laid': 2628, 'kissing': 2629, 'sweaty': 2630, 'subtle': 2631, 'drum': 2632, 'sticks': 2633, 'react': 2634, 'lilies': 2635, 'brownie': 2636, 'foosball': 2637, 'violated': 2638, 'lo': 2639, 'helen': 2640, 'photos': 2641, 'fiancee': 2642, 'scooch': 2643, 'teaches': 2644, 'annabelle': 2645, 'swimming': 2646, 'eggs': 2647, 'embryos': 2648, 'gumball': 2649, 'regatta': 2650, 'monana': 2651, 'canvas': 2652, 'monet': 2653, 'phonetically': 2654, 'memorize': 2655, 'carefully': 2656, '23': 2657, 'client': 2658, 'testify': 2659, 'owes': 2660, 'cursed': 2661, 'andie': 2662, 'mcdowell': 2663, 'jo’s': 2664, 'horoscope': 2665, 'lover’s': 2666, 'spat': 2667, 'psyched': 2668, 'landlord': 2669, \"makin'\": 2670, 'joy': 2671, 'practicing': 2672, 'seats': 2673, 'prove': 2674, 'oi': 2675, 'glen': 2676, 'fiancée': 2677, 'guide': 2678, 'roommates': 2679, 'boles': 2680, 'avoid': 2681, \"she'd\": 2682, 'attic': 2683, 'given': 2684, 'purpose': 2685, 'growing': 2686, \"y'see\": 2687, 'official': 2688, 'guilt': 2689, 'vibe': 2690, 'steak': 2691, 'u': 2692, 'overseas': 2693, '28': 2694, 'mirror': 2695, 'dictionary': 2696, 'barley': 2697, 'sophisticated': 2698, 'cereal': 2699, 'flicks': 2700, 'pushover': 2701, \"someone's\": 2702, 'albany': 2703, 'younger': 2704, 'taller': 2705, 'science': 2706, 'beef': 2707, 'wesley': 2708, 'stopped': 2709, 'worrying': 2710, \"'it's\": 2711, 'caller': 2712, 'alice': 2713, 'data': 2714, 'pleasant': 2715, 'knowing': 2716, 'ding': 2717, 'elaine': 2718, 'slight': 2719, 'integrity': 2720, 'whoo': 2721, 'drops': 2722, 'you…you': 2723, 'chatracus': 2724, 'slide': 2725, 'happiness': 2726, 'weak': 2727, 'martin': 2728, 'scorcese': 2729, 'sat': 2730, 'everyone’s': 2731, 'vision': 2732, 'recently': 2733, \"comin'\": 2734, 'staten': 2735, \"paolo's\": 2736, 'tasted': 2737, 'rhonda': 2738, 'scott': 2739, 'masculine': 2740, 'casting': 2741, 'tons': 2742, 'cuter': 2743, 'cleaner': 2744, 'err': 2745, 'restaurants': 2746, 'pre': 2747, 'liquor': 2748, 'loveable': 2749, 'thousand': 2750, 'expression': 2751, 'moms': 2752, 'ordinary': 2753, 'knives': 2754, 'ankle': 2755, 'hip': 2756, 'liam’s': 2757, 'reject': 2758, 'revenge': 2759, 'bouillabaisse': 2760, 'stare': 2761, 'speacial': 2762, 'crab': 2763, 'cakes': 2764, 'woowoo': 2765, \"ma'am\": 2766, 'cigarette': 2767, 'p': 2768, 'tar': 2769, 'pipe': 2770, 'dunno': 2771, 'station': 2772, 'hamburger': 2773, 'customers': 2774, 'morgan': 2775, 'cancellation': 2776, 'facility': 2777, 'chippy': 2778, 'jennifer': 2779, 'master': 2780, 'poet': 2781, 'shadow': 2782, 'lip': 2783, 'related': 2784, 'howdy': 2785, 'wa': 2786, 'whistle': 2787, 'comin’': 2788, 'bearnaise': 2789, 'convention': 2790, 'kate': 2791, 'miller': 2792, 'wrapped': 2793, 'female': 2794, 'setting': 2795, 'skip': 2796, 'peep': 2797, 'nose': 2798, 'grandfather': 2799, 'welch': 2800, 'mia': 2801, 'hamm': 2802, 'just—i': 2803, 'summer': 2804, 'cuba': 2805, 'northern': 2806, 'parade': 2807, 'hired': 2808, 'swans': 2809, 'lemme': 2810, 'messing': 2811, 'petrie': 2812, '717': 2813, 'sulk': 2814, 'huntley': 2815, 'scottish': 2816, 'trifle': 2817, 'bicep': 2818, 'memorial': 2819, 'salt': 2820, 'simmons': 2821, \"adam's\": 2822, 'company’s': 2823, 'transition': 2824, 'kl': 2825, 'gr': 2826, 'amount': 2827, 'detail': 2828, 'definite': 2829, 'confidence': 2830, 'chris': 2831, 'turning': 2832, 'betcha': 2833, \"'my\": 2834, 'vessel': 2835, 'emptier': 2836, 'interviewed': 2837, 'coached': 2838, 'ccan': 2839, 'ambulance': 2840, 'wander': 2841, 'nether': 2842, 'eternity': 2843, \"what've\": 2844, 'usual': 2845, 'aerobics': 2846, 'poster': 2847, \"'bout\": 2848, 'perk': 2849, 'village': 2850, 'ish': 2851, 'tomorow': 2852, 'seek': 2853, 'comfort': 2854, 'wry': 2855, 'respects': 2856, 'phobo': 2857, 'phewbedo': 2858, 'phaybobo': 2859, 'auditions': 2860, 'blinding': 2861, 'lifting': 2862, 'weights': 2863, 'passed': 2864, 'to—you': 2865, 'sticking': 2866, '15s': 2867, 'eighteenth': 2868, 'artifact': 2869, 'calcutta': 2870, 'so…here’s': 2871, 'homo': 2872, 'erectus': 2873, 'hum': 2874, 'keynote': 2875, 'speaker': 2876, 'clerk': 2877, 'academics': 2878, 'sand': 2879, 'flood': 2880, 'bash': 2881, 'lima': 2882, 'bean': 2883, 'bubbling': 2884, 'forth': 2885, 'erin': 2886, 'calms': 2887, 'peaceful': 2888, 'nodded': 2889, 'insecure': 2890, 'deaf': 2891, 'constantly': 2892, 'reassure': 2893, 'airplane': 2894, 'dwha': 2895, '‘see': 2896, 'saturday’': 2897, 'pa': 2898, 'haa': 2899, 'monger': 2900, 'nailed': 2901, 'foxtrot': 2902, 'tango': 2903, 'marge': 2904, 'wracking': 2905, 'callbacks': 2906, 'nose—i': 2907, 'concussion': 2908, 'sensing': 2909, 'controlled': 2910, 'glee': 2911, \"she'll\": 2912, \"david's\": 2913, 'recover': 2914, 'elizabeth’s': 2915, 'wh—excuse': 2916, 'matt': 2917, 'lauer’s': 2918, 'cleaned': 2919, 'ooooohh': 2920, 'scouts': 2921, 'camped': 2922, 'ooo': 2923, 'moral': 2924, 'obligation': 2925, 'oop': 2926, 'really…very': 2927, 'event': 2928, 'hilarious': 2929, 'hundreds': 2930, 'rode': 2931, 'faced': 2932, 'fears': 2933, 'ultimately': 2934, 'overcame': 2935, 'my…': 2936, 'options': 2937, 'prototypes': 2938, 'mushroom': 2939, 'cap': 2940, 'bologna': 2941, 'toothpicks': 2942, 'people’s': 2943, 'mashuga': 2944, 'tasty': 2945, 'adrienne’s': 2946, 'victor': 2947, 'onion': 2948, 'character’s': 2949, 'rookie': 2950, 'greg': 2951, 'jenny': 2952, 'souvenirs': 2953, 'poisoning': 2954, 'number’s': 2955, 'bazida': 2956, \"name's\": 2957, 'ronni': 2958, 'mortician': 2959, 'yard': 2960, 'ships': 2961, \"y'ever\": 2962, \"d'know\": 2963, \"y'haven't\": 2964, \"dad's\": 2965, \"'em's\": 2966, 'dick': 2967, 'guru': 2968, 'saj': 2969, 'shampoo': 2970, 'mesozoic': 2971, '21st': 2972, 'low': 2973, 'as…': 2974, 'courtside': 2975, 'anvil': 2976, 'aim': 2977, \"o'\": 2978, 'babes': 2979, 'immodest': 2980, 'whiff': 2981, 'frowned': 2982, 'upon': 2983, 'judgey': 2984, 'holierthanthou': 2985, 'reputation': 2986, 'mcnailshisstudents': 2987, 'block': 2988, 'thursday': 2989, 'keystone': 2990, 'caplin': 2991, 'wayne': 2992, 'rumors': 2993, 'stressed': 2994, 'swore': 2995, 'gods': 2996, 'sock': 2997, 'forcing': 2998, 'well…so': 2999, 'numb': 3000, 'nuts—oh': 3001, 'bobby’s': 3002, 'gangster': 3003, 'rap': 3004, '‘cause…': 3005, 'visiting': 3006, \"victoria's\": 3007, 'model': 3008, 'trapped': 3009, \"'hey\": 3010, 'og': 3011, 'ee': 3012, 'i’m…': 3013, 'mean…': 3014, 'snooty': 3015, 'stocked': 3016, 'donut': 3017, 'conclusion': 3018, 'joel': 3019, 'burg': 3020, 'briefcase': 3021, 'basis': 3022, 'freakish': 3023, 'hanger': 3024, 'batman’s': 3025, 'expressly': 3026, 'val': 3027, 'kilmer': 3028, 'film…that': 3029, 'batman': 3030, 'bond’s': 3031, 'so—if': 3032, 'ordained': 3033, 'weddings': 3034, 'clergy': 3035, 'passion': 3036, 'rubs': 3037, 'dot': 3038, 'ink': 3039, 'sofa': 3040, 'cushions': 3041, 'spaghetti': 3042, 'homeless': 3043, 'know—well': 3044, 'denise': 3045, 'secretly': 3046, 'hire': 3047, 'espresso': 3048, 'brutal': 3049, 'willing': 3050, 'travelling': 3051, 'handing': 3052, 'chickeeeen': 3053, \"jason's\": 3054, 'initiated': 3055, 'danger': 3056, 'waste': 3057, 'olivia': 3058, 'extras': 3059, 'scenes': 3060, 'christian': 3061, 'sanders': 3062, 'bitten': 3063, 'peacock': 3064, 'rsvp': 3065, 'veil': 3066, 'kurt': 3067, 'douglas': 3068, 'doooo': 3069, 'pep': 3070, 'rally': 3071, 'pyramid': 3072, 'anyway—umm': 3073, 'actually…': 3074, 'marijuana': 3075, 'cigarettes': 3076, 'dubbies': 3077, 'ripped': 3078, 'microphones': 3079, 'holocaust': 3080, 'earth': 3081, 'enhh': 3082, 'canned': 3083, 'dug': 3084, 'occasional': 3085, 'drink…ing': 3086, 'binge': 3087, 'position': 3088, 'buyer': 3089, 'effective': 3090, 'expense': 3091, \"grandma's\": 3092, 'cheeks': 3093, 'a…a': 3094, 'bossy': 3095, 'domineering': 3096, 'sloppy': 3097, 'immature': 3098, 'cousin': 3099, 'obsessive': 3100, 'voices': 3101, 'hamilton': 3102, 'rear': 3103, 'france…in': 3104, 'europe…western': 3105, 'backpacking': 3106, 'western': 3107, 'hmm—wait': 3108, 'studied': 3109, 'foothills': 3110, 'mount': 3111, 'tibidaybo': 3112, 'uh…we’re': 3113, 'umm…we’re': 3114, 'filter': 3115, 'tipped': 3116, 'error': 3117, 'credited': 3118, 'inconvenience': 3119, 'copies': 3120, 'really—i': 3121, 'panties': 3122, 'exotic': 3123, 'vermont': 3124, 'daiquiris': 3125, 'virgin': 3126, 'carpet': 3127, 'what—y’know': 3128, 'layman': 3129, 'arrogance': 3130, 'unable': 3131, 'to—i': 3132, 'couldn’t…': 3133, 'huh—hmm': 3134, 'incident': 3135, 'scale': 3136, 'dumbest': 3137, 'everybody’s': 3138, 'appointments': 3139, 'greasy': 3140, 'puzzle—beer': 3141, 'faeces': 3142, 'videos': 3143, 'museums': 3144, 'version': 3145, 'gooooood': 3146, 'racecar': 3147, 'velula': 3148, 'jester': 3149, 'wins': 3150, 'daydreaming': 3151, 'outer': 3152, 'taj': 3153, 'mahal': 3154, 'slots': 3155, 'grope': 3156, 'parallel': 3157, 'stairway': 3158, 'counting': 3159, 'lightning': 3160, 'begins—stop': 3161, '—now': 3162, 'hockey': 3163, 'claims': 3164, 'dangerous': 3165, 'liaisons': 3166, 'bernie’s': 3167, 'phase': 3168, 'realised': 3169, 'given’': 3170, 'topic': 3171, 'ruth': 3172, 'plain': 3173, 'disarray': 3174, 'flung': 3175, 'physically': 3176, 'depending': 3177, 'species': 3178, 'alumni': 3179, 'reads': 3180, 'fossil': 3181, 'samples': 3182, 'steps': 3183, 'entirely': 3184, 'giiiiiiift': 3185, 'drying': 3186, 'rod': 3187, 'horribly': 3188, 'traumatised': 3189, 'mommy': 3190, 'clock’s': 3191, 'ambitious': 3192, 'here—': 3193, 'pyjama': 3194, 'stating': 3195, 'ohhhhh': 3196, 'plague': 3197, 'houses': 3198, 'clawing': 3199, 'cats': 3200, 'umm…here': 3201, 'ooh—hey': 3202, 'donuts': 3203, 'odour': 3204, 'chinese': 3205, 'transit': 3206, 'authority': 3207, 'carseat': 3208, 'onto': 3209, 'stroller': 3210, 'introducing': 3211, 'applied': 3212, 'fierce': 3213, \"but—i'm\": 3214, 'applaud': 3215, 'intended': 3216, 'admitted': 3217, 'friendship': 3218, 'farrell': 3219, 'overheard': 3220, 'big……ot': 3221, 'bigot': 3222, 'tests': 3223, 'learners': 3224, 'essay': 3225, 'say…the': 3226, 'documents': 3227, 'toss': 3228, 'shedder': 3229, 'claim': 3230, 'mexico': 3231, 'national': 3232, 'sundays': 3233, 'danielle': 3234, 'telescope': 3235, 'creeps': 3236, 'spatters': 3237, 'watched': 3238, 'pot': 3239, 'beeps': 3240, 'hasn’t': 3241, '‘okay': 3242, 'pheebs’': 3243, 'saddest': 3244, 'camper': 3245, 'minimal': 3246, 'nuh': 3247, 'rethink': 3248, 'underwear—you': 3249, 'fabric': 3250, 'softener': 3251, 'adventure': 3252, 'delvecchio': 3253, 'imagination': 3254, 'split': 3255, 'hibachi': 3256, 'affect': 3257, 'form': 3258, 'grant': 3259, \"brother's\": 3260, 'accidentally': 3261, 'valuable': 3262, 'socks': 3263, 'ju': 3264, 'girlie': 3265, 'wired': 3266, 'stereo': 3267, 'disturbing': 3268, 'uh…how': 3269, 'belongs': 3270, 'peek': 3271, 'spoil': 3272, 'whazzup': 3273, 'molly': 3274, 'tellin’': 3275, 'complex': 3276, 'unlikely': 3277, 'geeky': 3278, 'older': 3279, 'vulnerability': 3280, 'factor': 3281, 'here—but': 3282, '29': 3283, 'savings': 3284, 'focusing': 3285, 'consider': 3286, 'length': 3287, 'rope': 3288, 'nouse': 3289, 'grace': 3290, 'rhyming': 3291, 'styles': 3292, 'beside': 3293, 'song’s': 3294, 'partners': 3295, '‘look': 3296, 'meaning’': 3297, 'pen': 3298, 'doodle': 3299, 'besides': 3300, 'busty': 3301, 'general': 3302, 'ment': 3303, 'brilliant': 3304, 'sincerely': 3305, 'wisely': 3306, 'radio’s': 3307, 'selfish': 3308, 'particular': 3309, 'superb': 3310, 'complaint': 3311, 'traditional': 3312, 'grilled': 3313, \"fixin's\": 3314, 'funyuns': 3315, \"havin'\": 3316, 'magnificent': 3317, 'vividly': 3318, 'mouthful': 3319, 'reverse': 3320, 'nana': 3321, \"'me\": 3322, 'java': 3323, \"joe's'\": 3324, '1939': 3325, '24': 3326, 'or…poison': 3327, 'involved': 3328, 'mature': 3329, 'category': 3330, 'returning': 3331, 'nominees': 3332, 'wheeler': 3333, 'aquarius': 3334, 'gemini': 3335, 'taurus': 3336, 'virgo': 3337, 'sagittarius': 3338, 'ahh…': 3339, 'meantime': 3340, 'bernard': 3341, 'hear——mother': 3342, 'contracting': 3343, 'wide': 3344, 'pelvis': 3345, 'unwitting': 3346, 'essence': 3347, 'anyone’s': 3348, 'crawl': 3349, 'meddled': 3350, 'according': 3351, 'ta': 3352, 'da': 3353, 'sparkly': 3354, 'wonerful': 3355, 'capades': 3356, 'wooo': 3357, 'matador': 3358, \"life's\": 3359, 'stupider': 3360, 'medieval': 3361, 'nighttime': 3362, 'antique': 3363, 'fortune': 3364, 'ridiculously': 3365, 'crystal': 3366, 'flennin': 3367, 'depressing': 3368, 'soft': 3369, 'pretzel': 3370, 'id': 3371, 'bras': 3372, \"do—y'know\": 3373, 'be—okay': 3374, 'jurassic': 3375, \"something's\": 3376, 'scarf': 3377, 'tucked': 3378, 'ewww': 3379, 'ezels': 3380, 'campaign': 3381, 'millionaire': 3382, 'caf': 3383, 'hazel': 3384, 'non': 3385, 'foam': 3386, 'missy': 3387, 'alternate': 3388, 'crawler': 3389, 'raised': 3390, 'awareness': 3391, 'angry': 3392, 'doorknob': 3393, 'grows': 3394, 'spring': 3395, 'adorable': 3396, 'equipped': 3397, 'destroy': 3398, 'fourteen': 3399, 'protected': 3400, 'tornado': 3401, 'dreamless': 3402, 'dirt': 3403, 'dumping': 3404, 'reality': 3405, 'artelle': 3406, 'blobbies': 3407, 'cleansing': 3408, 'firemen': 3409, \"son's\": 3410, \"susan's\": 3411, 'cramps': 3412, 'voulez': 3413, 'vous': 3414, 'coucher': 3415, 'avec': 3416, 'moi': 3417, 'ce': 3418, 'soir': 3419, 'cafe': 3420, 'maurice': 3421, 'fascinated': 3422, 'talky': 3423, 'golden': 3424, 'menu': 3425, 'mischa': 3426, 'silently': 3427, 'airhead': 3428, 'forgets': 3429, 'picture—i': 3430, 'churo': 3431, \"stain's\": 3432, 'paulette': 3433, 'vincent': 3434, 'soaked': 3435, 'thrilled': 3436, 'judy': 3437, 'diaphragm': 3438, 'chew': 3439, 'halloween': 3440, 'withdrawal': 3441, 'applying': 3442, 'grasp': 3443, '‘easy': 3444, 'tab’': 3445, 'zine': 3446, 'nighties': 3447, 'nerves': 3448, 'deadened': 3449, 'someone’s': 3450, 'haircut': 3451, 'thirty': 3452, 'guides': 3453, 'alley': 3454, 'wave': 3455, 'butternut': 3456, 'squash': 3457, 'squatternut': 3458, 'buash': 3459, 'reliving': 3460, 'indians': 3461, 'i—y’know': 3462, 'tad': 3463, 'edgy': 3464, 'parents’': 3465, 'ette': 3466, 'papers': 3467, 'chewed': 3468, 'trap': 3469, 'chewing': 3470, 'harassment': 3471, 'cases': 3472, 'urine': 3473, 'cuts': 3474, 'broiling': 3475, 'pan': 3476, 'caterpillars': 3477, 'william': 3478, 'sonoma': 3479, 'champ': 3480, 'battery': 3481, 'skimp': 3482, '112': 3483, '110': 3484, 'mingle': 3485, 'ease': 3486, 'solider': 3487, 'tommy': 3488, 'lobbing': 3489, 'yellow': 3490, 'method': 3491, 'chord': 3492, '7143457': 3493, 'layering': 3494, 'fabrics': 3495, 'colours': 3496, 'instance': 3497, 'schemp': 3498, 'deadline': 3499, 'fundamentally': 3500, 'unmarriable': 3501, 'parachute': 3502, 'knapsack': 3503, 'dressy': 3504, 'franzblau': 3505, 'merry': 3506, 'platform': 3507, 'millennium': 3508, '‘cut': 3509, 'update': 3510, '‘cos': 3511, 'stronger': 3512, 'scarves': 3513, 'tulip': 3514, 'tourists': 3515, 'poems': 3516, 'back—stop': 3517, 'mini': 3518, 'van': 3519, 'neutral': 3520, 'haul': 3521, '2030': 3522, \"there'll\": 3523, 'computers': 3524, 'functions': 3525, 'theoretically': 3526, 'download': 3527, 'interfacing': 3528, 'bumb': 3529, 'stays': 3530, \"bernoulli's\": 3531, 'principle': 3532, \"newton's\": 3533, \"mike's\": 3534, 'aaargh': 3535, 'slutty': 3536, 'lingerie': 3537, \"hurtin'\": 3538, 'skidmark': 3539, 'overdue': 3540, 'apology': 3541, 'overweight': 3542, 'ni': 3543, 'chou': 3544, 'stroke': 3545, 'tingly': 3546, 'hickey': 3547, 'speaks': 3548, 'i…i’m': 3549, 'held': 3550, 'um…but—okay': 3551, 'scores': 3552, 'winning': 3553, 'roast': 3554, 'pizzas': 3555, 'just—there’s': 3556, 'peeing': 3557, 'brush': 3558, 'lid': 3559, 'dokey': 3560, 'per': 3561, 'se': 3562, 'interactive': 3563, 'wildlife': 3564, 'animals': 3565, 'docile': 3566, 'cornered': 3567, 'objects': 3568, 'banana': 3569, 'whatcha': 3570, 'blade': 3571, \"diego's\": 3572, 'rabbit': 3573, 'gains': 3574, 'specimen': 3575, 'process': 3576, 'allow': 3577, 'haha': 3578, 'hyphen': 3579, 'bicentennial': 3580, 'ethan': 3581, 'felon': 3582, \"fallin'\": 3583, 'shiny': 3584, 'chad': 3585, 'miserable': 3586, 'seahorse': 3587, 'seahorses': 3588, 'toward': 3589, 'terrified': 3590, \"goofin'\": 3591, 'namesake': 3592, 'clamed': 3593, 'avoiding': 3594, '“aww': 3595, 'entering': 3596, 'puppies': 3597, 'wound': 3598, 'exposed': 3599, 'sexually': 3600, 'lover': 3601, 'satisfy': 3602, 'fulfill': 3603, 'eel': 3604, 'replica': 3605, 'sends': 3606, 'orbit': 3607, 'captain': 3608, 'clark': 3609, 'happier': 3610, 'ceremony': 3611, 'oh…wait…you': 3612, 'reinforcements': 3613, 'hitchhiker': 3614, 'rape—': 3615, 'tiffany': 3616, 'a—ah': 3617, 'pays': 3618, 'got…the…other': 3619, 'pl': 3620, 'this—ah': 3621, 'stripper’s': 3622, 'just—if': 3623, 'matches': 3624, 'timing': 3625, 'fires': 3626, 'gossiping': 3627, 'havin’': 3628, 'diets': 3629, 'moderation': 3630, 'understood': 3631, 'unbearable': 3632, 'tequila': 3633, 'partly': 3634, 'lesson': 3635, 'session': 3636, '1200': 3637, 'lycra': 3638, 'spandex': 3639, 'gym…treat': 3640, \"anyone's\": 3641, 'that…he': 3642, 'dared': 3643, 'briefs': 3644, 'that’s…kinda': 3645, 'flap': 3646, 'heels': 3647, 'posture': 3648, 'lumberjack': 3649, 'lacys': 3650, 'masculinity': 3651, 'lusts': 3652, 'i…it’s': 3653, 'nursing': 3654, 'emma': 3655, 'yowza': 3656, 'lord': 3657, 'that…': 3658, 'suity': 3659, 'pillman': 3660, 'counselor': 3661, 'added': 3662, 'but—come': 3663, 'vulnerable': 3664, 'cha': 3665, 'ching': 3666, 'sung': 3667, 'authorized': 3668, 'distribute': 3669, 'just—y’know': 3670, 'reciprocate': 3671, 'blessing': 3672, 'well…i': 3673, 'meaningless': 3674, 'surrounded': 3675, 'charm': 3676, 'mistaken': 3677, 'badly': 3678, 'that—it’s': 3679, 'convince': 3680, 'ravioli': 3681, 'supportive': 3682, 'opens': 3683, \"robbie's\": 3684, 'doin': 3685, 'snow': 3686, 'knick': 3687, 'ewing': 3688, 'rebuilding': 3689, 'waah': 3690, 'aa': 3691, 'costalano': 3692, 'choking': 3693, 'retainer': 3694, 'hating': 3695, 'sweating': 3696, 'punches': 3697, 'devil': 3698, 'satan': 3699, 'smock': 3700, 'robert’s': 3701, 'complementary': 3702, 'climbing': 3703, 'retiling': 3704, 'yo': 3705, 'spackel': 3706, 'lay': 3707, 'copying': 3708, 'issac’s': 3709, 'ing': 3710, 'philly': 3711, 'champagne': 3712, 'anniversaries': 3713, 'lasts': 3714, 'fantasy': 3715, 'fantasies': 3716, 'sour': 3717, 'sneak': 3718, 'cockpit': 3719, 'stewardess': 3720, 'trial': 3721, 'supplies': 3722, 'blouses': 3723, 'dresses': 3724, 'just……throw': 3725, 'purses': 3726, 'bloomingdale’s': 3727, 'okayyyyy': 3728, 'paint': 3729, 'sword': 3730, 'replace': 3731, 'baguette': 3732, 'balded': 3733, 'nnnnn': 3734, 'key’s': 3735, '98': 3736, '90': 3737, 'wh—no': 3738, 'tennille': 3739, 'reporter': 3740, 'influence…': 3741, 'dozens': 3742, 'case——yay': 3743, 'callin’': 3744, '92': 3745, 'wxrk': 3746, 'k': 3747, 'challenge': 3748, 'karen': 3749, 'tron': 3750, 'ditch': 3751, 'possibility': 3752, 'girlfriends': 3753, 'intellectual': 3754, 'missing': 3755, 'conquer': 3756, 'banned': 3757, '49': 3758, 'trainer': 3759, 'koon': 3760, 'doe': 3761, 'brazilian': 3762, 'octagon': 3763, 'designed': 3764, 'ruptured': 3765, 'doctor…': 3766, 'stu': 3767, 'joked': 3768, 'shows': 3769, 'morse': 3770, 'lied': 3771, 'risky': 3772, 'frozen': 3773, 'lake': 3774, 'mask': 3775, 'cameron': 3776, 'diaz': 3777, 'sabotage': 3778, 'jill’s': 3779, 'gate': 3780, 'packing': 3781, 'trench': 3782, 'bums': 3783, 'genuinely': 3784, 'fort': 3785, 'danced': 3786, 'how…how': 3787, 'fallen': 3788, 'apologizing': 3789, 'furnished': 3790, 'rage': 3791, '12': 3792, 'cookie': 3793, 'seventeen': 3794, '‘laser': 3795, 'floyd’': 3796, 'planetarium': 3797, 'occurred': 3798, '‘the': 3799, 'munchies': 3800, 'nyu': 3801, 'dorms': 3802, \"'cookie\": 3803, 'envelope': 3804, 'stuffers': 3805, 'stamp': 3806, 'lickers': 3807, 'resumes': 3808, 'soaps': 3809, 'shampoos': 3810, 'need——what': 3811, 'cake': 3812, 'murray': 3813, 'handed': 3814, 'yugoslavian': 3815, 'not—that’s': 3816, 'milan': 3817, 'respond': 3818, 'superiors': 3819, 'l': 3820, 'after—i': 3821, 'lyin’': 3822, 'triskaidekaphobia': 3823, 'edges': 3824, 'entitled': 3825, 'angel': 3826, 'losing': 3827, 'losers': 3828, 'spin': 3829, 'google': 3830, 'think—oh': 3831, 'gimmie': 3832, 'flirt': 3833, 'ewwuck': 3834, 'graceful': 3835, 'visit': 3836, 'dried': 3837, 'seashores': 3838, 'abandoner': 3839, 'licence': 3840, 'revoked': 3841, 'slips': 3842, 'cotton': 3843, 'lifeguards': 3844, 'dismantle': 3845, 'care—i': 3846, 'mean…i': 3847, 'offered': 3848, 'fran': 3849, 'wenus': 3850, 'whooooaaaa': 3851, 'picky': 3852, 'musta': 3853, 'grieve': 3854, \"where'd\": 3855, 'wholesale': 3856, 'socket': 3857, 'tinted': 3858, 'brownies': 3859, 'tooth': 3860, 'invisible': 3861, 'dentists': 3862, 'hygienist’s': 3863, '‘albino': 3864, 'bob’': 3865, 'oral': 3866, 'hygiene': 3867, 'floss': 3868, 'coincidence': 3869, 'dreadful': 3870, 'acted': 3871, 'england': 3872, 'celebrities': 3873, 'spots': 3874, 'forgiving': 3875, 'sarandon': 3876, 'political': 3877, 'donated': 3878, 'cans': 3879, 'geography': 3880, 'jetway': 3881, 'i—he': 3882, 'federal': 3883, 'regulations': 3884, 'beats': 3885, 'receive': 3886, 'doubling': 3887, 'bonus': 3888, '100': 3889, 'get—no': 3890, 'somehow': 3891, 'presenting': 3892, 'sarah': 3893, 'tuttle’s': 3894, 'mission': 3895, 'asteroid': 3896, '37135': 3897, 'fraid': 3898, \"kiddin'\": 3899, 'aaaah': 3900, 'wonderfulness': 3901, 'creepy': 3902, 'screwing': 3903, 'handshake': 3904, 'that—i': 3905, 'greely’s': 3906, 'theory': 3907, 'dominance': 3908, 'freeman': 3909, 'graduate': 3910, 'seminar': 3911, 'sounded': 3912, 'shmair': 3913, 'superstar': 3914, 'pouring': 3915, 'feasted': 3916, 'closet': 3917, 'don’t—whoa': 3918, 'crushing': 3919, 'bones': 3920, 'ouch': 3921, 'constructive': 3922, 'solution': 3923, 'chalk': 3924, 'lesbian': 3925, 'headin’': 3926, '1982': 3927, 'hardware': 3928, 'redo': 3929, 'highly': 3930, 'inch': 3931, 'glued': 3932, 'pry': 3933, 'gouged': 3934, 'surgeons': 3935, 'colon': 3936, 'tummy': 3937, 'purred': 3938, 'heck': 3939, 'the—that': 3940, '‘hi': 3941, 'homesick': 3942, 'lads': 3943, 'babe': 3944, 'scrum': 3945, 'gorgeous': 3946, 'kind’ve': 3947, 'bedrooms': 3948, 'baths': 3949, 'erwin’s': 3950, 'skinny': 3951, 'sunglasses': 3952, 'radio': 3953, 'holds': 3954, 'spaceship': 3955, 'stung': 3956, 'pill': 3957, 'good…pill': 3958, 'joshua’s': 3959, 'style—what': 3960, 'victorian': 3961, 'doorknobs': 3962, 'cupert': 3963, 'hewitt': 3964, 'dragged': 3965, 'ornate': 3966, 'ninth': 3967, 'finest': 3968, 'oak': 3969, 'east': 3970, 'mississippi': 3971, 'prettiest': 3972, 'lace': 3973, 'craftsmanship': 3974, 'gepeto': 3975, '300': 3976, 'changing': 3977, 'offer': 3978, 'profit': 3979, 'accountant': 3980, 'shark': 3981, 'crossword': 3982, 'dots': 3983, 'maroon': 3984, 'comedy': 3985, 'petite': 3986, 'wee': 3987, 'mute': 3988, 'jewelry': 3989, 'neurologist': 3990, 'comprehensive': 3991, 'overview': 3992, 'status': 3993, 'response': 3994, 'clifford': 3995, 'burnett': 3996, 'november': 3997, 'based': 3998, 'cliff': 3999, 'mathematician': 4000, 'widower': 4001, 'muriel': 4002, 'draddle': 4003, 'rhymes': 4004, 'pole': 4005, 'breaths': 4006, 'do…is': 4007, 'atmosphere': 4008, 'sidestep': 4009, '26': 4010, 'mazel': 4011, 'tov': 4012, 'uptown': 4013, 'was—if': 4014, 'racist': 4015, 'positive': 4016, 'trading': 4017, 'zinfandel': 4018, 'closest': 4019, 'angelica': 4020, 'bask': 4021, 'triumph': 4022, 'kissey': 4023, 'meaning': 4024, 'european': 4025, \"just—y'know—stop\": 4026, 'culture': 4027, 'puke': 4028, 'grabs': 4029, 'cheered': 4030, 'wisdom': 4031, 'you…we’re': 4032, 'wha—what': 4033, 'river': 4034, 'coast': 4035, 'coast’s': 4036, 'yo—you': 4037, 'else’s': 4038, 'ceilings': 4039, 'sage': 4040, 'branches': 4041, 'sacramental': 4042, 'oregano': 4043, 'semen': 4044, 'righteous': 4045, 'ritual': 4046, \"barry's\": 4047, 'adam': 4048, \"ritter's\": 4049, 'receipt': 4050, 'nokululu': 4051, 'oon': 4052, 'scotty': 4053, 'jared': 4054, \"paulo's\": 4055, 'grappa': 4056, 'salmon': 4057, 'sets': 4058, 'slash': 4059, 'bathtub': 4060, 'sooo': 4061, 'skedaddle': 4062, 'highness': 4063, 'loaner': 4064, 'helper': 4065, 'imaginary': 4066, 'lafite': 4067, 'nicely': 4068, 'damaged': 4069, 'zillionaire': 4070, 'dakota': 4071, 'zip': 4072, 'something’s': 4073, '“hey': 4074, 'nudes': 4075, '“nah': 4076, 'roller': 4077, 'shrink': 4078, 'stumble': 4079, 'item': 4080, 'j437': 4081, 'winterberry': 4082, 'we…we': 4083, 'herbal': 4084, 'savory': 4085, 'resist': 4086, 'brides': 4087, 'donna': 4088, 'carin': 4089, 'clubbing': 4090, 'imagining': 4091, 'guests': 4092, 'fireman': 4093, 'journal': 4094, 'drawings': 4095, 'colors': 4096, 'access': 4097, 'so’s': 4098, 'math': 4099, 'paths': 4100, 'stayed': 4101, 'random': 4102, 'intimidated': 4103, 'bore': 4104, 'manipulative': 4105, 'doors': 4106, \"just—i'll\": 4107, 'adjustment': 4108, 'progress': 4109, 'removal': 4110, 'glaucoma': 4111, 'haunt': 4112, 'step': 4113, 'cubby': 4114, 'max': 4115, 'snug': 4116, 'sergio': 4117, 'valente’s': 4118, 'delivered': 4119, 'buttery': 4120, 'crumbly': 4121, 'graham': 4122, 'cracker': 4123, 'filling…': 4124, 'filled': 4125, 'tempted': 4126, 'momma’s': 4127, 'bakery': 4128, 'scooter': 4129, 'hill': 4130, 'tracks': 4131, 'boobie': 4132, 'payback': 4133, 'express': 4134, 'silence': 4135, 'lambs': 4136, 'specifically': 4137, 'rubber': 4138, 'glue': 4139, \"whatever——can't\": 4140, \"yes'\": 4141, 'er': 4142, 'sandra’s': 4143, 'very—really': 4144, 'julie’s': 4145, 'dilated': 4146, 'fingers': 4147, 'compare': 4148, 'interrupting': 4149, 'fed': 4150, 'bridal': 4151, 'al': 4152, 'ruined': 4153, 'raining': 4154, 'bright': 4155, 'paleontologists': 4156, 'shirts': 4157, 'others': 4158, 'punched': 4159, 'clunked': 4160, 'massager': 4161, 'reach': 4162, 'originated': 4163, 'aroma': 4164, 'giraffe’s': 4165, 'pirate': 4166, 'foster': 4167, 'puppets': 4168, 'pete’s': 4169, 'interviewing': 4170, 'opponent': 4171, 'trains': 4172, 'iran': 4173, 'thieves': 4174, 'capable': 4175, 'burger': 4176, 'slap': 4177, 'conducting': 4178, 'experiment': 4179, 'smack': 4180, 'websites': 4181, 'website': 4182, 'unnecessary': 4183, 'government': 4184, 'tabs': 4185, 'branching': 4186, 'solidify': 4187, 'declare': 4188, 'bowmont’s': 4189, 'howie’s': 4190, 'editing': 4191, 'edit': 4192, 'spoken': 4193, 'refill': 4194, 'she—well': 4195, 'don': 4196, 'janet': 4197, 'officer—fireman': 4198, 'wrap': 4199, 'blanket': 4200, 'you—okay': 4201, 'warrant': 4202, 'reattach': 4203, 'disconnect': 4204, 'circle': 4205, 'hoop': 4206, \"phone's\": 4207, 'pointing': 4208, 'doorman': 4209, \"peeper's\": 4210, 'nngghhh': 4211, 'heating': 4212, 'radiator': 4213, 'rdtor': 4214, \"sidney's\": 4215, 'heyy': 4216, 'screen': 4217, 'actor’s': 4218, 'guild': 4219, 'residual': 4220, 'kinda…': 4221, 'expired': 4222, \"phoebe's\": 4223, 'boats': 4224, 'viking': 4225, 'moist': 4226, 'surefire': 4227, 'talker': 4228, 'tissues': 4229, 'freemont': 4230, 'west': 4231, 'westmont': 4232, 'westburg': 4233, 'drove': 4234, 'enema': 4235, 'look…of': 4236, 'off…wonderment': 4237, 'gaze': 4238, 'drowning': 4239, 'moron': 4240, 'guarantee': 4241, 'deals': 4242, 'locked': 4243, 'wh…': 4244, 'shook': 4245, 'seal': 4246, 'wo': 4247, 'meat': 4248, 'sweats': 4249, 'all…we’re': 4250, 'press': 4251, 'whoa—hey—wh': 4252, 'sliver': 4253, 'other’s': 4254, 'hometowns': 4255, 'classroom': 4256, 'students': 4257, 'picturing': 4258, 'candels': 4259, 'lit': 4260, \"belt's\": 4261, 'belt': 4262, 'midterms': 4263, 'countries': 4264, 'tone': 4265, 'to—whoa': 4266, 'dogs': 4267, 'lag': 4268, 'flies': 4269, 'angeles': 4270, 'loses': 4271, 'touching': 4272, 'cell': 4273, 'rang': 4274, 'features': 4275, 'handbag': 4276, 'puzzler': 4277, 'puzzle': 4278, 'contracts': 4279, 'genitals': 4280, 'phones': 4281, 'noisy': 4282, 'passin’': 4283, 'sex…and': 4284, 'rocked': 4285, 'globe': 4286, 'united': 4287, 'knitting': 4288, 'knit': 4289, 'careers': 4290, 'ashamed': 4291, 'advised': 4292, 'role': 4293, 'homosexually': 4294, 'auditioned': 4295, 'contract': 4296, 'luchhi': 4297, 'daytime': 4298, 'unexpected': 4299, 'uh…boy': 4300, 'nominated': 4301, 'simon': 4302, 'wait—oh—hey—huh': 4303, 'sorts': 4304, 'closes': 4305, 'realilized': 4306, 'duh': 4307, 'judgmental': 4308, 'destiny': 4309, 'fulfilled': 4310, 'ageist': 4311, 'fresh': 4312, 'chipper': 4313, 'nooooo': 4314, 'napping': 4315, 'whaddya': 4316, 'shy': 4317, 'preppy': 4318, 'understanding': 4319, 'flattery': 4320, 'dismiss': 4321, 'draft': 4322, 'wynona': 4323, 'rider': 4324, 'ironic': 4325, 'goofy': 4326, 'lurker': 4327, 'listener': 4328, 'trays': 4329, 'floopy': 4330, 'nonsense': 4331, 'english': 4332, 'permanent': 4333, 'organize': 4334, 'plural': 4335, 'allright': 4336, 'stable': 4337, 'obstacles': 4338, 'mediocre': 4339, 'zelner’s': 4340, 'lure': 4341, 'distract': 4342, 'anything—minute': 4343, 'finding': 4344, 'iron': 4345, 'steel': 4346, 'operators': 4347, 'schools': 4348, 'chip’s': 4349, 'motorcycle': 4350, 'letterman': 4351, 'wore': 4352, 'ec': 4353, 'mascot': 4354, 'mellow': 4355, 'hey—ooh': 4356, 'loosen': 4357, 'strings': 4358, 'mena': 4359, 'unattractive': 4360, 'laying': 4361, 'someday': 4362, 'bathrooms': 4363, 'spice': 4364, 'is—it’s': 4365, 'life…': 4366, 'threesome': 4367, 'diner': 4368, 'skates': 4369, 'cocoa': 4370, 'lumps': 4371, 'whipped': 4372, 'add': 4373, 'peas': 4374, 'onions': 4375, \"they'd\": 4376, 'inflatable': 4377, \"d'y'see\": 4378, 'yoghurt': 4379, 'containers': 4380, 'uncoordinated': 4381, 'adjoining': 4382, 'corneas': 4383, 'reserve': 4384, 'donate': 4385, 'fluids': 4386, 'cycles': 4387, 'applicants': 4388, 'studies': 4389, 'participate': 4390, 'effects': 4391, 'receptionists': 4392, 'potato': 4393, 'chips': 4394, 'oven': 4395, 'crumbies': 4396, 'monkey': 4397, 'wiped': 4398, 'biggie': 4399, 'platforms': 4400, 'taping': 4401, 'underpants': 4402, 'dancer': 4403, 'loosened': 4404, 'hips': 4405, 'rhythm': 4406, 'y’think': 4407, 'up’s': 4408, 'deliveries': 4409, 'description': 4410, 'patrick': 4411, 'ummmmmmmm': 4412, 'date—oh': 4413, 'dinner’s': 4414, 'fried': 4415, 'lot’s': 4416, 'inflate': 4417, 'flopping': 4418, 'cocktails': 4419, 'appalachia': 4420, 'tongue': 4421, 'ear': 4422, 'rambunctious': 4423, \"it's—not\": 4424, 'graduation': 4425, 'surrogate': 4426, 'you…are…not': 4427, 'cufflinks': 4428, 'pulse': 4429, 'saucy': 4430, 'electrical': 4431, \"nobody's\": 4432, 'judging': 4433, 'mister': 4434, 'pinching': 4435, 'explanation': 4436, 'pinchable': 4437, 'bulging': 4438, 'biceps—she': 4439, 'wrestle': 4440, 'manuver': 4441, 'hangs': 4442, 'poke': 4443, 'higher': 4444, 'angle': 4445, 'approaching': 4446, 'gonna—': 4447, 'pervert': 4448, 'i’m…i’m': 4449, 'pat': 4450, 'sajak': 4451, 'alex': 4452, 'trebek': 4453, 'woolery': 4454, 'eyelash': 4455, 'curler': 4456, 'oohh': 4457, 'oooooooooooooohhhhhhhhhhh': 4458, 'thomas': 4459, 'gobb': 4460, 'assign': 4461, 'tricks': 4462, 'convey': 4463, 'basic': 4464, '‘i': 4465, 'fishhook': 4466, 'eyebrow': 4467, 'it’': 4468, 'divide': 4469, '232': 4470, 'dismissed': 4471, 'trib': 4472, 'eric': 4473, 'winter': 4474, 'c’mon': 4475, 'azzz': 4476, 'bench': 4477, 'directly': 4478, 'assuming': 4479, 'nurturing': 4480, 'philadelphia': 4481, 'layers': 4482, 'if—no': 4483, 'licked': 4484, 'presumptuous': 4485, 'lame': 4486, 'and—oh': 4487, 'kyle’s': 4488, 'divert': 4489, 'listings': 4490, 'coma': 4491, 'ramoray’s': 4492, 'swallow': 4493, 'sonic': 4494, 'blaster': 4495, 'zone': 4496, 'lodged': 4497, 'throat': 4498, 'naturally': 4499, 'hurts': 4500, 'gellers': 4501, 'porsch': 4502, 'stories': 4503, 'neighbor': 4504, 'might’ve': 4505, 'chunk': 4506, 'newspaper': 4507, 'darlings': 4508, 'the——wowww': 4509, 'beyond': 4510, 'lasted': 4511, 'hanukkah': 4512, 'easter': 4513, 'bunny’s': 4514, 'funeral': 4515, 'lighting': 4516, 'japan': 4517, 'fictitious': 4518, 'agree': 4519, 'psychic': 4520, 'substantial': 4521, 'ridicule': 4522, 'fisher': 4523, \"how've\": 4524, 'firm': 4525, 'represents': 4526, 'fooled': 4527, 'cancer': 4528, 'emphysema': 4529, \"candle's\": 4530, '46': 4531, '47': 4532, 'bwah': 4533, 'definitive': 4534, 'mwwwooooo': 4535, 'crowd': 4536, 'see—darnit': 4537, 'shuffleboard': 4538, 'relatives': 4539, 'nesele': 4540, 'tolouse': 4541, 'nestley': 4542, 'toulouse': 4543, 'nestlé': 4544, 'toll': 4545, 'americans': 4546, 'butcher': 4547, 'language': 4548, 'cupboard': 4549, 'plaid': 4550, 'appealing': 4551, 'patches': 4552, 'cigs': 4553, 'contest': 4554, 'collect': 4555, 'flaw': 4556, 'constant': 4557, 'pronouncing': 4558, 'snort': 4559, 'laughs': 4560, 'flaws': 4561, 'chews': 4562, 'endearing': 4563, 'speaking': 4564, 'correctly': 4565, 'indeed': 4566, 'hooohhh': 4567, 'gloves': 4568, 'torme': 4569, 'paper’s': 4570, '‘probably’': 4571, 'hopes': 4572, 'yourselves': 4573, 'yoo': 4574, 'litman': 4575, 'neurolic': 4576, 'keeper': 4577, 'admire': 4578, 'raising': 4579, 'meets': 4580, 'i—that’s': 4581, 'tim': 4582, 'disgustingtons': 4583, 'wheeled': 4584, 'juicy': 4585, 'crowning': 4586, 'see—i': 4587, 'example': 4588, 'normally': 4589, 'sweatpants': 4590, 'triplets': 4591, 'thrice': 4592, 'randy': 4593, 'explains': 4594, 'santa’s': 4595, 'cardboard': 4596, 'cutout': 4597, 'evander': 4598, 'holyfield': 4599, 'bachelor': 4600, 'irreplaceable': 4601, 'generations': 4602, 'objectively': 4603, 'yeah—wait': 4604, '…a': 4605, 'slice……six': 4606, '94': 4607, 'humidity': 4608, 'dense': 4609, 'poetry': 4610, '350': 4611, 'na': 4612, 'windshield': 4613, 'mommies': 4614, 'ju——hi': 4615, 'eddie': 4616, 'moskowitz': 4617, 'reaffirms': 4618, 'faith': 4619, 'strain': 4620, 'straightener': 4621, 'unattended': 4622, 'is…': 4623, 'is…do': 4624, 'drift': 4625, 'bonding': 4626, 'belong': 4627, 'oranges': 4628, 'memorized': 4629, '1a': 4630, 'saying…': 4631, 'layer': 4632, 'crack': 4633, 'foundation': 4634, 'asbestos': 4635, 'ceiling': 4636, 'childhood': 4637, 'uncooked': 4638, 'batter': 4639, 'eater': 4640, 'unreasonable': 4641, 'rescue': 4642, 'resume': 4643, 'implementing': 4644, 'colored': 4645, 'labels': 4646, 'folders': 4647, 'brighten': 4648, 'cabinets': 4649, 'mentor': 4650, 'cultivating': 4651, 'relationships': 4652, 'you…know': 4653, 'geographically': 4654, 'ude': 4655, 'awhile': 4656, 'with…girls': 4657, 'defense': 4658, 'genie': 4659, 'brunch': 4660, 'deciding': 4661, 'disrupts': 4662, 'wrecking': 4663, 'phial': 4664, 'pox': 4665, 'release': 4666, 'hallway': 4667, 'throws': 4668, 'annoy': 4669, 'flower': 4670, 'startin’': 4671, 'faint': 4672, 'about……there': 4673, 'so…nice': 4674, 'subscription': 4675, 'roommate’s': 4676, 'protective': 4677, 'ah…': 4678, 'yours…is': 4679, 'kettle': 4680, 'differ': 4681, 'oooooh': 4682, 'raquetball': 4683, 'grip': 4684, 'ciao': 4685, 'bela': 4686, 'uhhhh': 4687, 'jockstrap': 4688, 'hel': 4689, \"'excuse\": 4690, 'blarrglarrghh': 4691, 'kiddin’': 4692, 'valentine’s': 4693, 'handbook': 4694, 'describe': 4695, 'naughty': 4696, 'taboo': 4697, 'december': 4698, 'photographer': 4699, 'jeffrey': 4700, 'kings': 4701, 'heavy': 4702, 'metal': 4703, 'carcass': 4704, '‘c’': 4705, '‘k’': 4706, 'kleinman’s': 4707, 'overpriced': 4708, 'schoolchildren': 4709, 'dangle': 4710, 'pressuring': 4711, 'testosteroney': 4712, 'francisco': 4713, 'interestingly': 4714, 'leaf': 4715, 'blower': 4716, 'spontaneous': 4717, 'phrase': 4718, 'shove': 4719, \"people'll\": 4720, \"guys'll\": 4721, 'scooching': 4722, 'riggs': 4723, 'willick': 4724, 'funnily': 4725, 'hop': 4726, 'patio': 4727, 'ladybugs': 4728, 'insects': 4729, 'suggest': 4730, 'dining': 4731, 'spraying': 4732, 'cologne': 4733, 'hottest': 4734, \"'cha\": 4735, 'waved': 4736, 'myth': 4737, 'lenny': 4738, 'believes': 4739, 'implantation': 4740, 'thick': 4741, 'now—go': 4742, 'prepare': 4743, 'embryossss': 4744, 'stairs': 4745, 'attach': 4746, '75': 4747, 'costs': 4748, 'that—okay': 4749, 'johnos': 4750, 'yum': 4751, 'efficient': 4752, 'condom': 4753, 'wallet': 4754, 'just…take': 4755, 'tanned': 4756, 'fluff': 4757, 'pillows': 4758, 'bills': 4759, 'helloooo': 4760, 'arrested': 4761, 'painterly': 4762, 'impasto': 4763, 'painted': 4764, 'outdoors': 4765, 'elusive': 4766, 'subject': 4767, 'monster': 4768, 'trucks': 4769, 'spelt': 4770, 'impressed': 4771, 'frontal': 4772, 'temporal': 4773, 'zygomatic': 4774, 'craniotomy': 4775, 'renaissance': 4776, 'caravaggio': 4777, 'uses': 4778, 'chiaroscuro': 4779, 'highlight': 4780, 'anguish': 4781, 'bumpy': 4782, 'libbing': 4783, 'thorough': 4784, '‘does': 4785, 'glance': 4786, 'overall': 4787, 'attorney': 4788, 'annulled': 4789, 'therapy': 4790, 'forms': 4791, 'witnesses': 4792, \"teachin'\": 4793, 'eyed': 4794, 'spends': 4795, 'leaves': 4796, \"mcdowell's\": 4797, 'rodney': 4798, 'nightmare': 4799, 'storming': 4800, 'manuscript': 4801, 'josephine': 4802, 'ninja': 4803, 'advantage': 4804, 'advance': 4805, 'twelfth': 4806, 'nineteenth': 4807, 'announces': 4808, 'crime': 4809, 'sidekick': 4810, 'lead': 4811, 'series': 4812, 'dreamed': 4813, 'preparing': 4814, 'technology': 4815, 'bactine': 4816, 'tooty': 4817, 'owner': 4818, \"'poor\": 4819, \"tooty'\": 4820, \"he'd\": 4821, 'trampled': 4822, 'steam': 4823, 'responsibility': 4824, 'that’ll': 4825, 'shhh': 4826, 'runnin’': 4827, '555': 4828, '9323': 4829, 'prom': 4830, 'product': 4831, 'violation': 4832, 'grief': 4833, 'illegally': 4834, 'subletting': 4835, 'matters': 4836, 'handsome': 4837, 'repeating': 4838, 'comic': 4839, 'confront': 4840, 'shall': 4841, 'intercourse': 4842, 'hike': 4843, 'toots': 4844, 'arrest': 4845, 'register': 4846, 'world’s': 4847, 'hangover': 4848, 'brooklyn': 4849, 'heights': 4850, 'cleveland': 4851, \"waitin'\": 4852, 'lorraine': 4853, \"yellin'\": 4854, 'christmastime': 4855, 'donations': 4856, 'spreading': 4857, 'spread': 4858, \"time's\": 4859, 'timer': 4860, 'brag': 4861, 'sore': 4862, 'dime': 4863, 'lint': 4864, 'canadian': 4865, 'coin': 4866, 'contribution': 4867, 'complaints': 4868, 'profile': 4869, 'hoops': 4870, 'trrrribbiani': 4871, 'audition’s': 4872, 'that’d': 4873, '437': 4874, 'shift': 4875, 'increase': 4876, 'rolling': 4877, 'referring': 4878, 'increasing': 4879, 'understaffed': 4880, 'appreciation': 4881, 'sanctity': 4882, 'defend': 4883, 'drain': 4884, 'ants': 4885, 'uff': 4886, 'thigh': 4887, 'theatre': 4888, 'multiplex': 4889, 'anytime': 4890, 'posters': 4891, 'taped': 4892, 'survived': 4893, 'fellas': 4894, 'instinct': 4895, 'risked': 4896, 'eez': 4897, 'carat': 4898, 'congress': 4899, 'debating': 4900, 'deficit': 4901, 'reduction': 4902, 'mayor': 4903, 'fares': 4904, 'teams': 4905, 'agamemnon': 4906, 'stares': 4907, 'knack': 4908, 'reacquainted': 4909, 'yeah—no': 4910, 'donald': 4911, 'trump': 4912, 'expert': 4913, 'torture': 4914, 'captured': 4915, 'girlfriend—which': 4916, 'shorten': 4917, 'muse': 4918, 'confusion': 4919, 'rare': 4920, 'bookstore': 4921, 'madison': 4922, 'architecture': 4923, \"patrick's\": 4924, 'cathedral': 4925, 'pastry': 4926, 'thai': 4927, 'antiquities': 4928, 'wing': 4929, 'ancient': 4930, 'egypt': 4931, 'mesopotamia': 4932, 'byzantine': 4933, 'critics': 4934, 'monogamy': 4935, 'carryon': 4936, 'impact': 4937, 'just…can’t': 4938, 'skating': 4939, 'shin': 4940, 'estate': 4941, 'dad’s': 4942, 'limb': 4943, 'is…oh': 4944, 'modeling': 4945, 'clap': 4946, 'tune': 4947, 'chorus': 4948, 'favored': 4949, 'unconsciously': 4950, 'marvel': 4951, 'grab…grab': 4952, 'sail': 4953, 'cheer': 4954, 'pony': 4955, 'count': 4956, 'fun’s': 4957, 'dehydrated': 4958, 'japanese': 4959, 'noodles': 4960, 'fluorescent': 4961, \"'co\": 4962, \"dependent'\": 4963, \"'self\": 4964, \"destructive'\": 4965, 'shelley': 4966, 'flush': 4967, 'bump': 4968, 'of…sexy': 4969, 'it—the': 4970, 'charisma': 4971, 'fail': 4972, 'midterm': 4973, 'you—i': 4974, 'blushed': 4975, 'toughest': 4976, \"'a\": 4977, 'protects': 4978, 'interests': 4979, '56': 4980, 'swimmers': 4981, 'sarah’s': 4982, 'tallies': 4983, 'ohhhh': 4984, 'debbie': 4985, '321': 4986, 'charla': 4987, '278': 4988, 'platonic': 4989, 'goof': 4990, 'term': 4991, 'dictionary…': 4992, 'technical': 4993, 'definition': 4994, 'fangled': 4995, 'original': 4996, 'dan': 4997, 'neighborhood': 4998, 'serves': 4999, \"woman's\": 5000, 'planet': 5001, 'yarns': 5002, 'weave': 5003, 'not—no': 5004, 'flick': 5005, 'jus': 5006, 'prison': 5007, 'bitches': 5008, 'hide': 5009, 'lunges': 5010, 'maintenance': 5011, 'flaky': 5012, 'lee': 5013, 'cranky': 5014, 'cuz': 5015, 'kiddy': 5016, 'invasive': 5017, 'vaginal': 5018, 'exam': 5019, 'suckers': 5020, 'mugsy': 5021, 'magical': 5022, 'regularly': 5023, 'scheduled': 5024, 'programming': 5025, 'wigging': 5026, 'luggage': 5027, 'canada': 5028, 'ill': 5029, 'monnnnn': 5030, 'felicity': 5031, 'unsuccessful': 5032, 'sized': 5033, 'tips': 5034, 'brady': 5035, 'ramoray': 5036, 'cole': 5037, 'slaw': 5038, 'buns': 5039, 'ground': 5040, 'formerly': 5041, 'cows': 5042, 'turkeys': 5043, 'senior': 5044, 'whoooaa': 5045, \"women's\": 5046, 'milwaukee': 5047, 'disneyland': 5048, '1989': 5049, 'mechanical': 5050, 'dutch': 5051, 'fixed': 5052, 'kingdom': 5053, 'patience': 5054, 'yore': 5055, 'yesteryear': 5056, 'redecorate—and': 5057, 'cheap': 5058, 'offs': 5059, 'junk': 5060, '60': 5061, 'colonial': 5062, 'the—okay': 5063, 'portion': 5064, 'rick': 5065, 'injure': 5066, 'greenpeace': 5067, 'crew': 5068, 'shoppin’': 5069, 'screaming': 5070, 'await': 5071, 'mri': 5072, 'scans': 5073, 'dna': 5074, 'gathered': 5075, 'simple': 5076, 'carbon': 5077, 'leathery': 5078, 'wrinkled': 5079, 'massaging': 5080, 'stimulate': 5081, 'flow': 5082, 'frustrating': 5083, 'lovable': 5084, 'continue': 5085, 'glare': 5086, 'streetlight': 5087, 'breadstick': 5088, 'fangs': 5089, 'advanced': 5090, 'paul’s': 5091, 'café': 5092, 'afterwards': 5093, 'sixteen': 5094, 'pound': 5095, 'childbirth': 5096, 'sections': 5097, 'stretching': 5098, 'co': 5099, \"star's\": 5100, 'lips': 5101, 'involve': 5102, 'projectile': 5103, 'vomiting': 5104, 'vail': 5105, \"guys'd\": 5106, 'syphilis': 5107, 'thankful': 5108, 'thanksgivings': 5109, 'sucked': 5110, 'lousy': 5111, 'crappy': 5112, 'bugger': 5113, 'ma’am': 5114, 'italy—please': 5115, 'strapping': 5116, 'jaw': 5117, \"blue's\": 5118, 'colour': 5119, \"'memo\": 5120, 'crash': 5121, 'embassy': 5122, 'parties': 5123, 'phobia': 5124, 'needles': 5125, 'phd': 5126, 'orally': 5127, 'circumstances': 5128, 'injection': 5129, 'tho': 5130, 'sting': 5131, 'well…let’s': 5132, 'balcony': 5133, 'up—i': 5134, 'checkbook': 5135, 'loyal': 5136, 'ummm': 5137, 'clogging': 5138, 'unclogging': 5139, 'didn’t—i': 5140, 'coveralls': 5141, \"girl's\": 5142, 'whoops': 5143, 'amanda': 5144, 'parent’s': 5145, 'salami': 5146, 'unhook': 5147, 'uh…': 5148, 'just—it’s': 5149, 'bouquet': 5150, 'moonlit': 5151, 'rue': 5152, 'sprinkle': 5153, 'rose': 5154, 'pedals': 5155, 'loooot': 5156, 'electricity': 5157, 'goal': 5158, 'no——one': 5159, 'gar’': 5160, 'hammered': 5161, 'shooting': 5162, 'fork': 5163, 'luckiest': 5164, 'luckier': 5165, 'wires': 5166, 'wire': 5167, 'disappears': 5168, 'baseboard': 5169, 'darling': 5170, 'up…and': 5171, 'teamwork': 5172, 'carl': 5173, 'gawd': 5174, 'pooper': 5175, 'within': 5176, 'coyotes': 5177, 'members': 5178, 'herd': 5179, 'slimmer': 5180, 'visited': 5181, 'nancy': 5182, 'thompson': 5183, 'blowing': 5184, 'budget': 5185, 'period': 5186, 'grandma’s': 5187, 'sleepy': 5188, 'sleeperson': 5189, 'remembering': 5190, 'nicknames': 5191, 'brownshirt': 5192, 'cutie': 5193, 'mcpretty': 5194, 'student': 5195, 'burt': 5196, 'rejections': 5197, \"'weren't\": 5198, 'believable': 5199, \"'pretty\": 5200, 'yummy': 5201, \"kissin'\": 5202, 'obsessed': 5203, 'restraining': 5204, 'orrr': 5205, 'fabutec': 5206, 'info': 5207, 'mercial': 5208, 'mop': 5209, 'waxing': 5210, 'organic': 5211, 'substances': 5212, 'discovered': 5213, 'depths': 5214, 'forest': 5215, \"'scuse\": 5216, 'sucking': 5217, 'corporate': 5218, 'varrrrrroom': 5219, 'varrrrrrrrrrom': 5220, 'varrrrrrrrroom': 5221, 'bed’s': 5222, 'upstate': 5223, 'cars': 5224, 'mercy': 5225, 'burring': 5226, 'pictures—are': 5227, 'rides': 5228, '‘where': 5229, 'barn': 5230, 'bend': 5231, 'outdoorsy': 5232, 'handy': 5233, 'win—ross': 5234, '1…2…3—go': 5235, 'jinx': 5236, 'meal': 5237, 'you’re—you': 5238, 'seller': 5239, 'suitcases': 5240, 'raisin': 5241, 'proves': 5242, 'harm': 5243, 'starlight': 5244, 'suicide': 5245, 'blueberry': 5246, 'muffin': 5247, 'cash': 5248, 'yearly': 5249, 'trips': 5250, 'rec': 5251, 'rustic': 5252, 'custom': 5253, 'waisted': 5254, 'duchess': 5255, 'gown': 5256, 'rack': 5257, 'phillips': 5258, 'unnatural': 5259, 'shed': 5260, 'separate': 5261, 'underneath': 5262, 'squat': 5263, 'ted': 5264, 'scares': 5265, 'teddy': 5266, 'andrew': 5267, 'pear': 5268, '‘i’': 5269, 'martini': 5270, 'chuckles': 5271, 'aboard': 5272, 'y—': 5273, 'ohhhhhhhh': 5274, 'steered': 5275, 'noticing': 5276, 'commercials': 5277, 'smacks': 5278, 'vacuum': 5279, 'polish': 5280, 'tip': 5281, 'stoop': 5282, 'bridge': 5283, \"talkin'\": 5284, 'liar': 5285, 'but…the': 5286, 'police': 5287, 'suspects': 5288, 'load': 5289, 'nor': 5290, 'i—i': 5291, 'good—ohh': 5292, 'blurtin’': 5293, 'curtain': 5294, 'hazard': 5295, 'safety': 5296, 'rub': 5297, 'gibson': 5298, 'clint': 5299, 'eastwood': 5300, 'tension': 5301, 'onstage': 5302, 'chemistry': 5303, 'benefit': 5304, 'erotic': 5305, 'wildly': 5306, 'unpopular': 5307, 'ph': 5308, 'fuzzy': 5309, 'mints': 5310, 'gross': 5311, \"'n'\": 5312, 'pets': 5313, 'occasionally': 5314, 'pose': 5315, 'tail': 5316, 'frisbee': 5317, \"ronni's\": 5318, 'tuxedos': 5319, 'approved': 5320, 'hookin’': 5321, 'attend': 5322, 'limited': 5323, 'seating': 5324, 'gellar': 5325, 'relative': 5326, 'fewer': 5327, 'bricks': 5328, 'musicians': 5329, 'bone': 5330, 'practical': 5331, 'monroe…': 5332, \"mark's\": 5333, 'becoming': 5334, 'confidant': 5335, 'complain': 5336, 'musical': 5337, 'visits': 5338, 'meanwhile': 5339, \"'maybe\": 5340, 'understands': 5341, \"'ohh\": 5342, \"'yes\": 5343, \"'wh\": 5344, \"'hhiii\": 5345, \"'i\": 5346, \"'man\": 5347, 'wise': 5348, 'geeks': 5349, 'anthropologists': 5350, 'dudes': 5351, 'don’t—maybe': 5352, 'hulk': 5353, 'mentioned': 5354, 'independent': 5355, 'chubby': 5356, 'puzzles': 5357, 'pbs': 5358, 'themselves': 5359, 'supporting': 5360, \"just—it's\": 5361, 'controversial': 5362, 'wallets': 5363, 'garden': 5364, 'gentleman': 5365, 'reacted': 5366, 'lunch…by': 5367, 'why’s': 5368, 'bummer': 5369, 'craziest': 5370, 'upset—for': 5371, 'endless': 5372, 'unfulfilling': 5373, 'exclusive': 5374, 'granted': 5375, 'settle': 5376, 'lane': 5377, 'cartoons': 5378, 'knife': 5379, 'slays': 5380, 'attacking': 5381, 'attacked': 5382, 'stopping': 5383, 'finishing': 5384, 'inflict': 5385, 'closely': 5386, 'devon': 5387, 'swift': 5388, 'bloke': 5389, 'knees': 5390, 'brolin': 5391, 'russell': 5392, 'assume': 5393, 'again—nooooooo': 5394, 'youngest': 5395, 'uh—how': 5396, 'vice': 5397, 'reconfiguration': 5398, 'statistical': 5399, 'factoring': 5400, 'companies': 5401, 'cooked': 5402, 'reviewer': 5403, 'admits': 5404, 'spit': 5405, 'cocky': 5406, 'shining': 5407, 'leak': 5408, 'luckily': 5409, 'settled': 5410, 'but—y’know': 5411, 'destined': 5412, 'soul': 5413, 'mate': 5414, 'girl’s': 5415, 'misses': 5416, \"her'\": 5417, 'sight': 5418, 'delicacy': 5419, 'moment’s': 5420, 'janice’s': 5421, '‘dear': 5422, 'hubba': 5423, 'bubba': 5424, 'birthday’': 5425, 'enemas': 5426, '‘mattress': 5427, 'king’': 5428, 'showroom': 5429, 'bambi': 5430, 'maaaaadd': 5431, 'plan’s': 5432, 'was…the': 5433, 'that…you': 5434, 'dances': 5435, 'super': 5436, 'electrician': 5437, 'shocks': 5438, 'responsible': 5439, 'folks': 5440, \"'euphoria\": 5441, \"unbound'\": 5442, 'nora': 5443, 'tyler': 5444, \"mom's\": 5445, 'passing': 5446, '79': 5447, \"'mistress\": 5448, 'cider': 5449, 'cinamon': 5450, 'eraser': 5451, 'busted': 5452, 'slammer': 5453, 'come—i': 5454, 'believe—i': 5455, 'nature’s': 5456, 'spectacular': 5457, 'phenomenon': 5458, 'right—hey': 5459, 'respected': 5460, 'astronomer': 5461, 'sting’s': 5462, 'pen…that': 5463, 'passageway': 5464, 'trudie': 5465, 'ben’s': 5466, 'mum': 5467, 'positions': 5468, 'hassidic': 5469, 'jewellers': 5470, 'softball': 5471, 'innate': 5472, 'ness': 5473, 'adore': 5474, 'personally': 5475, 'gallon': 5476, 'beginners': 5477, 'basics': 5478, 'upstage': 5479, 'downstage': 5480, 'snuggle': 5481, 'volcano': 5482, 'driving': 5483, 'bowl': 5484, 'further': 5485, \"man's\": 5486, 'crank': 5487, 'intrigued': 5488, 'fluid': 5489, 'darts': 5490, 'spackle': 5491, 'tiki': 5492, 'weeeell': 5493, 'rum': 5494, 'zero': 5495, 'frightening': 5496, \"tryin'\": 5497, 'suggested': 5498, 'is…is': 5499, 'somewhere…fun': 5500, 'mean—no': 5501, 'you—you': 5502, 'mozzarella': 5503, 'jalepino': 5504, 'poppers': 5505, 'barney': 5506, 'courthouse': 5507, 'women…': 5508, 'markson': 5509, 'fact—yes': 5510, 'brains': 5511, 'smeared': 5512, 'you—yeah': 5513, 'manhattan': 5514, 'stores': 5515, 'schmoon': 5516, 'transferring': 5517, 'rome': 5518, 'vienna': 5519, 'transferred': 5520, 'attract': 5521, 'stinkers': 5522, 'tedlock': 5523, 'kostelick': 5524, 'prank': 5525, 'memos': 5526, 'sweets': 5527, 'surprisingly': 5528, 'strict': 5529, 'too…': 5530, '…go': 5531, '—eh': 5532, 'classics': 5533, 'marshmellows': 5534, 'concentric': 5535, 'feature': 5536, 'mcclane': 5537, 'botched': 5538, 'operation': 5539, 'murder': 5540, 'punctuated': 5541, 'equilibrium': 5542, 'devonian': 5543, 'hooking': 5544, 'smarter': 5545, 'cracks': 5546, 'wink': 5547, 'apron': 5548, 'aprons': 5549, 'screening': 5550, 'groom—no': 5551, 'minor': 5552, 'setback': 5553, 'argument': 5554, 'las': 5555, 'dice': 5556, \"nothin'\": 5557, 'guys’': 5558, 'i—rachel': 5559, 'deserves': 5560, 'informed': 5561, 'ohh—do': 5562, 'hydrosaurids': 5563, 'unearthed': 5564, 'main': 5565, 'locations': 5566, 'hydrosaurs': 5567, 'beth': 5568, 'loreo': 5569, 'well…did': 5570, 'just…i’ll': 5571, 'unfortunate': 5572, 'weight': 5573, 'levels': 5574, 'adiós': 5575, 'beaudalire': 5576, \"translation's\": 5577, 'ferry': 5578, 'nova': 5579, 'scotia': 5580, 'these’ll': 5581, 'orange': 5582, 'demeaning': 5583, 'remotely': 5584, 'yentel': 5585, 'dowdy': 5586, 'dues': 5587, 'mira': 5588, 'arthritic': 5589, 'seamstress': 5590, 'misery': 5591, 'amusing': 5592, 'mannequin': 5593, 'mannequins': 5594, 'pickle': 5595, \"yesterday's\": 5596, 'potential': 5597, 'hearts': 5598, 'plunge': 5599, 'pit': 5600, 'depression': 5601, 'momma': 5602, 'alessandro’s': 5603, 'spoon': 5604, 'torn': 5605, 'desire': 5606, 'abysmal': 5607, 'thata': 5608, 'introduction': 5609, 'hollandaise': 5610, 'yolk': 5611, 'shallots': 5612, 'shirvel': 5613, 'tarragon': 5614, 'argentinaaaa': 5615, 'seeeeen': 5616, 'fifty': 5617, 'pesos': 5618, 'spleeeen': 5619, 'olé': 5620, 'entertain': 5621, 'requesting': 5622, 'bored': 5623, \"hasn't\": 5624, 'pharmaceutical': 5625, 'pharmacists': 5626, 'medeio': 5627, 'eva': 5628, 'trorro': 5629, 'womba': 5630, 'fringe': 5631, 'sides': 5632, 'goodnight': 5633, 'readers': 5634, 'mikey': 5635, 'rattraps': 5636, 'capture': 5637, 'possom': 5638, 'wisecracking': 5639, 'owl': 5640, 'traps': 5641, \"bob's\": 5642, 'suzie': 5643, \"ladies'\": 5644, 'rejection': 5645, 'waited': 5646, 'puts': 5647, 'bath': 5648, 'loong': 5649, 'carried': 5650, 'breadsticks': 5651, 'marinara': 5652, 'vodka': 5653, 'celery': 5654, 'lebanon': 5655, 'nasty': 5656, 'hanks': 5657, 'meg': 5658, 'ryan': 5659, 'bullets': 5660, 'guns': 5661, 'slower': 5662, 'thing…': 5663, 'bleeding': 5664, 'crotch': 5665, 'umm…oh…who’s': 5666, 'soccer': 5667, 'player': 5668, 'that…hot': 5669, 'pinned': 5670, 'tickled': 5671, 'freckles': 5672, 'muscle': 5673, 'plinky': 5674, 'plunky': 5675, \"richard's\": 5676, 'meadow': 5677, 'breeze': 5678, 'moonlight': 5679, 'flowing': 5680, 'trees': 5681, 'wagering': 5682, 'permitted': 5683, 'waterfalls': 5684, 'trickling': 5685, 'fountains': 5686, 'calming': 5687, 'babbling': 5688, 'brook': 5689, 'awake': 5690, \"somethin'\": 5691, 'afternoons': 5692, 'stephanie': 5693, 'swirl': 5694, 'strippers': 5695, 'exit': 5696, 'awwwww': 5697, 'competitor': 5698, 'll': 5699, 'queens': 5700, 'traveled': 5701, 'southern': 5702, 'bees': 5703, 'solved': 5704, 'grocery': 5705, 'or—oh': 5706, 'linda': 5707, 'camp': 5708, 'tradition': 5709, 'abandoned': 5710, 'couples': 5711, 'braverman': 5712, 'protecting': 5713, 'kicky': 5714, 'pancakes': 5715, 'distance': 5716, \"father's\": 5717, 'fitting': 5718, 'cave': 5719, 'courage': 5720, 'fettuccini': 5721, 'alfredo': 5722, 'bubbles': 5723, 'perceptive': 5724, 'freaks': 5725, 'sending': 5726, 'cruise': 5727, 'razor': 5728, 'stitches': 5729, 'realises': 5730, 'sister’s': 5731, 'teeny': 5732, 'comprehends': 5733, 'thousands': 5734, 'miles': 5735, 'disappointed': 5736, 'decent': 5737, 'booked': 5738, 'postponing': 5739, 'option': 5740, '‘or’': 5741, 'backwards': 5742, 'please—': 5743, 'refer': 5744, 'bobo': 5745, 'sullies': 5746, 'leon': 5747, 'purchasing': 5748, 'kidney': 5749, \"guys'\": 5750, 'gotcha': 5751, '87': 5752, 'vcr': 5753, 'penis': 5754, 'bleaker': 5755, 'mama': 5756, \"lisettie's\": 5757, 'enh': 5758, 'that—this': 5759, '11': 5760, 'code': 5761, 'requires': 5762, 'material': 5763, 'convenience': 5764, 'warning': 5765, 'bust': 5766, 'vendor': 5767, 'picks': 5768, 'towels': 5769, 'loaned': 5770, 'nineteen': 5771, 'nimitz': 5772, 'interviews': 5773, 'napkin': 5774, 'greeting': 5775, 'charred': 5776, 'picture—wow': 5777, 'hairier': 5778, 'chief': 5779, 'bonfire': 5780, \"valentine's\": 5781, 'busiest': 5782, \"'pl'\": 5783, \"'g\": 5784, 'stephanopoulos': 5785, 'mediterranean': 5786, 'curiously': 5787, 'intelligent': 5788, \"stephanopoulos'\": 5789, 'snuffalopagus': 5790, \"bird's\": 5791, 'spying': 5792, 'rainy': 5793, 'allll': 5794, 'boredddd': 5795, 'choices': 5796, 'printed': 5797, 'winona': 5798, 'ryder': 5799, 'michelle': 5800, 'pfieffer': 5801, 'dorothy': 5802, 'hammel': 5803, 'spins': 5804, 'fonzie': 5805, 'resident': 5806, \"should've\": 5807, 'specified': 5808, 'needing': 5809, 'qualified': 5810, 'doogie': 5811, 'askin’': 5812, 'officer…pretty': 5813, 'petty': 5814, 'margaritas': 5815, 'weekly': 5816, 'gerston': 5817, 'santos': 5818, 'pleased': 5819, 'pickles': 5820, 'tampons': 5821, 'penny': 5822, 'stock': 5823, 'tow': 5824, 'innocent': 5825, 'victim': 5826, 'punish': 5827, 'bound': 5828, 'rights': 5829, 'minks': 5830, \"i—it's\": 5831, 'sits': 5832, 'straighten': 5833, 'via': 5834, 'burritos': 5835, 'reservation': 5836, 'burrito': 5837, 'shakin’': 5838, \"o'mally\": 5839, 'quicker': 5840, 'required': 5841, 'evaluate': 5842, 'gel': 5843, 'cowlicky': 5844, 'sweepin’': 5845, 'chimney': 5846, 'vicar': 5847, 'zelda': 5848, 'little…erotica': 5849, 'sexuality': 5850, 'shortbread': 5851, 'scotland': 5852, 'by…vikings': 5853, 'rediscover': 5854, 'roots': 5855, 'bagpipes': 5856, 'personalities': 5857, 'pillow': 5858, 'string': 5859, 'doggy': 5860, 'kitten': 5861, 'but—bye': 5862, 'janine’s': 5863, 'dancers': 5864, 'alot': 5865, 'whippin’': 5866, '“oh': 5867, 'swedish': 5868, 'opinion': 5869, 'foxy': 5870, 'sexless': 5871, 'misunderstand': 5872, 'flexed': 5873, 'concert': 5874, 'allan': 5875, 'planner': 5876, 'theta': 5877, 'beta': 5878, 'pi': 5879, 'syracuse': 5880, 'repeat': 5881, 'closets': 5882, 'here—do': 5883, 'blacken': 5884, 'edge': 5885, 'forehead': 5886, 'pencil': 5887, 'anybody’s': 5888, 'hoshi': 5889, 'ringside': 5890, 'affected': 5891, 'concentration': 5892, 'bout': 5893, 'zana': 5894, 'spindler': 5895, 'devane': 5896, 'kelly': 5897, 'goldie': 5898, 'sulkov': 5899, 'mcguire': 5900, 'beardsley': 5901, 'dorfman': 5902, 'wedgie': 5903, 'architect': 5904, 'fiction': 5905, 'writers': 5906, 'even…': 5907, 'stanley': 5908, 'leading': 5909, 'temporary': 5910, 'invest': 5911, 'nostril': 5912, 'sneezed': 5913, 'bats': 5914, 'flew': 5915, \"tellin'\": 5916, 'leaned': 5917, 'perfectly': 5918, 'superficial': 5919, 'insignificant': 5920, 'apples': 5921}\n"
     ]
    }
   ],
   "source": [
    "# train 단어 집합 매핑 확인\n",
    "\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0546f850",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "train에 존재하는 단어 수는 약 6000개 정도임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7fabb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### train, test 문장들을 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c07c011",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(train['Utterance'])\n",
    "X_test = tokenizer.texts_to_sequences(test['Utterance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22021cae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[428, 1, 34, 3, 753, 429, 28, 19, 2823, 2824, 127, 3, 2825, 1134, 4, 2826, 883, 884], [2, 885, 101, 39, 690, 802], [9, 1, 73, 9, 1, 73]]\n",
      "[[72, 24, 31, 3, 326, 1384, 22], [7, 78, 15, 124, 51, 1024, 20, 14, 155, 63, 61, 28, 129, 12, 3755, 74, 51, 102, 4, 2219, 146, 12, 3, 452, 1862], [2, 29, 11]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:3])\n",
    "print(X_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d9f263",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "정수 인코딩 과정 중 빈 리스트로 변한 문장은 없는지 확인:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ecc934f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 112\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test 결측치 확인\n",
    "\n",
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
    "drop_test = [index for index, sentence in enumerate(X_test) if len(sentence) < 1]\n",
    "print(len(drop_train), len(drop_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ab17f4a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 Quoi?\n",
      "62 Bumpys\n",
      "111 Oui.\n",
      "127 Hé!\n",
      "200 Hé!\n",
      "205 Quoi?\n",
      "270 Quoi?\n",
      "381 Quoi?\n",
      "395 Diane Keaton.\n",
      "401 Merci!\n",
      "423 Hé!\n",
      "444 Hé!\n",
      "456 Hé!\n",
      "489 Quoi?!\n",
      "511 Gin.\n",
      "517 Hé!\n",
      "518 Hé!\n",
      "549 Hose.\n",
      "550 Hé!\n",
      "565 Quoi?!\n",
      "601 Quoi?\n",
      "616 Merci.\n",
      "622 Hein?\n",
      "738 WOOOO HOOOO !!!\n",
      "767 Shhhh!\n",
      "793 Arghh !!\n",
      "807 Quoi?\n",
      "825 Hé!\n",
      "834 Merci!\n",
      "846 «Kay?\n",
      "902 Leslie?\n",
      "905 Hé!\n",
      "906 Hose.\n",
      "919 Hé!\n",
      "920 Hose.\n",
      "944 Quoi?\n",
      "992 Quoi?!\n",
      "1001 Quoi?\n",
      "1015 Quoi?\n",
      "1053 Merci.\n",
      "1116 Il\n",
      "1141 Hose.\n",
      "1146 Bonjour?\n",
      "1153 Canceled?!\n",
      "1189 Quoi?\n",
      "1219 Earl Grey?\n",
      "1222 Hé!\n",
      "1241 Mains.\n",
      "1242 Mains!\n",
      "1246 816, merci!\n",
      "1247 Merci.\n",
      "1273 Oui!\n",
      "1278 Hé!\n",
      "1279 Hé!\n",
      "1284 Mec!Mec!\n",
      "1295 Quoi?!\n",
      "1300 Hé!\n",
      "1312 Hein?\n",
      "1334 Quoi?\n",
      "1336 Quoi?!\n",
      "1351 Stuart!\n",
      "1374 Exclude!!\n",
      "1396 Quoi?\n",
      "1487 Hé!\n",
      "1504 Ahhhh !!\n",
      "1513 Hé!\n",
      "1570 Diverse\n",
      "1581 Robust boys?\n",
      "1594 Hé!\n",
      "1647 Bonjour.\n",
      "1723 Hose !!\n",
      "1752 Oui.\n",
      "1757 Donkey?Donkey?Donkey?\n",
      "1776 Quoi?\n",
      "1777 Fishing core.\n",
      "1780 -Peaches?\n",
      "1786 Westminster Abbey!\n",
      "1788 Hé!\n",
      "1794 Homme,\n",
      "1801 Quoi?\n",
      "1895 Hé!\n",
      "1902 Quoi?!\n",
      "1914 Hé!\n",
      "1936 Quoi?\n",
      "1959 Hose.\n",
      "1983 Quoi?!\n",
      "2018 Hé!\n",
      "2081 Deeper.\n",
      "2082 Deeper.\n",
      "2083 Deeper.\n",
      "2118 Quoi?!\n",
      "2146 Hé!\n",
      "2147 Hé!\n",
      "2167 Poisson!\n",
      "2186 Hein?\n",
      "2190 Hé!\n",
      "2195 Mmmm.bien.\n",
      "2218 Ginger.\n",
      "2222 Oui.\n",
      "2261 Voilà!\n",
      "2273 Hose.\n",
      "2274 Hose.\n",
      "2287 Quoi?\n",
      "2293 Hé!\n",
      "2338 Relieved?\n",
      "2389 Rossy, Rossy.\n",
      "2444 Oui!\n",
      "2448 Merci!\n",
      "2473 Hose.\n",
      "2510 Quoi?!\n",
      "2514 S'amuser.\n",
      "2575 Hose.\n"
     ]
    }
   ],
   "source": [
    "# test에 존재하는 빈 리스트로 인코딩된 문장 확인\n",
    "for i in drop_test:\n",
    "    print(i, test['Utterance'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54132c4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 중요 사실  \n",
    "test 데이터에 **영어가 아닌 프랑스어, 스페인어**로 추정되는 표현이 들어있음!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5fb344",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 정수 인코딩된 train 문장의 길이 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcb98456",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 길이: 69\n",
      "평균 길이: 8.061767944739213\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 최대, 평균 길이 파악 후 히스토그램으로 시각화\n",
    "\n",
    "print('최대 길이:', max(len(review) for review in X_train))\n",
    "print('평균 길이:', sum(map(len, X_train))/len(X_train))\n",
    "\n",
    "plt.hist([len(review) for review in X_train], bins=50)\n",
    "plt.xlabel('length of sequences')\n",
    "plt.ylabel('num counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d80f2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 정수 인코딩된 test 문장의 길이 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f3be172",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 길이: 39\n",
      "평균 길이: 7.875095785440613\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 최대, 평균 길이 파악 후 히스토그램으로 시각화\n",
    "\n",
    "print('최대 길이:', max(len(review) for review in X_test))\n",
    "print('평균 길이:', sum(map(len, X_test))/len(X_test))\n",
    "\n",
    "plt.hist([len(review) for review in X_test], bins=50)\n",
    "plt.xlabel('length of sequences')\n",
    "plt.ylabel('num counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f1fd3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 전처리에 사용할 다양한 토크나이저 작동방식 비교  \n",
    "Modeling에서 BERT 계열 모델을 사용할 예정이라 BERT 계열 토크나이저들의 작동방식을 비교하고자 함  \n",
    "train에 존재하는 문장 중 하나를 골라 sample로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a125d2f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sample: train에 존재하는 문장\n",
    "\n",
    "sample = \"What?! What is with everybody? It's Thanksgiving, not...Truth-Day!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f241719e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### BERT 계열 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0adf7543",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', '?', '!', 'What', 'is', 'with', 'everybody', '?', 'It', \"'\", 's', 'Thanksgiving', ',', 'not', '.', '.', '.', 'Truth', '-', 'Day', '!'] \n",
      "\n",
      "{'input_ids': [101, 1327, 136, 106, 1327, 1110, 1114, 10565, 136, 1135, 112, 188, 17148, 117, 1136, 119, 119, 119, 9907, 118, 2295, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} \n",
      "\n",
      "[CLS] What?! What is with everybody? It's Thanksgiving, not... Truth - Day! [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 'bert-base-cased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# 토크나이저 작동방식 확인\n",
    "result = tokenizer.tokenize(sample)\n",
    "print(result, '\\n')\n",
    "encoded = tokenizer(sample)\n",
    "print(encoded, '\\n')\n",
    "decoded = tokenizer.decode(encoded['input_ids'])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "650ce13c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', '?', '!', 'what', 'is', 'with', 'everybody', '?', 'it', \"'\", 's', 'thanksgiving', ',', 'not', '.', '.', '.', 'truth', '-', 'day', '!'] \n",
      "\n",
      "{'input_ids': [101, 2054, 1029, 999, 2054, 2003, 2007, 7955, 1029, 2009, 1005, 1055, 15060, 1010, 2025, 1012, 1012, 1012, 3606, 1011, 2154, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} \n",
      "\n",
      "[CLS] what?! what is with everybody? it's thanksgiving, not... truth - day! [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 토크나이저 작동방식 확인\n",
    "result = tokenizer.tokenize(sample)\n",
    "print(result, '\\n')\n",
    "encoded = tokenizer(sample)\n",
    "print(encoded, '\\n')\n",
    "decoded = tokenizer.decode(encoded['input_ids'])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de1305",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### cased vs uncased\n",
    "위 출력결과를 보면 input_ids의 번호 종류, 그리고 다시 복원했을 때의 대소문자에서 차이가 있음!  \n",
    "\n",
    "cased는 대소문자 그대로 둠, uncased는 모두 소문자로 통일  \n",
    "같은 단어를 같다고 인식하기 위해선 uncased가 더 좋음(실제로 예측 성능 결과도 더 좋았음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "434729ea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', '?', '!', 'what', 'is', 'with', 'everybody', '?', 'it', \"'\", 's', 'thanksgiving', ',', 'not', '.', '.', '.', 'truth', '-', 'day', '!'] \n",
      "\n",
      "{'input_ids': [101, 2054, 1029, 999, 2054, 2003, 2007, 7955, 1029, 2009, 1005, 1055, 15060, 1010, 2025, 1012, 1012, 1012, 3606, 1011, 2154, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} \n",
      "\n",
      "[CLS] what?! what is with everybody? it's thanksgiving, not... truth - day! [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 'bert-large-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "\n",
    "# 토크나이저 작동방식 확인\n",
    "result = tokenizer.tokenize(sample)\n",
    "print(result, '\\n')\n",
    "encoded = tokenizer(sample)\n",
    "print(encoded, '\\n')\n",
    "decoded = tokenizer.decode(encoded['input_ids'])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4de5a6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### base-uncased vs large-uncased  \n",
    "토큰화 위치, 부여 번호('input_ids'), 복원했을 때의 문장에서 차이 없었음  \n",
    "그러나 아래와 같은 차이가 존재(https://iq.opengenus.org/bert-base-vs-bert-large/)  \n",
    "- encoder 층 개수 차이: base(12 layers), large(24 layers)  \n",
    "층이 늘어난 만큼 attention head와 parameter(가중치) 개수도 늘어남  \n",
    "- attention head 수 차이: base(12개), large(16개)  \n",
    "- parameter 수 차이: base(1억1천만, 110 million), largea(3억4천만, 340 million)\n",
    "- hidden layer 수 차이: base(768 layers), large(1024 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "572c6340",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', '?', '!', 'what', 'is', 'with', 'everybody', '?', 'it', \"'\", 's', 'thanksgiving', ',', 'not', '.', '.', '.', 'truth', '-', 'day', '!'] \n",
      "\n",
      "{'input_ids': [101, 2054, 1029, 999, 2054, 2003, 2007, 7955, 1029, 2009, 1005, 1055, 15060, 1010, 2025, 1012, 1012, 1012, 3606, 1011, 2154, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} \n",
      "\n",
      "[CLS] what?! what is with everybody? it's thanksgiving, not... truth - day! [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 'bert-large-uncased-whole-word-masking'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased-whole-word-masking')\n",
    "\n",
    "# 토크나이저 작동방식 확인\n",
    "result = tokenizer.tokenize(sample)\n",
    "print(result, '\\n')\n",
    "encoded = tokenizer(sample)\n",
    "print(encoded, '\\n')\n",
    "decoded = tokenizer.decode(encoded['input_ids'])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66d545",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### uncased vs uncased-whole-word-masking\n",
    "이 역시 서로 토큰화 위치, 부여 번호('input_ids'), 복원했을 때의 문장에서 차이 없었음  \n",
    "그러나 마스킹하는 방식에 있어서 차이가 존재!  \n",
    "- 지금까지의 토크나이저의 경우 마스킹이 한 단어 내에서도 일부만 가리는 식으로 위치할 수 있는데, whole-word-masking에서는 한 단어로 되어 있는 것은 전부 마스킹 처리가 된다는 차이가 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d9982",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### EmoBERTa 토크나이저  \n",
    "EmoBERTa: pre-trained \"roberta-large\" 모델에서 시작한 토크나이저로, 대화 상황의 텍스트를 사전학습하여 담화 내에서의 감정 변화 및 분석에 특화된 토크나이저  \n",
    "(참고 논문: https://arxiv.org/pdf/2108.12009.pdf)\n",
    "\n",
    "base와 large의 차이는 앞서 BERT와 동일!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa0c8731",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', '?!', 'ĠWhat', 'Ġis', 'Ġwith', 'Ġeverybody', '?', 'ĠIt', \"'s\", 'ĠThanksgiving', ',', 'Ġnot', '...', 'Truth', '-', 'Day', '!'] \n",
      "\n",
      "{'input_ids': [0, 2264, 17516, 653, 16, 19, 3370, 116, 85, 18, 8126, 6, 45, 734, 41286, 12, 10781, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} \n",
      "\n",
      "<s>What?! What is with everybody? It's Thanksgiving, not...Truth-Day!</s>\n"
     ]
    }
   ],
   "source": [
    "# 'tae898/emoberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained('tae898/emoberta-base')\n",
    "\n",
    "# 토크나이저 작동방식 확인\n",
    "result = tokenizer.tokenize(sample)\n",
    "print(result, '\\n')\n",
    "encoded = tokenizer(sample)\n",
    "print(encoded, '\\n')\n",
    "decoded = tokenizer.decode(encoded['input_ids'])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e658829b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', '?!', 'ĠWhat', 'Ġis', 'Ġwith', 'Ġeverybody', '?', 'ĠIt', \"'s\", 'ĠThanksgiving', ',', 'Ġnot', '...', 'Truth', '-', 'Day', '!'] \n",
      "\n",
      "{'input_ids': [0, 2264, 17516, 653, 16, 19, 3370, 116, 85, 18, 8126, 6, 45, 734, 41286, 12, 10781, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} \n",
      "\n",
      "<s>What?! What is with everybody? It's Thanksgiving, not...Truth-Day!</s>\n"
     ]
    }
   ],
   "source": [
    "# 'tae898/emoberta-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained('tae898/emoberta-large')\n",
    "\n",
    "# 토크나이저 작동방식 확인\n",
    "result = tokenizer.tokenize(sample)\n",
    "print(result, '\\n')\n",
    "encoded = tokenizer(sample)\n",
    "print(encoded, '\\n')\n",
    "decoded = tokenizer.decode(encoded['input_ids'])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c1560",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### BERT-uncased vs EmoBERTa   \n",
    "- 대소문자 소문자로 통일 여부: bert-uncased(o), emoberta(x) -> emoberta에는 아직 uncased가 구현되어 있지 않음, 따라서 'Utterance'들을 입력할 때 `lower()`로 낮춰주는 작업을 수동으로 추가해야 할 것 같음 -> 해봤더니 오히려 성능 안좋았음  \n",
    "- 'input_ids'의 숫자가 다른 것은 너무 당연하니 pass\n",
    "- 문장의 앞뒤에 추가되는 토큰: bert-uncased([CLS],[SEP]), emoberta(s,/s), 이거는 BERT와 RoBERTa의 차이에서 비롯된 것  \n",
    "\n",
    "\n",
    "- **가장 중요해 보였던 차이**: bert 토크나이저에는 `'token_type_ids'`가 있었지만 **emoberta 토크나이저에는 아예 없었음!!** 이 부분 때문에 추후 모델 구조를 짤때 BERT와 EmoBERTa 중 어떤 것이냐에 따라 다르게 짜야 하였음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cb434f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1daccd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## test의 일부 speaker 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7448376a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "앞서 EDA 과정에서 발견한 이름이 달랐던 test의 상위 이름 6명을 train과 맞게 변경:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0f9aa97",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joeyy       411\n",
       "Chadler     379\n",
       "Rossi       373\n",
       "Rachell     356\n",
       "Mornica     346\n",
       "Phoebe      291\n",
       "Janice       31\n",
       "Emily        16\n",
       "Director     16\n",
       "Gunther      13\n",
       "Name: Speaker, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변경 전\n",
    "\n",
    "test['Speaker'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "970ddd2f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2610/2610 [00:02<00:00, 913.37it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(len(test))):\n",
    "    test['Speaker'].replace('Joeyy', 'Joey', inplace=True)\n",
    "    test['Speaker'].replace('Chadler', 'Chandler', inplace=True)\n",
    "    test['Speaker'].replace('Rossi', 'Ross', inplace=True)\n",
    "    test['Speaker'].replace('Rachell', 'Rachel', inplace=True)\n",
    "    test['Speaker'].replace('Mornica', 'Monica', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70a86b9f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joey        411\n",
       "Chandler    379\n",
       "Ross        373\n",
       "Rachel      356\n",
       "Monica      346\n",
       "Phoebe      291\n",
       "Janice       31\n",
       "Emily        16\n",
       "Director     16\n",
       "Gunther      13\n",
       "Name: Speaker, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변경 후 \n",
    "\n",
    "test['Speaker'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa6df0f3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>Why do all the coffee cups have figures below?</td>\n",
       "      <td>Mark</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>Oh. It's so Monica can follow. Of this way, if...</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>You know what?</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>Come on, Lydia, you can do it.</td>\n",
       "      <td>Joey</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>To push!</td>\n",
       "      <td>Joey</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>TEST_2605</td>\n",
       "      <td>Yeah, I mean, go Ross, no one will even notice...</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>TEST_2606</td>\n",
       "      <td>They don't listen to me?</td>\n",
       "      <td>Ross</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>TEST_2607</td>\n",
       "      <td>Of course, they listen to you! Everyone listen...</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>TEST_2608</td>\n",
       "      <td>Monica, do you really think I should try this ...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>TEST_2609</td>\n",
       "      <td>I think you look good.</td>\n",
       "      <td>Monica</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2610 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                          Utterance Speaker  \\\n",
       "0     TEST_0000     Why do all the coffee cups have figures below?    Mark   \n",
       "1     TEST_0001  Oh. It's so Monica can follow. Of this way, if...  Rachel   \n",
       "2     TEST_0002                                     You know what?  Rachel   \n",
       "3     TEST_0003                     Come on, Lydia, you can do it.    Joey   \n",
       "4     TEST_0004                                           To push!    Joey   \n",
       "...         ...                                                ...     ...   \n",
       "2605  TEST_2605  Yeah, I mean, go Ross, no one will even notice...  Rachel   \n",
       "2606  TEST_2606                           They don't listen to me?    Ross   \n",
       "2607  TEST_2607  Of course, they listen to you! Everyone listen...  Rachel   \n",
       "2608  TEST_2608  Monica, do you really think I should try this ...    Ross   \n",
       "2609  TEST_2609                             I think you look good.  Monica   \n",
       "\n",
       "      Dialogue_ID  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               1  \n",
       "4               1  \n",
       "...           ...  \n",
       "2605          279  \n",
       "2606          279  \n",
       "2607          279  \n",
       "2608          279  \n",
       "2609          279  \n",
       "\n",
       "[2610 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 재확인\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ea5a6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## train, test의 Utterance 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad2278",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "EmoBERTa 논문을 보니 입력 데이터를 \"Speaker : Utterance\" 식으로 만들어주었음  \n",
    "따라서 train, test의 'Speaker' column을 'Utterance'와 합치는 식으로 변형 시도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a3e6a5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae0c1e5c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>also I was the point person on my company’s tr...</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>You must’ve had your hands full.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>So let’s talk a little bit about your duties.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>0</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                          Utterance  \\\n",
       "0  TRAIN_0000  also I was the point person on my company’s tr...   \n",
       "1  TRAIN_0001                   You must’ve had your hands full.   \n",
       "2  TRAIN_0002                            That I did. That I did.   \n",
       "3  TRAIN_0003      So let’s talk a little bit about your duties.   \n",
       "4  TRAIN_0004                             My duties?  All right.   \n",
       "\n",
       "           Speaker  Dialogue_ID    Target  \n",
       "0         Chandler            0   neutral  \n",
       "1  The Interviewer            0   neutral  \n",
       "2         Chandler            0   neutral  \n",
       "3  The Interviewer            0   neutral  \n",
       "4         Chandler            0  surprise  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변형 전\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7903d717",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train['Utterance'] = train['Speaker'] + \" : \" + train['Utterance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d968229d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>Chandler : also I was the point person on my c...</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>The Interviewer : You must’ve had your hands f...</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>Chandler : That I did. That I did.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>The Interviewer : So let’s talk a little bit a...</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>Chandler : My duties?  All right.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>0</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                          Utterance  \\\n",
       "0  TRAIN_0000  Chandler : also I was the point person on my c...   \n",
       "1  TRAIN_0001  The Interviewer : You must’ve had your hands f...   \n",
       "2  TRAIN_0002                 Chandler : That I did. That I did.   \n",
       "3  TRAIN_0003  The Interviewer : So let’s talk a little bit a...   \n",
       "4  TRAIN_0004                  Chandler : My duties?  All right.   \n",
       "\n",
       "           Speaker  Dialogue_ID    Target  \n",
       "0         Chandler            0   neutral  \n",
       "1  The Interviewer            0   neutral  \n",
       "2         Chandler            0   neutral  \n",
       "3  The Interviewer            0   neutral  \n",
       "4         Chandler            0  surprise  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변형 후\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3f4d4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c839f221",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>TEST_2605</td>\n",
       "      <td>Yeah, I mean, go Ross, no one will even notice...</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>TEST_2606</td>\n",
       "      <td>They don't listen to me?</td>\n",
       "      <td>Ross</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>TEST_2607</td>\n",
       "      <td>Of course, they listen to you! Everyone listen...</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>TEST_2608</td>\n",
       "      <td>Monica, do you really think I should try this ...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>TEST_2609</td>\n",
       "      <td>I think you look good.</td>\n",
       "      <td>Monica</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                          Utterance Speaker  \\\n",
       "2605  TEST_2605  Yeah, I mean, go Ross, no one will even notice...  Rachel   \n",
       "2606  TEST_2606                           They don't listen to me?    Ross   \n",
       "2607  TEST_2607  Of course, they listen to you! Everyone listen...  Rachel   \n",
       "2608  TEST_2608  Monica, do you really think I should try this ...    Ross   \n",
       "2609  TEST_2609                             I think you look good.  Monica   \n",
       "\n",
       "      Dialogue_ID  \n",
       "2605          279  \n",
       "2606          279  \n",
       "2607          279  \n",
       "2608          279  \n",
       "2609          279  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변형 전\n",
    "\n",
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62fdb5b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test['Utterance'] = test['Speaker'] + \" : \" + test['Utterance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "073f145b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>TEST_2605</td>\n",
       "      <td>Rachel : Yeah, I mean, go Ross, no one will ev...</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>TEST_2606</td>\n",
       "      <td>Ross : They don't listen to me?</td>\n",
       "      <td>Ross</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>TEST_2607</td>\n",
       "      <td>Rachel : Of course, they listen to you! Everyo...</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>TEST_2608</td>\n",
       "      <td>Ross : Monica, do you really think I should tr...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>TEST_2609</td>\n",
       "      <td>Monica : I think you look good.</td>\n",
       "      <td>Monica</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                          Utterance Speaker  \\\n",
       "2605  TEST_2605  Rachel : Yeah, I mean, go Ross, no one will ev...  Rachel   \n",
       "2606  TEST_2606                    Ross : They don't listen to me?    Ross   \n",
       "2607  TEST_2607  Rachel : Of course, they listen to you! Everyo...  Rachel   \n",
       "2608  TEST_2608  Ross : Monica, do you really think I should tr...    Ross   \n",
       "2609  TEST_2609                    Monica : I think you look good.  Monica   \n",
       "\n",
       "      Dialogue_ID  \n",
       "2605          279  \n",
       "2606          279  \n",
       "2607          279  \n",
       "2608          279  \n",
       "2609          279  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변형 후\n",
    "\n",
    "test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7fb05",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Label encoding  \n",
    "sklearn.preprocessing의 LabelEncoder() 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ca1306a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        neutral\n",
       "1        neutral\n",
       "2        neutral\n",
       "3        neutral\n",
       "4       surprise\n",
       "          ...   \n",
       "9984     neutral\n",
       "9985     neutral\n",
       "9986    surprise\n",
       "9987     neutral\n",
       "9988         joy\n",
       "Name: Target, Length: 9989, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변환 전\n",
    "\n",
    "train['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa71404b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train의 Target column을 범주로 변환\n",
    "\n",
    "le = LabelEncoder()\n",
    "le = le.fit(train['Target'])\n",
    "train['Target'] = le.transform(train['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "770628cb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       4\n",
       "1       4\n",
       "2       4\n",
       "3       4\n",
       "4       6\n",
       "       ..\n",
       "9984    4\n",
       "9985    4\n",
       "9986    6\n",
       "9987    4\n",
       "9988    3\n",
       "Name: Target, Length: 9989, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변환 후\n",
    "\n",
    "train['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a20fe359",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness',\n",
       "       'surprise'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 범주 종류 확인\n",
    "\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae567fe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train/Validation set 분리  \n",
    "train의 'Dialogue_ID' column을 기준으로 1016개(98%)/23개(2%)로 분리하고자 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6b0da0d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9725\n",
      "264\n"
     ]
    }
   ],
   "source": [
    "valid = train[~train['Dialogue_ID'].isin([i for i in range(1016)])].reset_index(drop=True)\n",
    "train = train[train['Dialogue_ID'].isin([i for i in range(1016)])].reset_index(drop=True)\n",
    "\n",
    "train_len = len(train)\n",
    "valid_len = len(valid)\n",
    "\n",
    "print(train_len)\n",
    "print(valid_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0b5187",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CustomDataset 클래스 생성\n",
    "getitem 부분 정의 시 **max_length**는 EDA에서 파악한 정수 인코딩된 문장 길이를 토대로 잘리는 부분 없이 넉넉하게 128로 지정하였음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "315f07fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 사용할 토크나이저 지정\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('tae898/emoberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10c81df5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# CustomDataset 클래스 정의\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, mode='train'):\n",
    "        self.dataset = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset['Utterance'][idx]\n",
    "        #text = text.lower()\n",
    "        inputs = self.tokenizer(text, padding='max_length', max_length=128,\n",
    "                               truncation=True, return_tensors='pt')\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            y = self.dataset['Target'][idx]\n",
    "            return input_ids, attention_mask, y\n",
    "        else:\n",
    "            return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e826659a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train/valid 데이터셋, 데이터로더 생성\n",
    "# BS 사용\n",
    "\n",
    "train = CustomDataset(train, mode='train')\n",
    "valid = CustomDataset(valid, mode='train')\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=BS, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid, batch_size=BS, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd71c4d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ad36d1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 주요 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c1dc47",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### class BaseModel 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b654b134",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, dropout=0.5, num_classes=len(le.classes_)):\n",
    "        super(BaseModel, self).__init__()\n",
    "        \n",
    "        self.bert = AutoModel.from_pretrained('tae898/emoberta-base')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, num_classes) # 768: base 임베딩 벡터의 hidden layer 차원\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input_id, mask):\n",
    "        _, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "        \n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22313ce",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### train 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8f7b425",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# EPOCHS 사용\n",
    "\n",
    "def train(model, optimizer, train_loader, test_loader, device):\n",
    "    model.to(device) \n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    best_score = 0\n",
    "    #best_model = \"None\"\n",
    "    \n",
    "    for epoch_num in range(EPOCHS): # 여기!\n",
    "        model.train() # train 모드로 변경\n",
    "        train_loss = []\n",
    "        for input_ids, attention_mask, train_label in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            train_label = train_label.to(device)\n",
    "            input_id = input_ids.to(device)\n",
    "            mask = attention_mask.to(device)\n",
    "            \n",
    "            output = model(input_id, mask) # forward 함수와 입력 형식 맞춰줌\n",
    "            \n",
    "            batch_loss = criterion(output, train_label.long())\n",
    "            train_loss.append(batch_loss.item())\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        val_loss, val_score = validation(model, criterion, test_loader, device)\n",
    "        print(f'Epoch [{epoch_num}], Train Loss : [{np.mean(train_loss) :.5f}] Val Loss : [{np.mean(val_loss) :.5f}] Val F1 Score : [{val_score:.5f}]')\n",
    "        \n",
    "        if best_score < val_score:\n",
    "            #best_model = model\n",
    "            best_score = val_score\n",
    "            torch.save(model.state_dict(), './model/best_emoberta_base_model.pth',\n",
    "                      _use_new_zipfile_serialization=False)\n",
    "            \n",
    "#    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ccaff7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### validation 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83b3e500",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validation(model, criterion, test_loader, device):\n",
    "    model.eval() # evaluation 모드로 변경\n",
    "    val_loss, model_preds, true_labels = [], [], []\n",
    "    \n",
    "    # pytorch의 autograd engine을 비활성화하여 gradient를 계산하지 않도록 함\n",
    "    # 보통 model.eval()과 함께 쓰임\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, valid_label in tqdm(test_loader):\n",
    "            valid_label = valid_label.to(device)\n",
    "            input_id = input_ids.to(device)\n",
    "            mask = attention_mask.to(device)\n",
    "            \n",
    "            output = model(input_id, mask)\n",
    "            \n",
    "            batch_loss = criterion(output, valid_label.long())\n",
    "            val_loss.append(batch_loss.item())\n",
    "            \n",
    "            model_preds += output.argmax(1).detach().cpu().numpy().tolist()\n",
    "            true_labels += valid_label.detach().cpu().numpy().tolist()\n",
    "        val_f1 = f1_score(true_labels, model_preds, average=\"macro\")\n",
    "    \n",
    "    return val_loss, val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b76e53",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "512c649e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tae898/emoberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at tae898/emoberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1216/1216 [01:33<00:00, 13.06it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 43.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], Train Loss : [0.99802] Val Loss : [0.92437] Val F1 Score : [0.44305]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [01:31<00:00, 13.34it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 44.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.80344] Val Loss : [0.90892] Val F1 Score : [0.54457]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [01:30<00:00, 13.42it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 44.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.66720] Val Loss : [0.92293] Val F1 Score : [0.55155]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [01:31<00:00, 13.35it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 44.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.55763] Val Loss : [1.00066] Val F1 Score : [0.55593]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [01:31<00:00, 13.32it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 44.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.45546] Val Loss : [1.16590] Val F1 Score : [0.55022]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# LR 사용\n",
    "\n",
    "model = BaseModel()\n",
    "model.eval()\n",
    "optimizer = Adam(params = model.parameters(), lr=LR)\n",
    "\n",
    "train(model, optimizer, train_dataloader, valid_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278f1d1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a62a299d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tae898/emoberta-base were not used when initializing RobertaModel: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at tae898/emoberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (bert): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=7, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_model = BaseModel()\n",
    "infer_model.load_state_dict(torch.load('./model/best_emoberta_base_model.pth'))\n",
    "infer_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51224356",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 모델 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7bc0b7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### test 데이터로더 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e55170ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BS 사용\n",
    "\n",
    "test = CustomDataset(test, mode=\"test\")\n",
    "test_dataloader = DataLoader(test, batch_size=BS, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce5146",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### inference 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2deb03e2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval() # evaluation 모드로 변경\n",
    "    test_predict = []\n",
    "    \n",
    "    for input_ids, attention_mask in tqdm(test_loader):\n",
    "        input_id = input_ids.to(device)\n",
    "        mask = attention_mask.to(device)\n",
    "        \n",
    "        y_pred = model(input_id, mask)\n",
    "        test_predict += y_pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "    print('Done.')\n",
    "    return test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37858cbb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [00:07<00:00, 43.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds = inference(infer_model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da88300c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b88b0957",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 4, 4, 4, 3, 3, 3, 3, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "# 앞에 10개만 label 확인\n",
    "\n",
    "print(preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f94826f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['surprise', 'neutral', 'neutral', 'neutral', 'joy', 'joy', 'joy',\n",
       "       'joy', 'sadness', 'joy'], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LabelEncoder로 숫자로 인코딩했던 값을 다시 원래 문자 label로 변환\n",
    "\n",
    "preds = le.inverse_transform(preds)\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7de6ebcc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 311,\n",
       " 'disgust': 25,\n",
       " 'fear': 33,\n",
       " 'joy': 444,\n",
       " 'neutral': 1383,\n",
       " 'sadness': 161,\n",
       " 'surprise': 253}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds에 존재하는 감정 종류 파악\n",
    "\n",
    "unique, counts = np.unique(preds, return_counts=True)\n",
    "uniq_cnt_dict = dict(zip(unique, counts))\n",
    "uniq_cnt_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831bf724",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 예측 결과물 Sumit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d535f10",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  Target\n",
       "0  TEST_0000       0\n",
       "1  TEST_0001       0\n",
       "2  TEST_0002       0\n",
       "3  TEST_0003       0\n",
       "4  TEST_0004       0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = pd.read_csv('./data/sample_submission.csv')\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "708a994c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    Target\n",
       "0  TEST_0000  surprise\n",
       "1  TEST_0001   neutral\n",
       "2  TEST_0002   neutral\n",
       "3  TEST_0003   neutral\n",
       "4  TEST_0004       joy"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit['Target'] = preds\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b0b019d5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "submit.to_csv('./predictions/best_emoberta_base_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f809dda1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}